{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2de004bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-26 06:01:52.270 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /home/antonio/anaconda3/lib/python3.12/site-packages/ipykernel_launcher.py [ARGUMENTS]\n",
      "2026-01-26 06:01:52.275 No runtime found, using MemoryCacheStorageManager\n",
      "2026-01-26 06:01:52.278 No runtime found, using MemoryCacheStorageManager\n",
      "2026-01-26 06:01:52.311 Session state does not function when running a script without `streamlit run`\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import talib\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from scipy import stats\n",
    "from scipy.stats import gaussian_kde, pointbiserialr\n",
    "from itertools import combinations, product\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ===================== PAGE CONFIGURATION =====================\n",
    "st.set_page_config(\n",
    "    page_title=\"Advanced Rule-Extraction Platform\",\n",
    "    page_icon=\"ðŸš€\",\n",
    "    layout=\"wide\",\n",
    "    initial_sidebar_state=\"collapsed\"\n",
    ")\n",
    "\n",
    "# ===================== ELEGANT DARK MODE STYLING =====================\n",
    "st.markdown(\"\"\"\n",
    "    <style>\n",
    "    @import url('https://fonts.googleapis.com/css2?family=Inter:wght@200;300;400;500;600;700&display=swap');\n",
    "    \n",
    "    * {\n",
    "        font-family: 'Inter', sans-serif;\n",
    "    }\n",
    "    \n",
    "    .stApp {\n",
    "        background: #0f0f0f;\n",
    "    }\n",
    "    \n",
    "    .main-header {\n",
    "        color: #f0f0f0;\n",
    "        font-weight: 200;\n",
    "        font-size: 3rem;\n",
    "        text-align: center;\n",
    "        letter-spacing: -0.03em;\n",
    "        margin-bottom: 0.3rem;\n",
    "    }\n",
    "    \n",
    "    .sub-header {\n",
    "        text-align: center;\n",
    "        color: #808080;\n",
    "        font-size: 0.95rem;\n",
    "        font-weight: 300;\n",
    "        margin-bottom: 3rem;\n",
    "        letter-spacing: 0.02em;\n",
    "    }\n",
    "    \n",
    "    .config-section {\n",
    "        background: linear-gradient(135deg, #1a1a1a 0%, #222222 100%);\n",
    "        border: 1px solid #333333;\n",
    "        border-radius: 12px;\n",
    "        padding: 1.8rem;\n",
    "        margin-bottom: 1.5rem;\n",
    "        box-shadow: 0 4px 6px rgba(0, 0, 0, 0.3);\n",
    "    }\n",
    "    \n",
    "    .section-title {\n",
    "        color: #d0d0d0;\n",
    "        font-weight: 400;\n",
    "        font-size: 0.85rem;\n",
    "        text-transform: uppercase;\n",
    "        letter-spacing: 0.1em;\n",
    "        margin-bottom: 1.2rem;\n",
    "        padding-bottom: 0.5rem;\n",
    "        border-bottom: 1px solid #333333;\n",
    "    }\n",
    "    \n",
    "    .optimization-card {\n",
    "        background: linear-gradient(135deg, #1a1a2e 0%, #16213e 100%);\n",
    "        border: 2px solid #0f3460;\n",
    "        border-radius: 16px;\n",
    "        padding: 2rem;\n",
    "        margin: 1rem 0;\n",
    "        box-shadow: 0 8px 16px rgba(0, 0, 0, 0.4);\n",
    "        transition: transform 0.2s ease;\n",
    "    }\n",
    "    \n",
    "    .optimization-card:hover {\n",
    "        transform: translateY(-4px);\n",
    "        border-color: #4a69bd;\n",
    "    }\n",
    "    \n",
    "    .method-badge {\n",
    "        display: inline-block;\n",
    "        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
    "        color: white;\n",
    "        padding: 0.5rem 1rem;\n",
    "        border-radius: 20px;\n",
    "        font-size: 0.85rem;\n",
    "        font-weight: 600;\n",
    "        margin: 0.5rem 0.5rem 0.5rem 0;\n",
    "        text-transform: uppercase;\n",
    "        letter-spacing: 0.05em;\n",
    "    }\n",
    "    \n",
    "    .metric-card {\n",
    "        background: linear-gradient(135deg, #1e1e2e 0%, #2a2a3a 100%);\n",
    "        border: 1px solid #3a3a4a;\n",
    "        border-radius: 12px;\n",
    "        padding: 1.5rem;\n",
    "        text-align: center;\n",
    "        box-shadow: 0 4px 8px rgba(0, 0, 0, 0.3);\n",
    "    }\n",
    "    \n",
    "    .metric-value {\n",
    "        font-size: 2.5rem;\n",
    "        font-weight: 600;\n",
    "        color: #4ade80;\n",
    "        margin: 0.5rem 0;\n",
    "    }\n",
    "    \n",
    "    .metric-label {\n",
    "        font-size: 0.85rem;\n",
    "        color: #9ca3af;\n",
    "        text-transform: uppercase;\n",
    "        letter-spacing: 0.05em;\n",
    "    }\n",
    "    \n",
    "    div[data-testid=\"metric-container\"] {\n",
    "        background: linear-gradient(135deg, #1a1a1a 0%, #252525 100%);\n",
    "        border: 1px solid #404040;\n",
    "        padding: 1.2rem;\n",
    "        border-radius: 8px;\n",
    "        box-shadow: 0 2px 4px rgba(0, 0, 0, 0.2);\n",
    "    }\n",
    "    \n",
    "    div[data-testid=\"metric-container\"] label {\n",
    "        color: #a0a0a0 !important;\n",
    "        font-size: 0.8rem !important;\n",
    "        text-transform: uppercase;\n",
    "        letter-spacing: 0.05em;\n",
    "    }\n",
    "    \n",
    "    div[data-testid=\"metric-container\"] > div {\n",
    "        color: #f0f0f0 !important;\n",
    "        font-weight: 500 !important;\n",
    "        font-size: 1.4rem !important;\n",
    "    }\n",
    "    \n",
    "    .stButton > button {\n",
    "        background: linear-gradient(135deg, #4a4a4a 0%, #606060 100%);\n",
    "        color: #f0f0f0;\n",
    "        border: 1px solid #666666;\n",
    "        padding: 0.8rem 3rem;\n",
    "        font-weight: 400;\n",
    "        font-size: 0.95rem;\n",
    "        border-radius: 8px;\n",
    "        letter-spacing: 0.05em;\n",
    "        transition: all 0.3s ease;\n",
    "    }\n",
    "    \n",
    "    .stButton > button:hover {\n",
    "        background: linear-gradient(135deg, #606060 0%, #707070 100%);\n",
    "        border-color: #888888;\n",
    "        transform: translateY(-2px);\n",
    "        box-shadow: 0 6px 20px rgba(255, 255, 255, 0.1);\n",
    "    }\n",
    "    \n",
    "    .stButton > button[kind=\"primary\"] {\n",
    "        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
    "        border-color: #764ba2;\n",
    "    }\n",
    "    \n",
    "    .stButton > button[kind=\"primary\"]:hover {\n",
    "        background: linear-gradient(135deg, #764ba2 0%, #667eea 100%);\n",
    "        box-shadow: 0 6px 20px rgba(118, 75, 162, 0.4);\n",
    "    }\n",
    "    \n",
    "    .stTabs [data-baseweb=\"tab-list\"] {\n",
    "        background: #1a1a1a;\n",
    "        border-radius: 8px;\n",
    "        padding: 0.3rem;\n",
    "        gap: 0.5rem;\n",
    "    }\n",
    "    \n",
    "    .stTabs [data-baseweb=\"tab\"] {\n",
    "        background: transparent;\n",
    "        color: #808080;\n",
    "        border-radius: 6px;\n",
    "        padding: 0.6rem 1.2rem;\n",
    "        transition: all 0.2s ease;\n",
    "        font-weight: 400;\n",
    "    }\n",
    "    \n",
    "    .stTabs [aria-selected=\"true\"] {\n",
    "        background: linear-gradient(135deg, #333333 0%, #404040 100%);\n",
    "        color: #f0f0f0;\n",
    "        box-shadow: 0 2px 4px rgba(0, 0, 0, 0.2);\n",
    "    }\n",
    "    \n",
    "    .info-badge {\n",
    "        background: linear-gradient(135deg, #1a1a1a 0%, #2a2a2a 100%);\n",
    "        color: #b0b0b0;\n",
    "        padding: 0.6rem 1.2rem;\n",
    "        border-radius: 8px;\n",
    "        border: 1px solid #404040;\n",
    "        font-size: 0.85rem;\n",
    "        margin: 1rem 0;\n",
    "        font-weight: 300;\n",
    "    }\n",
    "    \n",
    "    .success-badge {\n",
    "        background: linear-gradient(135deg, #1a2818 0%, #253023 100%);\n",
    "        color: #90ee90;\n",
    "        padding: 0.6rem 1.2rem;\n",
    "        border-radius: 8px;\n",
    "        border: 1px solid #4a5a48;\n",
    "        font-size: 0.85rem;\n",
    "        margin: 1rem 0;\n",
    "        font-weight: 300;\n",
    "    }\n",
    "    \n",
    "    .warning-badge {\n",
    "        background: linear-gradient(135deg, #2a1a18 0%, #302520 100%);\n",
    "        color: #f59e0b;\n",
    "        padding: 0.6rem 1.2rem;\n",
    "        border-radius: 8px;\n",
    "        border: 1px solid #5a4a48;\n",
    "        font-size: 0.85rem;\n",
    "        margin: 1rem 0;\n",
    "        font-weight: 300;\n",
    "    }\n",
    "    \n",
    "    .stSelectbox label, .stTextInput label, .stNumberInput label, .stSlider label {\n",
    "        color: #d0d0d0 !important;\n",
    "        font-size: 0.85rem !important;\n",
    "        font-weight: 400 !important;\n",
    "        text-transform: uppercase;\n",
    "        letter-spacing: 0.05em;\n",
    "    }\n",
    "    \n",
    "    .progress-text {\n",
    "        font-size: 1.1rem;\n",
    "        color: #4ade80;\n",
    "        font-weight: 500;\n",
    "        text-align: center;\n",
    "        margin: 1rem 0;\n",
    "    }\n",
    "    \n",
    "    hr {\n",
    "        border: none;\n",
    "        height: 1px;\n",
    "        background: linear-gradient(90deg, transparent, #404040, transparent);\n",
    "        margin: 2.5rem 0;\n",
    "    }\n",
    "    \n",
    "    ::-webkit-scrollbar {\n",
    "        width: 10px;\n",
    "        height: 10px;\n",
    "    }\n",
    "    \n",
    "    ::-webkit-scrollbar-track {\n",
    "        background: #1a1a1a;\n",
    "    }\n",
    "    \n",
    "    ::-webkit-scrollbar-thumb {\n",
    "        background: #404040;\n",
    "        border-radius: 5px;\n",
    "    }\n",
    "    \n",
    "    ::-webkit-scrollbar-thumb:hover {\n",
    "        background: #505050;\n",
    "    }\n",
    "    </style>\n",
    "    \"\"\", unsafe_allow_html=True)\n",
    "\n",
    "# ===================== TECHNICAL INDICATORS CLASS =====================\n",
    "class TechnicalIndicators:\n",
    "    \"\"\"Complete TALib indicators manager (200+ indicators)\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_single_indicator(name, h, l, c, v, o, p):\n",
    "        \"\"\"Calculate an indicator with error handling\"\"\"\n",
    "        try:\n",
    "            h = np.asarray(h, dtype=np.float64)\n",
    "            l = np.asarray(l, dtype=np.float64)\n",
    "            c = np.asarray(c, dtype=np.float64)\n",
    "            v = np.asarray(v, dtype=np.float64)\n",
    "            o = np.asarray(o, dtype=np.float64)\n",
    "            \n",
    "            # Overlay Studies\n",
    "            if name == 'BBANDS':\n",
    "                result = talib.BBANDS(c, timeperiod=p or 20)\n",
    "                return result[0]\n",
    "            elif name == 'DEMA': return talib.DEMA(c, timeperiod=p or 30)\n",
    "            elif name == 'EMA': return talib.EMA(c, timeperiod=p or 30)\n",
    "            elif name == 'HT_TRENDLINE': return talib.HT_TRENDLINE(c)\n",
    "            elif name == 'KAMA': return talib.KAMA(c, timeperiod=p or 30)\n",
    "            elif name == 'MA': return talib.MA(c, timeperiod=p or 30)\n",
    "            elif name == 'MAMA':\n",
    "                result = talib.MAMA(c, fastlimit=0.5, slowlimit=0.05)\n",
    "                return result[0] if isinstance(result, tuple) else result\n",
    "            elif name == 'MIDPOINT': return talib.MIDPOINT(c, timeperiod=p or 14)\n",
    "            elif name == 'MIDPRICE': return talib.MIDPRICE(h, l, timeperiod=p or 14)\n",
    "            elif name == 'SAR': return talib.SAR(h, l)\n",
    "            elif name == 'SAREXT': return talib.SAREXT(h, l)\n",
    "            elif name == 'SMA': return talib.SMA(c, timeperiod=p or 30)\n",
    "            elif name == 'T3': return talib.T3(c, timeperiod=p or 5)\n",
    "            elif name == 'TEMA': return talib.TEMA(c, timeperiod=p or 30)\n",
    "            elif name == 'TRIMA': return talib.TRIMA(c, timeperiod=p or 30)\n",
    "            elif name == 'WMA': return talib.WMA(c, timeperiod=p or 30)\n",
    "            \n",
    "            # Momentum Indicators\n",
    "            elif name == 'ADX': return talib.ADX(h, l, c, timeperiod=p or 14)\n",
    "            elif name == 'ADXR': return talib.ADXR(h, l, c, timeperiod=p or 14)\n",
    "            elif name == 'APO': return talib.APO(c, fastperiod=max(p//2, 2) if p else 12, slowperiod=p or 26)\n",
    "            elif name == 'AROON':\n",
    "                result = talib.AROON(h, l, timeperiod=p or 14)\n",
    "                return result[0] if isinstance(result, tuple) else result\n",
    "            elif name == 'AROONOSC': return talib.AROONOSC(h, l, timeperiod=p or 14)\n",
    "            elif name == 'BOP': return talib.BOP(o, h, l, c)\n",
    "            elif name == 'CCI': return talib.CCI(h, l, c, timeperiod=p or 14)\n",
    "            elif name == 'CMO': return talib.CMO(c, timeperiod=p or 14)\n",
    "            elif name == 'DX': return talib.DX(h, l, c, timeperiod=p or 14)\n",
    "            elif name == 'MACD':\n",
    "                result = talib.MACD(c, fastperiod=max(p//2, 2) if p else 12, slowperiod=p or 26)\n",
    "                return result[0] if isinstance(result, tuple) else result\n",
    "            elif name == 'MACDEXT':\n",
    "                result = talib.MACDEXT(c, fastperiod=max(p//2, 2) if p else 12, slowperiod=p or 26)\n",
    "                return result[0] if isinstance(result, tuple) else result\n",
    "            elif name == 'MACDFIX':\n",
    "                result = talib.MACDFIX(c)\n",
    "                return result[0] if isinstance(result, tuple) else result\n",
    "            elif name == 'MFI': return talib.MFI(h, l, c, v, timeperiod=p or 14)\n",
    "            elif name == 'MINUS_DI': return talib.MINUS_DI(h, l, c, timeperiod=p or 14)\n",
    "            elif name == 'MINUS_DM': return talib.MINUS_DM(h, l, timeperiod=p or 14)\n",
    "            elif name == 'MOM': return talib.MOM(c, timeperiod=p or 10)\n",
    "            elif name == 'PLUS_DI': return talib.PLUS_DI(h, l, c, timeperiod=p or 14)\n",
    "            elif name == 'PLUS_DM': return talib.PLUS_DM(h, l, timeperiod=p or 14)\n",
    "            elif name == 'PPO': return talib.PPO(c, fastperiod=max(p//2, 2) if p else 12, slowperiod=p or 26)\n",
    "            elif name == 'ROC': return talib.ROC(c, timeperiod=p or 10)\n",
    "            elif name == 'ROCP': return talib.ROCP(c, timeperiod=p or 10)\n",
    "            elif name == 'ROCR': return talib.ROCR(c, timeperiod=p or 10)\n",
    "            elif name == 'ROCR100': return talib.ROCR100(c, timeperiod=p or 10)\n",
    "            elif name == 'RSI': return talib.RSI(c, timeperiod=p or 14)\n",
    "            elif name == 'STOCH':\n",
    "                result = talib.STOCH(h, l, c, fastk_period=p or 5)\n",
    "                return result[0] if isinstance(result, tuple) else result\n",
    "            elif name == 'STOCHF':\n",
    "                result = talib.STOCHF(h, l, c, fastk_period=p or 5)\n",
    "                return result[0] if isinstance(result, tuple) else result\n",
    "            elif name == 'STOCHRSI':\n",
    "                result = talib.STOCHRSI(c, timeperiod=p or 14)\n",
    "                return result[0] if isinstance(result, tuple) else result\n",
    "            elif name == 'TRIX': return talib.TRIX(c, timeperiod=p or 30)\n",
    "            elif name == 'ULTOSC': return talib.ULTOSC(h, l, c, timeperiod1=max(p//3, 2) if p else 7, timeperiod2=max(p//2, 3) if p else 14, timeperiod3=p or 28)\n",
    "            elif name == 'WILLR': return talib.WILLR(h, l, c, timeperiod=p or 14)\n",
    "            \n",
    "            # Volume Indicators\n",
    "            elif name == 'AD': return talib.AD(h, l, c, v)\n",
    "            elif name == 'ADOSC': return talib.ADOSC(h, l, c, v, fastperiod=max(p//3, 2) if p else 3, slowperiod=p or 10)\n",
    "            elif name == 'OBV': return talib.OBV(c, v)\n",
    "            \n",
    "            # Volatility\n",
    "            elif name == 'ATR': return talib.ATR(h, l, c, timeperiod=p or 14)\n",
    "            elif name == 'NATR': return talib.NATR(h, l, c, timeperiod=p or 14)\n",
    "            elif name == 'TRANGE': return talib.TRANGE(h, l, c)\n",
    "            \n",
    "            # Cycle Indicators\n",
    "            elif name == 'HT_DCPERIOD': return talib.HT_DCPERIOD(c)\n",
    "            elif name == 'HT_DCPHASE': return talib.HT_DCPHASE(c)\n",
    "            elif name == 'HT_PHASOR':\n",
    "                result = talib.HT_PHASOR(c)\n",
    "                return result[0] if isinstance(result, tuple) else result\n",
    "            elif name == 'HT_SINE':\n",
    "                result = talib.HT_SINE(c)\n",
    "                return result[0] if isinstance(result, tuple) else result\n",
    "            elif name == 'HT_TRENDMODE': return talib.HT_TRENDMODE(c)\n",
    "            \n",
    "            # Statistics\n",
    "            elif name == 'BETA': return talib.BETA(h, l, timeperiod=p or 5)\n",
    "            elif name == 'CORREL': return talib.CORREL(h, l, timeperiod=p or 30)\n",
    "            elif name == 'LINEARREG': return talib.LINEARREG(c, timeperiod=p or 14)\n",
    "            elif name == 'LINEARREG_ANGLE': return talib.LINEARREG_ANGLE(c, timeperiod=p or 14)\n",
    "            elif name == 'LINEARREG_INTERCEPT': return talib.LINEARREG_INTERCEPT(c, timeperiod=p or 14)\n",
    "            elif name == 'LINEARREG_SLOPE': return talib.LINEARREG_SLOPE(c, timeperiod=p or 14)\n",
    "            elif name == 'STDDEV': return talib.STDDEV(c, timeperiod=p or 5)\n",
    "            elif name == 'TSF': return talib.TSF(c, timeperiod=p or 14)\n",
    "            elif name == 'VAR': return talib.VAR(c, timeperiod=p or 5)\n",
    "            \n",
    "            # Math Transforms\n",
    "            elif name == 'ACOS': return talib.ACOS(c)\n",
    "            elif name == 'ASIN': return talib.ASIN(c)\n",
    "            elif name == 'ATAN': return talib.ATAN(c)\n",
    "            elif name == 'CEIL': return talib.CEIL(c)\n",
    "            elif name == 'COS': return talib.COS(c)\n",
    "            elif name == 'COSH': return talib.COSH(c)\n",
    "            elif name == 'EXP': return talib.EXP(c)\n",
    "            elif name == 'FLOOR': return talib.FLOOR(c)\n",
    "            elif name == 'LN': return talib.LN(c)\n",
    "            elif name == 'LOG10': return talib.LOG10(c)\n",
    "            elif name == 'SIN': return talib.SIN(c)\n",
    "            elif name == 'SINH': return talib.SINH(c)\n",
    "            elif name == 'SQRT': return talib.SQRT(c)\n",
    "            elif name == 'TAN': return talib.TAN(c)\n",
    "            elif name == 'TANH': return talib.TANH(c)\n",
    "            \n",
    "            # Math Operators\n",
    "            elif name == 'ADD': return talib.ADD(c, c)\n",
    "            elif name == 'DIV': return talib.DIV(c, c)\n",
    "            elif name == 'MAX': return talib.MAX(c, timeperiod=p or 30)\n",
    "            elif name == 'MAXINDEX': return talib.MAXINDEX(c, timeperiod=p or 30)\n",
    "            elif name == 'MIN': return talib.MIN(c, timeperiod=p or 30)\n",
    "            elif name == 'MININDEX': return talib.MININDEX(c, timeperiod=p or 30)\n",
    "            elif name == 'MINMAX':\n",
    "                result = talib.MINMAX(c, timeperiod=p or 30)\n",
    "                return result[0] if isinstance(result, tuple) else result\n",
    "            elif name == 'MINMAXINDEX':\n",
    "                result = talib.MINMAXINDEX(c, timeperiod=p or 30)\n",
    "                return result[0] if isinstance(result, tuple) else result\n",
    "            elif name == 'MULT': return talib.MULT(c, c)\n",
    "            elif name == 'SUB': return talib.SUB(c, c)\n",
    "            elif name == 'SUM': return talib.SUM(c, timeperiod=p or 30)\n",
    "            \n",
    "            # Price Transform\n",
    "            elif name == 'AVGPRICE': return talib.AVGPRICE(o, h, l, c)\n",
    "            elif name == 'MEDPRICE': return talib.MEDPRICE(h, l)\n",
    "            elif name == 'TYPPRICE': return talib.TYPPRICE(h, l, c)\n",
    "            elif name == 'WCLPRICE': return talib.WCLPRICE(h, l, c)\n",
    "            \n",
    "            # Candle patterns\n",
    "            elif name.startswith('CDL'):\n",
    "                if hasattr(talib, name):\n",
    "                    func = getattr(talib, name)\n",
    "                    return func(o, h, l, c)\n",
    "            \n",
    "            return None\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    ALL_INDICATORS = [\n",
    "        'BBANDS', 'DEMA', 'EMA', 'HT_TRENDLINE', 'KAMA', 'MA',\n",
    "        'MAMA', 'MIDPOINT', 'MIDPRICE', 'SAR', 'SAREXT',\n",
    "        'SMA', 'T3', 'TEMA', 'TRIMA', 'WMA',\n",
    "        'ADX', 'ADXR', 'APO', 'AROON', 'AROONOSC', 'BOP',\n",
    "        'CCI', 'CMO', 'DX', 'MACD', 'MACDEXT', 'MACDFIX',\n",
    "        'MFI', 'MINUS_DI', 'MINUS_DM', 'MOM', 'PLUS_DI',\n",
    "        'PLUS_DM', 'PPO', 'ROC', 'ROCP', 'ROCR', 'ROCR100',\n",
    "        'RSI', 'STOCH', 'STOCHF', 'STOCHRSI', 'TRIX',\n",
    "        'ULTOSC', 'WILLR',\n",
    "        'AD', 'ADOSC', 'OBV',\n",
    "        'ATR', 'NATR', 'TRANGE',\n",
    "        'HT_DCPERIOD', 'HT_DCPHASE', 'HT_PHASOR', 'HT_SINE', 'HT_TRENDMODE',\n",
    "        'BETA', 'CORREL', 'LINEARREG', 'LINEARREG_ANGLE',\n",
    "        'LINEARREG_INTERCEPT', 'LINEARREG_SLOPE', 'STDDEV', 'TSF', 'VAR',\n",
    "        'ACOS', 'ASIN', 'ATAN', 'CEIL', 'COS', 'COSH',\n",
    "        'EXP', 'FLOOR', 'LN', 'LOG10', 'SIN', 'SINH',\n",
    "        'SQRT', 'TAN', 'TANH',\n",
    "        'ADD', 'DIV', 'MAX', 'MAXINDEX', 'MIN', 'MININDEX',\n",
    "        'MINMAX', 'MINMAXINDEX', 'MULT', 'SUB', 'SUM',\n",
    "        'AVGPRICE', 'MEDPRICE', 'TYPPRICE', 'WCLPRICE',\n",
    "    ]\n",
    "    \n",
    "    CANDLE_PATTERNS = [\n",
    "        'CDL2CROWS', 'CDL3BLACKCROWS', 'CDL3INSIDE', 'CDL3LINESTRIKE', 'CDL3OUTSIDE',\n",
    "        'CDL3STARSINSOUTH', 'CDL3WHITESOLDIERS', 'CDLABANDONEDBABY', 'CDLADVANCEBLOCK',\n",
    "        'CDLBELTHOLD', 'CDLBREAKAWAY', 'CDLCLOSINGMARUBOZU', 'CDLCONCEALBABYSWALL',\n",
    "        'CDLCOUNTERATTACK', 'CDLDARKCLOUDCOVER', 'CDLDOJI', 'CDLDOJISTAR',\n",
    "        'CDLDRAGONFLYDOJI', 'CDLENGULFING', 'CDLEVENINGDOJISTAR', 'CDLEVENINGSTAR',\n",
    "        'CDLGAPSIDESIDEWHITE', 'CDLGRAVESTONEDOJI', 'CDLHAMMER', 'CDLHANGINGMAN',\n",
    "        'CDLHARAMI', 'CDLHARAMICROSS', 'CDLHIGHWAVE', 'CDLHIKKAKE', 'CDLHIKKAKEMOD',\n",
    "        'CDLHOMINGPIGEON', 'CDLIDENTICAL3CROWS', 'CDLINNECK', 'CDLINVERTEDHAMMER',\n",
    "        'CDLKICKING', 'CDLKICKINGBYLENGTH', 'CDLLADDERBOTTOM', 'CDLLONGLEGGEDDOJI',\n",
    "        'CDLLONGLINE', 'CDLMARUBOZU', 'CDLMATCHINGLOW', 'CDLMATHOLD',\n",
    "        'CDLMORNINGDOJISTAR', 'CDLMORNINGSTAR', 'CDLONNECK', 'CDLPIERCING',\n",
    "        'CDLRICKSHAWMAN', 'CDLRISEFALL3METHODS', 'CDLSEPARATINGLINES',\n",
    "        'CDLSHOOTINGSTAR', 'CDLSHORTLINE', 'CDLSPINNINGTOP', 'CDLSTALLEDPATTERN',\n",
    "        'CDLSTICKSANDWICH', 'CDLTAKURI', 'CDLTASUKIGAP', 'CDLTHRUSTING',\n",
    "        'CDLTRISTAR', 'CDLUNIQUE3RIVER', 'CDLUPSIDEGAP2CROWS', 'CDLXSIDEGAP3METHODS'\n",
    "    ]\n",
    "    \n",
    "    CATEGORIES = {\n",
    "        \"SuperposiciÃ³n\": ['BBANDS', 'DEMA', 'EMA', 'HT_TRENDLINE', 'KAMA', 'MA',\n",
    "                     'MAMA', 'MIDPOINT', 'MIDPRICE', 'SAR', 'SAREXT',\n",
    "                     'SMA', 'T3', 'TEMA', 'TRIMA', 'WMA'],\n",
    "        \"Momentum\": ['ADX', 'ADXR', 'APO', 'AROON', 'AROONOSC', 'BOP',\n",
    "                     'CCI', 'CMO', 'DX', 'MACD', 'MACDEXT', 'MACDFIX',\n",
    "                     'MFI', 'MINUS_DI', 'MINUS_DM', 'MOM', 'PLUS_DI',\n",
    "                     'PLUS_DM', 'PPO', 'ROC', 'ROCP', 'ROCR', 'ROCR100',\n",
    "                     'RSI', 'STOCH', 'STOCHF', 'STOCHRSI', 'TRIX',\n",
    "                     'ULTOSC', 'WILLR'],\n",
    "        \"Volumen\": ['AD', 'ADOSC', 'OBV'],\n",
    "        \"Volatilidad\": ['ATR', 'NATR', 'TRANGE'],\n",
    "        \"Ciclos\": ['HT_DCPERIOD', 'HT_DCPHASE', 'HT_PHASOR', 'HT_SINE', 'HT_TRENDMODE'],\n",
    "        \"EstadÃ­sticas\": ['BETA', 'CORREL', 'LINEARREG', 'LINEARREG_ANGLE',\n",
    "                       'LINEARREG_INTERCEPT', 'LINEARREG_SLOPE', 'STDDEV', 'TSF', 'VAR'],\n",
    "        \"TransformaciÃ³n MatemÃ¡tica\": ['ACOS', 'ASIN', 'ATAN', 'CEIL', 'COS', 'COSH',\n",
    "                          'EXP', 'FLOOR', 'LN', 'LOG10', 'SIN', 'SINH',\n",
    "                          'SQRT', 'TAN', 'TANH'],\n",
    "        \"Operadores MatemÃ¡ticos\": ['ADD', 'DIV', 'MAX', 'MAXINDEX', 'MIN', 'MININDEX',\n",
    "                          'MINMAX', 'MINMAXINDEX', 'MULT', 'SUB', 'SUM'],\n",
    "        \"TransformaciÃ³n de Precio\": ['AVGPRICE', 'MEDPRICE', 'TYPPRICE', 'WCLPRICE'],\n",
    "        \"Patrones de Velas\": CANDLE_PATTERNS\n",
    "    }\n",
    "    \n",
    "    @classmethod\n",
    "    def needs_period(cls, indicator_name):\n",
    "        no_period = [\n",
    "            'HT_TRENDLINE', 'BOP', 'MACDFIX', 'AD', 'OBV', 'TRANGE',\n",
    "            'SAR', 'SAREXT', 'MAMA',\n",
    "            'HT_DCPERIOD', 'HT_DCPHASE', 'HT_PHASOR', 'HT_SINE', 'HT_TRENDMODE',\n",
    "            'AVGPRICE', 'MEDPRICE', 'TYPPRICE', 'WCLPRICE',\n",
    "            'ACOS', 'ASIN', 'ATAN', 'CEIL', 'COS', 'COSH',\n",
    "            'EXP', 'FLOOR', 'LN', 'LOG10', 'SIN', 'SINH',\n",
    "            'SQRT', 'TAN', 'TANH',\n",
    "            'ADD', 'DIV', 'MULT', 'SUB'\n",
    "        ] + cls.CANDLE_PATTERNS\n",
    "        \n",
    "        return indicator_name not in no_period\n",
    "    \n",
    "    @classmethod\n",
    "    def calculate_indicator(cls, indicator_name, high, low, close, volume, open_prices, period):\n",
    "        try:\n",
    "            result = cls.calculate_single_indicator(indicator_name, high, low, close, volume, open_prices, period)\n",
    "            if result is not None:\n",
    "                if not np.all(np.isnan(result)):\n",
    "                    return result\n",
    "            return None\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    @classmethod\n",
    "    def get_total_count(cls):\n",
    "        return len(cls.ALL_INDICATORS) + len(cls.CANDLE_PATTERNS)\n",
    "\n",
    "# ===================== HELPER FUNCTIONS =====================\n",
    "\n",
    "def calculate_indicators_for_dataset(data: pd.DataFrame, periods_to_test: List[int], \n",
    "                                    selected_categories: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"Calculate indicators WITHOUT look-ahead bias\"\"\"\n",
    "    high = data['High'].values\n",
    "    low = data['Low'].values\n",
    "    close = data['Close'].values\n",
    "    volume = data['Volume'].values if 'Volume' in data.columns else np.zeros_like(close)\n",
    "    open_prices = data['Open'].values\n",
    "    \n",
    "    indicators = pd.DataFrame(index=data.index)\n",
    "    \n",
    "    indicators_to_calc = []\n",
    "    if \"TODO\" in selected_categories:\n",
    "        indicators_to_calc = TechnicalIndicators.ALL_INDICATORS + TechnicalIndicators.CANDLE_PATTERNS\n",
    "    else:\n",
    "        for category in selected_categories:\n",
    "            if category in TechnicalIndicators.CATEGORIES:\n",
    "                indicators_to_calc.extend(TechnicalIndicators.CATEGORIES[category])\n",
    "    \n",
    "    indicators_to_calc = list(set(indicators_to_calc))\n",
    "    \n",
    "    for indicator_name in indicators_to_calc:\n",
    "        if TechnicalIndicators.needs_period(indicator_name):\n",
    "            for period in periods_to_test:\n",
    "                result = TechnicalIndicators.calculate_indicator(\n",
    "                    indicator_name, high, low, close, volume, open_prices, period\n",
    "                )\n",
    "                if result is not None:\n",
    "                    indicators[f'{indicator_name}_{period}'] = result\n",
    "        else:\n",
    "            result = TechnicalIndicators.calculate_indicator(\n",
    "                indicator_name, high, low, close, volume, open_prices, 0\n",
    "            )\n",
    "            if result is not None:\n",
    "                indicators[indicator_name] = result\n",
    "    \n",
    "    indicators = indicators.dropna(axis=1, how='all')\n",
    "    return indicators\n",
    "\n",
    "\n",
    "@st.cache_data\n",
    "def download_data(ticker: str, period: str) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"Download historical data\"\"\"\n",
    "    try:\n",
    "        data = yf.download(ticker, period=period, progress=False, auto_adjust=True, multi_level_index=False)\n",
    "        if data.empty:\n",
    "            st.error(f\"No data found for {ticker}\")\n",
    "            return None\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error downloading data: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "@st.cache_data\n",
    "def calculate_all_indicators(ticker: str, period: str, quantiles: int, min_return_days: int, \n",
    "                             max_return_days: int, periods_to_test: List[int], \n",
    "                             selected_categories: List[str]) -> Tuple:\n",
    "    \"\"\"Calculate all selected indicators - for visualization only\"\"\"\n",
    "    \n",
    "    data = download_data(ticker, period)\n",
    "    if data is None:\n",
    "        return None, None, None, None\n",
    "    \n",
    "    for i in range(min_return_days, max_return_days + 1):\n",
    "        data[f'retornos_{i}_dias'] = data['Close'].pct_change(i).shift(-i) * 100\n",
    "    \n",
    "    indicators_to_calc = []\n",
    "    if \"TODO\" in selected_categories:\n",
    "        indicators_to_calc = TechnicalIndicators.ALL_INDICATORS + TechnicalIndicators.CANDLE_PATTERNS\n",
    "    else:\n",
    "        for category in selected_categories:\n",
    "            if category in TechnicalIndicators.CATEGORIES:\n",
    "                indicators_to_calc.extend(TechnicalIndicators.CATEGORIES[category])\n",
    "    \n",
    "    indicators_to_calc = list(set(indicators_to_calc))\n",
    "    \n",
    "    total_calculations = sum(\n",
    "        len(periods_to_test) if TechnicalIndicators.needs_period(ind) else 1 \n",
    "        for ind in indicators_to_calc\n",
    "    )\n",
    "    \n",
    "    progress_bar = st.progress(0)\n",
    "    status_text = st.empty()\n",
    "    calculation_counter = 0\n",
    "    successful = 0\n",
    "    \n",
    "    high = data['High'].values\n",
    "    low = data['Low'].values\n",
    "    close = data['Close'].values\n",
    "    volume = data['Volume'].values if 'Volume' in data.columns else np.zeros_like(close)\n",
    "    open_prices = data['Open'].values\n",
    "    \n",
    "    indicators = pd.DataFrame(index=data.index)\n",
    "    \n",
    "    for indicator_name in indicators_to_calc:\n",
    "        if TechnicalIndicators.needs_period(indicator_name):\n",
    "            for period in periods_to_test:\n",
    "                calculation_counter += 1\n",
    "                status_text.text(f\"â³ Calculating {indicator_name}_{period}...\")\n",
    "                \n",
    "                result = TechnicalIndicators.calculate_indicator(\n",
    "                    indicator_name, high, low, close, volume, open_prices, period\n",
    "                )\n",
    "                \n",
    "                if result is not None:\n",
    "                    indicators[f'{indicator_name}_{period}'] = result\n",
    "                    successful += 1\n",
    "                \n",
    "                progress_bar.progress(calculation_counter / total_calculations)\n",
    "        else:\n",
    "            calculation_counter += 1\n",
    "            status_text.text(f\"â³ Calculating {indicator_name}...\")\n",
    "            \n",
    "            result = TechnicalIndicators.calculate_indicator(\n",
    "                indicator_name, high, low, close, volume, open_prices, 0\n",
    "            )\n",
    "            \n",
    "            if result is not None:\n",
    "                indicators[indicator_name] = result\n",
    "                successful += 1\n",
    "            \n",
    "            progress_bar.progress(calculation_counter / total_calculations)\n",
    "    \n",
    "    progress_bar.empty()\n",
    "    status_text.empty()\n",
    "    \n",
    "    indicators = indicators.dropna(axis=1, how='all')\n",
    "    \n",
    "    returns_data = {}\n",
    "    for indicator_col in indicators.columns:\n",
    "        try:\n",
    "            returns_data[indicator_col] = {}\n",
    "            for i in range(min_return_days, max_return_days + 1):\n",
    "                temp_df = pd.DataFrame({'indicator': indicators[indicator_col]})\n",
    "                ret_col = f'retornos_{i}_dias'\n",
    "                if ret_col in data.columns:\n",
    "                    temp_df[ret_col] = data[ret_col]\n",
    "                temp_df = temp_df.dropna()\n",
    "                \n",
    "                if len(temp_df) >= quantiles * 2:\n",
    "                    temp_df['quantile'] = pd.qcut(temp_df['indicator'], q=quantiles, duplicates='drop')\n",
    "                    grouped = temp_df.groupby('quantile')[ret_col].agg(['mean', 'std', 'count'])\n",
    "                    returns_data[indicator_col][f'retornos_{i}_dias_mean'] = grouped['mean']\n",
    "                    returns_data[indicator_col][f'retornos_{i}_dias_std'] = grouped['std']\n",
    "                    returns_data[indicator_col][f'retornos_{i}_dias_count'] = grouped['count']\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    st.markdown(f\"\"\"\n",
    "        <div class=\"success-badge\">\n",
    "            âœ“ Successfully calculated {successful} of {total_calculations} configurations\n",
    "        </div>\n",
    "    \"\"\", unsafe_allow_html=True)\n",
    "    \n",
    "    summary = {\n",
    "        'total_attempted': total_calculations,\n",
    "        'successful': successful,\n",
    "        'indicators_count': len(indicators.columns),\n",
    "        'data_points': len(data),\n",
    "        'date_range': f\"{data.index[0].strftime('%Y-%m-%d')} to {data.index[-1].strftime('%Y-%m-%d')}\",\n",
    "        'min_return_days': min_return_days,\n",
    "        'max_return_days': max_return_days\n",
    "    }\n",
    "    \n",
    "    return returns_data, indicators, data, summary\n",
    "\n",
    "\n",
    "def create_percentile_plot(indicators, returns_data, data, indicator_name, return_days, quantiles=10):\n",
    "    \"\"\"Create enhanced analysis plots\"\"\"\n",
    "    \n",
    "    if indicator_name not in indicators.columns or indicator_name not in returns_data:\n",
    "        return None\n",
    "    \n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=(\n",
    "            '<b>Distribution & Statistics</b>', '<b>Returns by Percentile</b>',\n",
    "            '<b>Rolling Correlation (126 days)</b>', '<b>Scatter Analysis</b>'\n",
    "        ),\n",
    "        specs=[\n",
    "            [{\"type\": \"histogram\"}, {\"type\": \"bar\"}],\n",
    "            [{\"type\": \"scatter\"}, {\"type\": \"scatter\"}]\n",
    "        ],\n",
    "        vertical_spacing=0.15,\n",
    "        horizontal_spacing=0.15\n",
    "    )\n",
    "    \n",
    "    gradient_colors = ['#FF6B6B', '#FE8C68', '#FEAA68', '#FEC868', '#FFE66D', \n",
    "                       '#C7E66D', '#8FE66D', '#5FE668', '#4FC668', '#51CF66']\n",
    "    \n",
    "    hist_data = indicators[indicator_name].dropna()\n",
    "    \n",
    "    if len(hist_data) > 0:\n",
    "        q1 = hist_data.quantile(0.01)\n",
    "        q99 = hist_data.quantile(0.99)\n",
    "        filtered_data = hist_data[(hist_data >= q1) & (hist_data <= q99)]\n",
    "        \n",
    "        mean_val = filtered_data.mean()\n",
    "        median_val = filtered_data.median()\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Histogram(\n",
    "                x=filtered_data,\n",
    "                nbinsx=100,\n",
    "                marker=dict(\n",
    "                    color='rgba(100, 150, 255, 0.4)',\n",
    "                    line=dict(color='rgba(100, 150, 255, 0.6)', width=0.5)\n",
    "                ),\n",
    "                name='Distribution',\n",
    "                showlegend=False,\n",
    "                histnorm='probability density'\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        kde = gaussian_kde(filtered_data.values)\n",
    "        x_range = np.linspace(filtered_data.min(), filtered_data.max(), 200)\n",
    "        kde_values = kde(x_range)\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=x_range,\n",
    "                y=kde_values,\n",
    "                mode='lines',\n",
    "                line=dict(color='#FFE66D', width=3),\n",
    "                name='KDE',\n",
    "                showlegend=False\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        fig.add_vline(x=mean_val, line=dict(color='#51CF66', width=2, dash='solid'), row=1, col=1)\n",
    "        fig.add_vline(x=median_val, line=dict(color='#FF6B6B', width=2, dash='dash'), row=1, col=1)\n",
    "        \n",
    "        fig.update_xaxes(range=[q1, q99], row=1, col=1)\n",
    "    \n",
    "    returns_col = f'retornos_{return_days}_dias_mean'\n",
    "    if returns_col in returns_data[indicator_name]:\n",
    "        returns_values = returns_data[indicator_name][returns_col]\n",
    "        x_labels = [f'P{i+1}' for i in range(len(returns_values))]\n",
    "        \n",
    "        max_abs = max(abs(returns_values.max()), abs(returns_values.min())) if returns_values.max() != returns_values.min() else 1\n",
    "        normalized_values = [(val + max_abs) / (2 * max_abs) for val in returns_values]\n",
    "        colors = [gradient_colors[min(int(norm * (len(gradient_colors) - 1)), len(gradient_colors)-1)] for norm in normalized_values]\n",
    "        \n",
    "        std_col = f'retornos_{return_days}_dias_std'\n",
    "        error_y = None\n",
    "        if std_col in returns_data[indicator_name]:\n",
    "            error_y = dict(\n",
    "                type='data',\n",
    "                array=returns_data[indicator_name][std_col],\n",
    "                visible=True,\n",
    "                color='rgba(255, 255, 255, 0.3)',\n",
    "                thickness=1.5,\n",
    "                width=4\n",
    "            )\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                x=x_labels,\n",
    "                y=returns_values,\n",
    "                marker=dict(\n",
    "                    color=colors,\n",
    "                    line=dict(color='rgba(255, 255, 255, 0.2)', width=1)\n",
    "                ),\n",
    "                text=[f'{val:.2f}%' for val in returns_values],\n",
    "                textposition='outside',\n",
    "                textfont=dict(size=10, color='white'),\n",
    "                error_y=error_y,\n",
    "                showlegend=False\n",
    "            ),\n",
    "            row=1, col=2\n",
    "        )\n",
    "    \n",
    "    if f'retornos_{return_days}_dias' in data.columns:\n",
    "        common_idx = data.index.intersection(indicators[indicator_name].index)\n",
    "        if len(common_idx) > 126:\n",
    "            aligned_returns = data.loc[common_idx, f'retornos_{return_days}_dias']\n",
    "            aligned_indicator = indicators.loc[common_idx, indicator_name]\n",
    "            \n",
    "            rolling_corr = aligned_returns.rolling(126).corr(aligned_indicator).dropna()\n",
    "            \n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=rolling_corr.index,\n",
    "                    y=rolling_corr.values,\n",
    "                    mode='lines',\n",
    "                    line=dict(color='#FFFFFF', width=2),\n",
    "                    showlegend=False\n",
    "                ),\n",
    "                row=2, col=1\n",
    "            )\n",
    "            \n",
    "            fig.add_hline(y=0, line=dict(color='rgba(255, 255, 255, 0.3)', width=1), row=2, col=1)\n",
    "    \n",
    "    if f'retornos_{return_days}_dias' in data.columns:\n",
    "        common_idx = data.index.intersection(indicators[indicator_name].index)\n",
    "        if len(common_idx) > 0:\n",
    "            x_data = indicators.loc[common_idx, indicator_name]\n",
    "            y_data = data.loc[common_idx, f'retornos_{return_days}_dias']\n",
    "            \n",
    "            mask = ~(x_data.isna() | y_data.isna())\n",
    "            if mask.sum() > 1:\n",
    "                x_clean = x_data[mask]\n",
    "                y_clean = y_data[mask]\n",
    "                \n",
    "                x_q1, x_q99 = x_clean.quantile([0.01, 0.99])\n",
    "                y_q1, y_q99 = y_clean.quantile([0.01, 0.99])\n",
    "                \n",
    "                scatter_mask = (x_clean >= x_q1) & (x_clean <= x_q99) & (y_clean >= y_q1) & (y_clean <= y_q99)\n",
    "                x_filtered = x_clean[scatter_mask]\n",
    "                y_filtered = y_clean[scatter_mask]\n",
    "                \n",
    "                fig.add_trace(\n",
    "                    go.Scattergl(\n",
    "                        x=x_filtered,\n",
    "                        y=y_filtered,\n",
    "                        mode='markers',\n",
    "                        marker=dict(\n",
    "                            size=4,\n",
    "                            color=y_filtered,\n",
    "                            colorscale='RdYlGn',\n",
    "                            opacity=0.6,\n",
    "                            line=dict(width=0),\n",
    "                            showscale=True\n",
    "                        ),\n",
    "                        showlegend=False\n",
    "                    ),\n",
    "                    row=2, col=2\n",
    "                )\n",
    "                \n",
    "                fig.update_xaxes(range=[x_q1, x_q99], row=2, col=2)\n",
    "                fig.update_yaxes(range=[y_q1, y_q99], row=2, col=2)\n",
    "    \n",
    "    fig.update_layout(\n",
    "        template=\"plotly_dark\",\n",
    "        height=800,\n",
    "        title={\n",
    "            'text': f\"<b>{indicator_name}</b> | Returns Analysis at {return_days} Days\",\n",
    "            'font': {'size': 24, 'color': '#f0f0f0', 'family': 'Inter'},\n",
    "            'x': 0.5,\n",
    "            'xanchor': 'center'\n",
    "        },\n",
    "        paper_bgcolor='#0D1117',\n",
    "        plot_bgcolor='#161B22',\n",
    "        showlegend=False,\n",
    "        font=dict(color='#C9D1D9', family='Inter', size=11),\n",
    "        margin=dict(t=80, b=60, l=60, r=120)\n",
    "    )\n",
    "    \n",
    "    fig.update_xaxes(gridcolor='#30363D', showgrid=True, zeroline=False)\n",
    "    fig.update_yaxes(gridcolor='#30363D', showgrid=True, zeroline=False)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# ===================== PARTICLE SWARM OPTIMIZATION =====================\n",
    "\n",
    "@dataclass\n",
    "class Particle:\n",
    "    \"\"\"Particle for PSO optimization\"\"\"\n",
    "    position: np.ndarray\n",
    "    velocity: np.ndarray\n",
    "    best_position: np.ndarray\n",
    "    best_fitness: float\n",
    "    current_fitness: float\n",
    "\n",
    "class ParticleSwarmOptimizer:\n",
    "    \"\"\"Particle Swarm Optimization for trading rules - IMPROVED\"\"\"\n",
    "    \n",
    "    def __init__(self, n_particles: int = 30, n_iterations: int = 50,\n",
    "                 w: float = 0.7, c1: float = 1.5, c2: float = 1.5):\n",
    "        self.n_particles = n_particles\n",
    "        self.n_iterations = n_iterations\n",
    "        self.w = w\n",
    "        self.c1 = c1\n",
    "        self.c2 = c2\n",
    "        self.global_best_position = None\n",
    "        self.global_best_fitness = -np.inf\n",
    "        self.fitness_history = []\n",
    "        self.valid_rules_found = 0\n",
    "        self.total_evaluations = 0\n",
    "\n",
    "\n",
    "class ImprovedParticleSwarmOptimizer(ParticleSwarmOptimizer):\n",
    "    \"\"\"Enhanced PSO with Multi-Swarm, Diversity Archive, and Adaptive Parameters\"\"\"\n",
    "    \n",
    "    def __init__(self, n_particles: int = 50, n_iterations: int = 80,\n",
    "                 w_max: float = 0.9, w_min: float = 0.4, c1: float = 1.5, c2: float = 1.5,\n",
    "                 n_swarms: int = 3, archive_size: int = 50):\n",
    "        super().__init__(n_particles, n_iterations, w_max, c1, c2)\n",
    "        self.w_max = w_max\n",
    "        self.w_min = w_min\n",
    "        self.n_swarms = n_swarms\n",
    "        self.archive_size = archive_size\n",
    "        self.diversity_archive = []  # Store (rule, fitness, indicators_used)\n",
    "        self.swarms_history = []\n",
    "    \n",
    "    def optimize_rule_parameters(self, \n",
    "                                indicators_df: pd.DataFrame,\n",
    "                                returns: pd.Series,\n",
    "                                indicator_names: List[str],\n",
    "                                progress_callback=None) -> Dict:\n",
    "        \"\"\"Multi-Swarm PSO with diversity preservation\"\"\"\n",
    "        \n",
    "        # Filter valid indicators\n",
    "        valid_indicators = [ind for ind in indicator_names \n",
    "                          if ind in indicators_df.columns \n",
    "                          and len(indicators_df[ind].dropna()) >= 20]\n",
    "        \n",
    "        if len(valid_indicators) < 2:\n",
    "            return {\n",
    "                'rules': [],\n",
    "                'fitness_scores': [],\n",
    "                'history': [-999],\n",
    "                'valid_rules_found': 0,\n",
    "                'message': f\"Not enough valid indicators (found {len(valid_indicators)})\"\n",
    "            }\n",
    "        \n",
    "        n_conditions = min(2, len(valid_indicators))\n",
    "        n_dims = n_conditions * 2\n",
    "        \n",
    "        # Create multiple swarms for diversity\n",
    "        particles_per_swarm = max(self.n_particles // self.n_swarms, 10)\n",
    "        swarms = []\n",
    "        \n",
    "        for swarm_id in range(self.n_swarms):\n",
    "            swarm_particles = []\n",
    "            \n",
    "            for i in range(particles_per_swarm):\n",
    "                # Different initialization per swarm\n",
    "                if swarm_id == 0:\n",
    "                    # Swarm 1: Low percentiles (oversold)\n",
    "                    position = np.random.uniform(0.0, 0.35, n_dims)\n",
    "                elif swarm_id == 1:\n",
    "                    # Swarm 2: Mid percentiles (neutral)\n",
    "                    position = np.random.uniform(0.35, 0.65, n_dims)\n",
    "                else:\n",
    "                    # Swarm 3: High percentiles (overbought)\n",
    "                    position = np.random.uniform(0.65, 1.0, n_dims)\n",
    "                \n",
    "                position = np.clip(position + np.random.randn(n_dims) * 0.03, 0, 1)\n",
    "                velocity = np.random.randn(n_dims) * 0.05\n",
    "                \n",
    "                particle = Particle(\n",
    "                    position=position,\n",
    "                    velocity=velocity,\n",
    "                    best_position=position.copy(),\n",
    "                    best_fitness=-np.inf,\n",
    "                    current_fitness=-np.inf\n",
    "                )\n",
    "                swarm_particles.append(particle)\n",
    "            \n",
    "            swarms.append({\n",
    "                'particles': swarm_particles,\n",
    "                'best_position': None,\n",
    "                'best_fitness': -np.inf\n",
    "            })\n",
    "        \n",
    "        # Track global best\n",
    "        global_best_position = None\n",
    "        global_best_fitness = -np.inf\n",
    "        \n",
    "        # Optimization loop\n",
    "        for iteration in range(self.n_iterations):\n",
    "            # Adaptive inertia weight (decreases over time)\n",
    "            w = self.w_max - (self.w_max - self.w_min) * (iteration / self.n_iterations)\n",
    "            \n",
    "            valid_count = 0\n",
    "            \n",
    "            # Evaluate each swarm\n",
    "            for swarm_id, swarm in enumerate(swarms):\n",
    "                for particle in swarm['particles']:\n",
    "                    # Multi-objective fitness\n",
    "                    fitness = self._evaluate_particle_multiobjective(\n",
    "                        particle.position, indicators_df, returns, valid_indicators\n",
    "                    )\n",
    "                    \n",
    "                    self.total_evaluations += 1\n",
    "                    if fitness > -500:\n",
    "                        valid_count += 1\n",
    "                        self.valid_rules_found += 1\n",
    "                    \n",
    "                    particle.current_fitness = fitness\n",
    "                    \n",
    "                    # Update personal best\n",
    "                    if fitness > particle.best_fitness:\n",
    "                        particle.best_fitness = fitness\n",
    "                        particle.best_position = particle.position.copy()\n",
    "                    \n",
    "                    # Update swarm best\n",
    "                    if fitness > swarm['best_fitness']:\n",
    "                        swarm['best_fitness'] = fitness\n",
    "                        swarm['best_position'] = particle.position.copy()\n",
    "                    \n",
    "                    # Update global best\n",
    "                    if fitness > global_best_fitness:\n",
    "                        global_best_fitness = fitness\n",
    "                        global_best_position = particle.position.copy()\n",
    "                \n",
    "                # Update velocities and positions for this swarm\n",
    "                for particle in swarm['particles']:\n",
    "                    r1, r2, r3 = np.random.rand(3)\n",
    "                    \n",
    "                    # Personal, swarm, and global influence\n",
    "                    cognitive = self.c1 * r1 * (particle.best_position - particle.position)\n",
    "                    social_swarm = self.c2 * r2 * (swarm['best_position'] - particle.position)\n",
    "                    social_global = 0.5 * r3 * (global_best_position - particle.position)\n",
    "                    \n",
    "                    particle.velocity = (w * particle.velocity + cognitive + \n",
    "                                       social_swarm + social_global)\n",
    "                    \n",
    "                    # Velocity clamping\n",
    "                    v_max = 0.2 * (1.0 - 0.5 * iteration / self.n_iterations)  # Decrease over time\n",
    "                    particle.velocity = np.clip(particle.velocity, -v_max, v_max)\n",
    "                    \n",
    "                    particle.position = particle.position + particle.velocity\n",
    "                    particle.position = np.clip(particle.position, 0, 1)\n",
    "            \n",
    "            # Inter-swarm migration (every 15 iterations)\n",
    "            if iteration % 15 == 0 and iteration > 0:\n",
    "                self._migrate_best_particles(swarms)\n",
    "            \n",
    "            # Add to diversity archive\n",
    "            if iteration % 10 == 0:\n",
    "                self._update_diversity_archive(\n",
    "                    swarms, indicators_df, valid_indicators\n",
    "                )\n",
    "            \n",
    "            # Restart worst particles if stagnating\n",
    "            if iteration > 20 and iteration % 20 == 0:\n",
    "                self._restart_worst_particles(swarms, n_dims, iteration)\n",
    "            \n",
    "            self.fitness_history.append(global_best_fitness)\n",
    "            \n",
    "            if progress_callback:\n",
    "                progress_callback(iteration + 1, self.n_iterations, \n",
    "                                global_best_fitness, valid_count)\n",
    "        \n",
    "        # Extract diverse rules from archive + top particles\n",
    "        final_rules = self._extract_final_diverse_rules(\n",
    "            swarms, indicators_df, valid_indicators\n",
    "        )\n",
    "        \n",
    "        return final_rules\n",
    "    \n",
    "    def _migrate_best_particles(self, swarms):\n",
    "        \"\"\"Exchange best particles between swarms\"\"\"\n",
    "        n_migrants = 2\n",
    "        \n",
    "        for i in range(len(swarms)):\n",
    "            source_swarm = swarms[i]\n",
    "            target_swarm = swarms[(i + 1) % len(swarms)]\n",
    "            \n",
    "            # Sort by fitness\n",
    "            sorted_particles = sorted(source_swarm['particles'], \n",
    "                                    key=lambda p: p.best_fitness, reverse=True)\n",
    "            \n",
    "            # Copy best to target swarm's worst\n",
    "            target_sorted = sorted(target_swarm['particles'],\n",
    "                                 key=lambda p: p.best_fitness)\n",
    "            \n",
    "            for j in range(min(n_migrants, len(sorted_particles), len(target_sorted))):\n",
    "                target_sorted[j].position = sorted_particles[j].position.copy()\n",
    "                target_sorted[j].velocity = sorted_particles[j].velocity.copy() * 0.5\n",
    "    \n",
    "    def _restart_worst_particles(self, swarms, n_dims, iteration):\n",
    "        \"\"\"Restart worst performing particles to maintain diversity\"\"\"\n",
    "        for swarm in swarms:\n",
    "            particles = swarm['particles']\n",
    "            sorted_particles = sorted(particles, key=lambda p: p.best_fitness)\n",
    "            \n",
    "            # Restart worst 30%\n",
    "            n_restart = max(len(particles) // 3, 2)\n",
    "            \n",
    "            for i in range(n_restart):\n",
    "                # Re-initialize with high diversity\n",
    "                sorted_particles[i].position = np.random.rand(n_dims)\n",
    "                sorted_particles[i].velocity = np.random.randn(n_dims) * 0.05\n",
    "                sorted_particles[i].best_fitness = -np.inf\n",
    "    \n",
    "    def _update_diversity_archive(self, swarms, indicators_df, valid_indicators):\n",
    "        \"\"\"Maintain archive of diverse high-quality rules\"\"\"\n",
    "        candidates = []\n",
    "        \n",
    "        for swarm in swarms:\n",
    "            for particle in swarm['particles']:\n",
    "                if particle.best_fitness > 0.1:  # Threshold for archive\n",
    "                    rule = self._decode_position(\n",
    "                        particle.best_position, indicators_df, valid_indicators\n",
    "                    )\n",
    "                    \n",
    "                    # Extract indicators\n",
    "                    indicators_used = set()\n",
    "                    for ind in valid_indicators:\n",
    "                        if ind in rule:\n",
    "                            indicators_used.add(ind)\n",
    "                    \n",
    "                    candidates.append({\n",
    "                        'rule': rule,\n",
    "                        'fitness': particle.best_fitness,\n",
    "                        'indicators': indicators_used,\n",
    "                        'position': particle.best_position.copy()\n",
    "                    })\n",
    "        \n",
    "        # Merge with existing archive\n",
    "        all_candidates = self.diversity_archive + candidates\n",
    "        \n",
    "        # Sort by fitness\n",
    "        all_candidates.sort(key=lambda x: x['fitness'], reverse=True)\n",
    "        \n",
    "        # Select diverse subset\n",
    "        selected = []\n",
    "        for candidate in all_candidates:\n",
    "            is_diverse = True\n",
    "            \n",
    "            for selected_item in selected:\n",
    "                # Check indicator overlap\n",
    "                overlap = len(candidate['indicators'] & selected_item['indicators'])\n",
    "                max_indicators = max(len(candidate['indicators']), \n",
    "                                   len(selected_item['indicators']))\n",
    "                \n",
    "                if max_indicators > 0:\n",
    "                    similarity = overlap / max_indicators\n",
    "                    if similarity > 0.7:  # Too similar\n",
    "                        is_diverse = False\n",
    "                        break\n",
    "            \n",
    "            if is_diverse:\n",
    "                selected.append(candidate)\n",
    "            \n",
    "            if len(selected) >= self.archive_size:\n",
    "                break\n",
    "        \n",
    "        self.diversity_archive = selected\n",
    "    \n",
    "    def _extract_final_diverse_rules(self, swarms, indicators_df, valid_indicators):\n",
    "        \"\"\"Extract final diverse rule set from archive and top particles\"\"\"\n",
    "        # Collect all candidates\n",
    "        all_candidates = list(self.diversity_archive)\n",
    "        \n",
    "        # Add top particles from each swarm\n",
    "        for swarm in swarms:\n",
    "            sorted_particles = sorted(swarm['particles'], \n",
    "                                    key=lambda p: p.best_fitness, reverse=True)\n",
    "            \n",
    "            for particle in sorted_particles[:5]:\n",
    "                if particle.best_fitness > 0.1:\n",
    "                    rule = self._decode_position(\n",
    "                        particle.best_position, indicators_df, valid_indicators\n",
    "                    )\n",
    "                    \n",
    "                    indicators_used = set()\n",
    "                    for ind in valid_indicators:\n",
    "                        if ind in rule:\n",
    "                            indicators_used.add(ind)\n",
    "                    \n",
    "                    all_candidates.append({\n",
    "                        'rule': rule,\n",
    "                        'fitness': particle.best_fitness,\n",
    "                        'indicators': indicators_used\n",
    "                    })\n",
    "        \n",
    "        # Remove duplicates and select diverse subset\n",
    "        all_candidates.sort(key=lambda x: x['fitness'], reverse=True)\n",
    "        \n",
    "        final_rules = []\n",
    "        final_fitness = []\n",
    "        seen_indicators = []\n",
    "        \n",
    "        for candidate in all_candidates:\n",
    "            is_diverse = True\n",
    "            \n",
    "            for seen_set in seen_indicators:\n",
    "                if candidate['indicators'] == seen_set:\n",
    "                    is_diverse = False\n",
    "                    break\n",
    "            \n",
    "            if is_diverse or len(final_rules) < 5:  # Always keep top 5\n",
    "                final_rules.append(candidate['rule'])\n",
    "                final_fitness.append(candidate['fitness'])\n",
    "                seen_indicators.append(candidate['indicators'])\n",
    "            \n",
    "            if len(final_rules) >= 30:  # Return more rules!\n",
    "                break\n",
    "        \n",
    "        return {\n",
    "            'rules': final_rules,\n",
    "            'fitness_scores': final_fitness,\n",
    "            'history': self.fitness_history,\n",
    "            'valid_rules_found': self.valid_rules_found,\n",
    "            'total_evaluations': self.total_evaluations,\n",
    "            'archive_size': len(self.diversity_archive)\n",
    "        }\n",
    "    \n",
    "    def _evaluate_particle_multiobjective(self, position: np.ndarray, \n",
    "                                         indicators_df: pd.DataFrame,\n",
    "                                         returns: pd.Series, \n",
    "                                         indicator_names: List[str]) -> float:\n",
    "        \"\"\"Multi-objective fitness: Sharpe + Win Rate + Signal Count + Uniqueness\"\"\"\n",
    "        try:\n",
    "            rule_condition = self._decode_position(position, indicators_df, indicator_names)\n",
    "            \n",
    "            parser = RobustConditionParser()\n",
    "            signals = parser.evaluate_condition(rule_condition, indicators_df)\n",
    "            \n",
    "            if signals.sum() < 3:\n",
    "                return -500\n",
    "            \n",
    "            # Align indices\n",
    "            common_idx = indicators_df.index.intersection(returns.index)\n",
    "            if len(common_idx) < 3:\n",
    "                return -450\n",
    "            \n",
    "            aligned_signals = signals[indicators_df.index.isin(common_idx)]\n",
    "            aligned_returns = returns.loc[common_idx]\n",
    "            \n",
    "            signal_returns = aligned_returns[aligned_signals].dropna()\n",
    "            \n",
    "            if len(signal_returns) < 3:\n",
    "                return -400\n",
    "            \n",
    "            # Calculate metrics\n",
    "            mean_return = signal_returns.mean()\n",
    "            if np.isnan(mean_return) or np.isinf(mean_return):\n",
    "                return -350\n",
    "            \n",
    "            std_return = signal_returns.std()\n",
    "            if std_return < 1e-8 or np.isnan(std_return):\n",
    "                std_return = 1.0\n",
    "            \n",
    "            sharpe = mean_return / std_return\n",
    "            win_rate = (signal_returns > 0).mean()\n",
    "            signal_count = signals.sum()\n",
    "            \n",
    "            # Multi-objective fitness with balanced weights\n",
    "            fitness_sharpe = sharpe * 0.35        # 35% - profitability\n",
    "            fitness_winrate = win_rate * 0.25     # 25% - consistency\n",
    "            fitness_signals = min(signal_count / 50, 1.0) * 0.20  # 20% - data sufficiency\n",
    "            fitness_return = (mean_return / 10) * 0.20  # 20% - absolute returns\n",
    "            \n",
    "            # Penalties\n",
    "            if signal_count < 10:\n",
    "                fitness_signals -= 0.3\n",
    "            if abs(sharpe) > 5:  # Extreme values\n",
    "                fitness_sharpe -= 0.5\n",
    "            \n",
    "            total_fitness = fitness_sharpe + fitness_winrate + fitness_signals + fitness_return\n",
    "            \n",
    "            return max(min(total_fitness, 5), -5)\n",
    "            \n",
    "        except Exception as e:\n",
    "            return -300\n",
    "    \n",
    "    def optimize_rule_parameters(self, \n",
    "                                indicators_df: pd.DataFrame,\n",
    "                                returns: pd.Series,\n",
    "                                indicator_names: List[str],\n",
    "                                progress_callback=None) -> Dict:\n",
    "        \"\"\"Optimize rule parameters using PSO - RETURN TOP RULES\"\"\"\n",
    "        \n",
    "        # Filter valid indicators first\n",
    "        valid_indicators = [ind for ind in indicator_names \n",
    "                          if ind in indicators_df.columns \n",
    "                          and len(indicators_df[ind].dropna()) >= 20]\n",
    "        \n",
    "        if len(valid_indicators) < 2:\n",
    "            return {\n",
    "                'rules': [],\n",
    "                'fitness_scores': [],\n",
    "                'history': [-999],\n",
    "                'valid_rules_found': 0,\n",
    "                'message': f\"Not enough valid indicators (found {len(valid_indicators)})\"\n",
    "            }\n",
    "        \n",
    "        # Reduce dimensions for better convergence - use 1-2 indicators\n",
    "        n_conditions = min(2, len(valid_indicators))\n",
    "        n_dims = n_conditions * 2  # indicator index + threshold for each condition\n",
    "        \n",
    "        particles = []\n",
    "        \n",
    "        # Initialize particles with MORE diverse positions\n",
    "        for i in range(self.n_particles):\n",
    "            # Use FIVE different initialization strategies for more diversity\n",
    "            strategy = i % 5\n",
    "            \n",
    "            if strategy == 0:\n",
    "                # Low percentiles (bearish)\n",
    "                position = np.random.uniform(0.0, 0.3, n_dims)\n",
    "            elif strategy == 1:\n",
    "                # Mid-low percentiles\n",
    "                position = np.random.uniform(0.2, 0.5, n_dims)\n",
    "            elif strategy == 2:\n",
    "                # Mid percentiles (neutral)\n",
    "                position = np.random.uniform(0.4, 0.6, n_dims)\n",
    "            elif strategy == 3:\n",
    "                # Mid-high percentiles\n",
    "                position = np.random.uniform(0.5, 0.8, n_dims)\n",
    "            else:\n",
    "                # High percentiles (bullish)\n",
    "                position = np.random.uniform(0.7, 1.0, n_dims)\n",
    "            \n",
    "            # Add small noise\n",
    "            position = np.clip(position + np.random.randn(n_dims) * 0.05, 0, 1)\n",
    "            velocity = np.random.randn(n_dims) * 0.05\n",
    "            \n",
    "            particle = Particle(\n",
    "                position=position,\n",
    "                velocity=velocity,\n",
    "                best_position=position.copy(),\n",
    "                best_fitness=-np.inf,\n",
    "                current_fitness=-np.inf\n",
    "            )\n",
    "            particles.append(particle)\n",
    "        \n",
    "        # Optimization loop\n",
    "        stagnation_counter = 0\n",
    "        best_fitness_history = []\n",
    "        \n",
    "        for iteration in range(self.n_iterations):\n",
    "            valid_count = 0\n",
    "            \n",
    "            for i, particle in enumerate(particles):\n",
    "                fitness = self._evaluate_particle(\n",
    "                    particle.position, indicators_df, returns, valid_indicators\n",
    "                )\n",
    "                \n",
    "                self.total_evaluations += 1\n",
    "                if fitness > -500:\n",
    "                    valid_count += 1\n",
    "                    self.valid_rules_found += 1\n",
    "                \n",
    "                particle.current_fitness = fitness\n",
    "                \n",
    "                if fitness > particle.best_fitness:\n",
    "                    particle.best_fitness = fitness\n",
    "                    particle.best_position = particle.position.copy()\n",
    "                \n",
    "                if fitness > self.global_best_fitness:\n",
    "                    self.global_best_fitness = fitness\n",
    "                    self.global_best_position = particle.position.copy()\n",
    "                    stagnation_counter = 0\n",
    "                else:\n",
    "                    stagnation_counter += 1\n",
    "            \n",
    "            # Update velocities and positions\n",
    "            for particle in particles:\n",
    "                r1, r2 = np.random.rand(2)\n",
    "                \n",
    "                cognitive = self.c1 * r1 * (particle.best_position - particle.position)\n",
    "                social = self.c2 * r2 * (self.global_best_position - particle.position)\n",
    "                \n",
    "                # Adaptive inertia\n",
    "                w_adaptive = self.w * (0.9 - 0.4 * iteration / self.n_iterations)\n",
    "                \n",
    "                particle.velocity = w_adaptive * particle.velocity + cognitive + social\n",
    "                \n",
    "                # Velocity clamping\n",
    "                v_max = 0.2\n",
    "                particle.velocity = np.clip(particle.velocity, -v_max, v_max)\n",
    "                \n",
    "                particle.position = particle.position + particle.velocity\n",
    "                particle.position = np.clip(particle.position, 0, 1)\n",
    "            \n",
    "            # Add diversity if stagnating\n",
    "            if stagnation_counter > 10 and iteration < self.n_iterations - 5:\n",
    "                # Reinitialize worst particles\n",
    "                worst_indices = np.argsort([p.best_fitness for p in particles])[:self.n_particles // 3]\n",
    "                for idx in worst_indices:\n",
    "                    particles[idx].position = np.random.rand(n_dims)\n",
    "                    particles[idx].velocity = np.random.randn(n_dims) * 0.05\n",
    "                stagnation_counter = 0\n",
    "            \n",
    "            self.fitness_history.append(self.global_best_fitness)\n",
    "            best_fitness_history.append(self.global_best_fitness)\n",
    "            \n",
    "            if progress_callback:\n",
    "                progress_callback(iteration + 1, self.n_iterations, \n",
    "                                self.global_best_fitness, valid_count)\n",
    "        \n",
    "        # Return TOP 10 DIVERSE rules instead of just 1\n",
    "        particles.sort(key=lambda p: p.best_fitness, reverse=True)\n",
    "        \n",
    "        top_rules = []\n",
    "        top_fitness = []\n",
    "        seen_indicators = []  # Track indicators used, not exact rules\n",
    "        \n",
    "        for particle in particles[:min(50, len(particles))]:  # Check top 50\n",
    "            # STRICTER: Only rules with positive fitness (actual edge)\n",
    "            if particle.best_fitness > 0.1:  # Must have some positive fitness\n",
    "                rule = self._decode_position(particle.best_position, indicators_df, valid_indicators)\n",
    "                \n",
    "                # Extract indicators from rule for diversity check\n",
    "                indicators_in_rule = set()\n",
    "                for ind in valid_indicators:\n",
    "                    if ind in rule:\n",
    "                        indicators_in_rule.add(ind)\n",
    "                \n",
    "                # Check if this indicator combination is new\n",
    "                is_diverse = True\n",
    "                for seen_set in seen_indicators:\n",
    "                    if indicators_in_rule == seen_set:\n",
    "                        is_diverse = False\n",
    "                        break\n",
    "                \n",
    "                if is_diverse or len(top_rules) < 3:  # Always allow first 3\n",
    "                    top_rules.append(rule)\n",
    "                    top_fitness.append(particle.best_fitness)\n",
    "                    seen_indicators.append(indicators_in_rule)\n",
    "                    \n",
    "                    if len(top_rules) >= 15:  # Collect more rules\n",
    "                        break\n",
    "        \n",
    "        return {\n",
    "            'rules': top_rules,\n",
    "            'fitness_scores': top_fitness,\n",
    "            'history': self.fitness_history,\n",
    "            'valid_rules_found': self.valid_rules_found,\n",
    "            'total_evaluations': self.total_evaluations,\n",
    "            'final_valid_rate': f\"{valid_count}/{self.n_particles}\"\n",
    "        }\n",
    "    \n",
    "    def _evaluate_particle(self, position: np.ndarray, indicators_df: pd.DataFrame,\n",
    "                          returns: pd.Series, indicator_names: List[str]) -> float:\n",
    "        \"\"\"Evaluate fitness - ANTI-OVERFITTING VERSION\"\"\"\n",
    "        try:\n",
    "            rule_condition = self._decode_position(position, indicators_df, indicator_names)\n",
    "            \n",
    "            parser = RobustConditionParser()\n",
    "            signals = parser.evaluate_condition(rule_condition, indicators_df)\n",
    "            \n",
    "            # Very lenient minimum\n",
    "            if signals.sum() < 3:\n",
    "                return -500\n",
    "            \n",
    "            # Align indices\n",
    "            common_idx = indicators_df.index.intersection(returns.index)\n",
    "            if len(common_idx) < 3:\n",
    "                return -450\n",
    "            \n",
    "            aligned_signals = signals[indicators_df.index.isin(common_idx)]\n",
    "            aligned_returns = returns.loc[common_idx]\n",
    "            \n",
    "            signal_returns = aligned_returns[aligned_signals].dropna()\n",
    "            \n",
    "            if len(signal_returns) < 3:\n",
    "                return -400\n",
    "            \n",
    "            # Calculate metrics\n",
    "            mean_return = signal_returns.mean()\n",
    "            \n",
    "            if np.isnan(mean_return) or np.isinf(mean_return):\n",
    "                return -350\n",
    "            \n",
    "            std_return = signal_returns.std()\n",
    "            if std_return < 1e-8 or np.isnan(std_return):\n",
    "                std_return = 1.0\n",
    "            \n",
    "            sharpe = mean_return / std_return\n",
    "            win_rate = (signal_returns > 0).mean()\n",
    "            \n",
    "            # Penalty for too few signals (overfitting indicator)\n",
    "            signal_penalty = 0\n",
    "            if signals.sum() < 10:\n",
    "                signal_penalty = -0.5\n",
    "            elif signals.sum() < 20:\n",
    "                signal_penalty = -0.2\n",
    "            \n",
    "            # Penalty for extreme Sharpe (likely overfitting)\n",
    "            extreme_penalty = 0\n",
    "            if abs(sharpe) > 5:\n",
    "                extreme_penalty = -1.0\n",
    "            elif abs(sharpe) > 3:\n",
    "                extreme_penalty = -0.5\n",
    "            \n",
    "            # Reward consistency and reasonable performance\n",
    "            base_fitness = sharpe * 0.4 + win_rate * 0.3 + (mean_return / 10) * 0.3\n",
    "            \n",
    "            # Apply penalties\n",
    "            fitness = base_fitness + signal_penalty + extreme_penalty\n",
    "            \n",
    "            # More moderate bounds\n",
    "            return max(min(fitness, 5), -5)\n",
    "            \n",
    "        except Exception as e:\n",
    "            return -300\n",
    "    \n",
    "    def _decode_position(self, position: np.ndarray, indicators_df: pd.DataFrame,\n",
    "                        indicator_names: List[str]) -> str:\n",
    "        \"\"\"Decode particle position to trading rule - DETERMINISTIC\"\"\"\n",
    "        n_conditions = len(position) // 2\n",
    "        conditions = []\n",
    "        \n",
    "        valid_indicators = [ind for ind in indicator_names \n",
    "                          if ind in indicators_df.columns \n",
    "                          and len(indicators_df[ind].dropna()) >= 10]\n",
    "        \n",
    "        if not valid_indicators:\n",
    "            return \"RSI_14 > 70\"\n",
    "        \n",
    "        # Use first dimension to determine number of conditions (deterministic)\n",
    "        if len(position) >= 2:\n",
    "            # If first position < 0.6, use 1 condition, else 2\n",
    "            if position[0] < 0.6:\n",
    "                n_conditions = 1\n",
    "            else:\n",
    "                n_conditions = min(2, len(position) // 2)\n",
    "        else:\n",
    "            n_conditions = 1\n",
    "        \n",
    "        for i in range(n_conditions):\n",
    "            idx_offset = i * 2\n",
    "            if idx_offset + 1 >= len(position):\n",
    "                break\n",
    "                \n",
    "            # Select indicator deterministically\n",
    "            ind_idx = int(position[idx_offset] * len(valid_indicators))\n",
    "            ind_idx = min(ind_idx, len(valid_indicators) - 1)\n",
    "            indicator = valid_indicators[ind_idx]\n",
    "            \n",
    "            # Select threshold using percentile\n",
    "            threshold_pct = position[idx_offset + 1]\n",
    "            \n",
    "            # Snap to common percentiles for stability\n",
    "            if threshold_pct < 0.25:\n",
    "                threshold_pct = 0.2\n",
    "            elif threshold_pct < 0.45:\n",
    "                threshold_pct = 0.3\n",
    "            elif threshold_pct < 0.55:\n",
    "                threshold_pct = 0.5\n",
    "            elif threshold_pct < 0.75:\n",
    "                threshold_pct = 0.7\n",
    "            else:\n",
    "                threshold_pct = 0.8\n",
    "            \n",
    "            ind_data = indicators_df[indicator].dropna()\n",
    "            \n",
    "            if len(ind_data) < 10:\n",
    "                continue  # Skip this indicator if insufficient data\n",
    "            \n",
    "            # Use percentile for threshold\n",
    "            threshold = ind_data.quantile(threshold_pct)\n",
    "            \n",
    "            # CORRECTED: ENFORCE BOUNDS for bounded indicators (STOCHRSI, RSI, etc.)\n",
    "            # This prevents impossible thresholds like STOCHRSI > 100\n",
    "            indicator_upper = indicator.upper()\n",
    "            if 'STOCHRSI' in indicator_upper or 'RSI' in indicator_upper or 'MFI' in indicator_upper:\n",
    "                # Bounded 0-100\n",
    "                threshold = np.clip(threshold, 0, 100)\n",
    "            elif 'STOCH' in indicator_upper and 'STOCHRSI' not in indicator_upper:\n",
    "                # Stochastic (not StochRSI) also 0-100\n",
    "                threshold = np.clip(threshold, 0, 100)\n",
    "            elif 'WILLR' in indicator_upper:\n",
    "                # Williams %R: -100 to 0\n",
    "                threshold = np.clip(threshold, -100, 0)\n",
    "            elif 'ADX' in indicator_upper or 'AROON' in indicator_upper:\n",
    "                # ADX and Aroon: 0-100\n",
    "                threshold = np.clip(threshold, 0, 100)\n",
    "            \n",
    "            # Ensure threshold is valid and reasonable\n",
    "            if np.isnan(threshold) or np.isinf(threshold):\n",
    "                threshold = ind_data.median()\n",
    "                if np.isnan(threshold) or np.isinf(threshold):\n",
    "                    continue  # Skip this indicator\n",
    "            \n",
    "            # Skip if indicator has no variance (all same value)\n",
    "            if ind_data.std() < 1e-10:\n",
    "                continue\n",
    "            \n",
    "            # Skip if threshold is unreasonably small (likely bad indicator)\n",
    "            if abs(threshold) < 1e-6 and ind_data.std() < 0.01:\n",
    "                continue\n",
    "            \n",
    "            # Choose operator based on threshold position\n",
    "            operator = '>' if threshold_pct > 0.5 else '<'\n",
    "            \n",
    "            # Format threshold properly with reasonable precision\n",
    "            if abs(threshold) < 0.01:\n",
    "                threshold_str = f\"{threshold:.6f}\"\n",
    "            elif abs(threshold) < 1:\n",
    "                threshold_str = f\"{threshold:.4f}\"\n",
    "            elif abs(threshold) < 100:\n",
    "                threshold_str = f\"{threshold:.2f}\"\n",
    "            else:\n",
    "                threshold_str = f\"{threshold:.1f}\"\n",
    "            \n",
    "            conditions.append(f\"{indicator} {operator} {threshold_str}\")\n",
    "        \n",
    "        if not conditions:\n",
    "            # Fallback\n",
    "            ind = valid_indicators[0]\n",
    "            threshold = indicators_df[ind].median()\n",
    "            return f\"{ind} > {threshold:.6f}\"\n",
    "        \n",
    "        # Return simple condition for better stability\n",
    "        if len(conditions) == 1:\n",
    "            return conditions[0]\n",
    "        else:\n",
    "            return f\"({conditions[0]}) AND ({conditions[1]})\"\n",
    "\n",
    "\n",
    "# ===================== GENETIC ALGORITHM =====================\n",
    "\n",
    "@dataclass\n",
    "class Chromosome:\n",
    "    \"\"\"Chromosome for genetic algorithm\"\"\"\n",
    "    genes: List[str]\n",
    "    fitness: float = -np.inf\n",
    "    \n",
    "class GeneticAlgorithm:\n",
    "    \"\"\"Genetic Algorithm for trading rule evolution - IMPROVED\"\"\"\n",
    "    \n",
    "    def __init__(self, population_size: int = 50, n_generations: int = 30,\n",
    "                 mutation_rate: float = 0.15, crossover_rate: float = 0.7):\n",
    "        self.population_size = population_size\n",
    "        self.n_generations = n_generations\n",
    "        self.mutation_rate = mutation_rate\n",
    "        self.crossover_rate = crossover_rate\n",
    "        self.fitness_history = []\n",
    "        self.best_chromosome = None\n",
    "        self.valid_rules_found = 0\n",
    "        self.total_evaluations = 0\n",
    "    \n",
    "    def evolve_rules(self, indicators_df: pd.DataFrame, returns: pd.Series,\n",
    "                    indicator_names: List[str], operators: List[str],\n",
    "                    percentiles: List[int], progress_callback=None) -> Dict:\n",
    "        \"\"\"Evolve trading rules using genetic algorithm - IMPROVED\"\"\"\n",
    "        \n",
    "        if not operators:\n",
    "            operators = ['>', '<']\n",
    "        if not percentiles:\n",
    "            percentiles = [10, 30, 50, 70, 90]\n",
    "        if not indicator_names:\n",
    "            raise ValueError(\"No indicators provided for genetic algorithm\")\n",
    "        \n",
    "        # Filter valid indicators\n",
    "        valid_indicators = [ind for ind in indicator_names \n",
    "                          if ind in indicators_df.columns \n",
    "                          and len(indicators_df[ind].dropna()) >= 10]\n",
    "        \n",
    "        if len(valid_indicators) < 2:\n",
    "            return {\n",
    "                'rule': \"RSI_14 > 70\",\n",
    "                'fitness': -999,\n",
    "                'history': [-999],\n",
    "                'valid_rules_found': 0,\n",
    "                'message': f\"Not enough valid indicators (found {len(valid_indicators)})\"\n",
    "            }\n",
    "        \n",
    "        try:\n",
    "            population = self._initialize_population(\n",
    "                indicators_df, valid_indicators, operators, percentiles\n",
    "            )\n",
    "        except ValueError as e:\n",
    "            return {\n",
    "                'rule': \"RSI_14 > 70\",\n",
    "                'fitness': -999,\n",
    "                'history': [-999],\n",
    "                'valid_rules_found': 0,\n",
    "                'message': str(e)\n",
    "            }\n",
    "        \n",
    "        # Evolution loop\n",
    "        for generation in range(self.n_generations):\n",
    "            valid_count = 0\n",
    "            \n",
    "            # Evaluate fitness\n",
    "            for chromosome in population:\n",
    "                if chromosome.fitness == -np.inf:\n",
    "                    self.total_evaluations += 1\n",
    "                    fitness = self._evaluate_fitness(chromosome, indicators_df, returns)\n",
    "                    chromosome.fitness = fitness\n",
    "                    \n",
    "                    if fitness > -500:\n",
    "                        valid_count += 1\n",
    "                        self.valid_rules_found += 1\n",
    "            \n",
    "            # Sort by fitness\n",
    "            population.sort(key=lambda x: x.fitness, reverse=True)\n",
    "            \n",
    "            # Update best\n",
    "            if population[0].fitness > (self.best_chromosome.fitness if self.best_chromosome else -np.inf):\n",
    "                self.best_chromosome = Chromosome(\n",
    "                    genes=population[0].genes.copy(),\n",
    "                    fitness=population[0].fitness\n",
    "                )\n",
    "            \n",
    "            self.fitness_history.append(population[0].fitness)\n",
    "            \n",
    "            # Elite selection - keep top 20%\n",
    "            elite_size = max(2, self.population_size // 5)\n",
    "            new_population = [Chromosome(genes=c.genes.copy(), fitness=c.fitness) \n",
    "                            for c in population[:elite_size]]\n",
    "            \n",
    "            # Generate offspring\n",
    "            attempts = 0\n",
    "            max_attempts = self.population_size * 10\n",
    "            \n",
    "            while len(new_population) < self.population_size and attempts < max_attempts:\n",
    "                attempts += 1\n",
    "                \n",
    "                parent1 = self._tournament_select(population, tournament_size=3)\n",
    "                parent2 = self._tournament_select(population, tournament_size=3)\n",
    "                \n",
    "                if np.random.rand() < self.crossover_rate:\n",
    "                    child = self._crossover(parent1, parent2)\n",
    "                else:\n",
    "                    child = Chromosome(genes=parent1.genes.copy())\n",
    "                \n",
    "                if np.random.rand() < self.mutation_rate:\n",
    "                    child = self._mutate(child, indicators_df, valid_indicators, \n",
    "                                       operators, percentiles)\n",
    "                \n",
    "                # Ensure child has genes\n",
    "                if child.genes:\n",
    "                    new_population.append(child)\n",
    "            \n",
    "            # Fill remaining with random if needed\n",
    "            while len(new_population) < self.population_size:\n",
    "                try:\n",
    "                    random_chromo = self._create_random_chromosome(\n",
    "                        indicators_df, valid_indicators, operators, percentiles\n",
    "                    )\n",
    "                    new_population.append(random_chromo)\n",
    "                except:\n",
    "                    break\n",
    "            \n",
    "            population = new_population[:self.population_size]\n",
    "            \n",
    "            if progress_callback:\n",
    "                progress_callback(generation + 1, self.n_generations, \n",
    "                                population[0].fitness, valid_count)\n",
    "        \n",
    "        if not self.best_chromosome or not self.best_chromosome.genes:\n",
    "            return {\n",
    "                'rules': [],\n",
    "                'fitness_scores': [],\n",
    "                'history': self.fitness_history,\n",
    "                'valid_rules_found': self.valid_rules_found,\n",
    "                'message': \"No valid rules evolved\"\n",
    "            }\n",
    "        \n",
    "        # Return TOP 10 DIVERSE rules\n",
    "        population.sort(key=lambda x: x.fitness, reverse=True)\n",
    "        \n",
    "        top_rules = []\n",
    "        top_fitness = []\n",
    "        seen_indicators = []  # Track indicators used\n",
    "        \n",
    "        for chromosome in population[:min(50, len(population))]:\n",
    "            # STRICTER: Only rules with positive fitness\n",
    "            if chromosome.fitness > 0.1 and chromosome.genes:\n",
    "                rule_str = \" AND \".join(f\"({g})\" for g in chromosome.genes)\n",
    "                \n",
    "                # Extract indicators from rule\n",
    "                indicators_in_rule = set()\n",
    "                for gene in chromosome.genes:\n",
    "                    # Extract indicator name from gene (before operator)\n",
    "                    parts = gene.split()\n",
    "                    if len(parts) >= 1:\n",
    "                        indicators_in_rule.add(parts[0])\n",
    "                \n",
    "                # Check if this indicator combination is new\n",
    "                is_diverse = True\n",
    "                for seen_set in seen_indicators:\n",
    "                    if indicators_in_rule == seen_set:\n",
    "                        is_diverse = False\n",
    "                        break\n",
    "                \n",
    "                if is_diverse or len(top_rules) < 3:  # Always allow first 3\n",
    "                    top_rules.append(rule_str)\n",
    "                    top_fitness.append(chromosome.fitness)\n",
    "                    seen_indicators.append(indicators_in_rule)\n",
    "                    \n",
    "                    if len(top_rules) >= 15:  # Collect more rules\n",
    "                        break\n",
    "        \n",
    "        return {\n",
    "            'rules': top_rules,\n",
    "            'fitness_scores': top_fitness,\n",
    "            'history': self.fitness_history,\n",
    "            'valid_rules_found': self.valid_rules_found,\n",
    "            'total_evaluations': self.total_evaluations\n",
    "        }\n",
    "    \n",
    "    def _create_random_chromosome(self, indicators_df: pd.DataFrame,\n",
    "                                 indicator_names: List[str], operators: List[str],\n",
    "                                 percentiles: List[int]) -> Chromosome:\n",
    "        \"\"\"Create a single random chromosome - PREFER SIMPLE RULES\"\"\"\n",
    "        # 70% chance of single condition, 30% chance of 2 conditions\n",
    "        if np.random.rand() < 0.7:\n",
    "            n_conditions = 1\n",
    "        else:\n",
    "            n_conditions = 2\n",
    "            \n",
    "        genes = []\n",
    "        \n",
    "        # Prefer common percentiles to avoid overfitting\n",
    "        common_percentiles = [20, 30, 50, 70, 80]\n",
    "        \n",
    "        for _ in range(n_conditions):\n",
    "            indicator = np.random.choice(indicator_names)\n",
    "            operator = np.random.choice(operators)\n",
    "            percentile = np.random.choice(common_percentiles if np.random.rand() < 0.7 else percentiles)\n",
    "            \n",
    "            ind_data = indicators_df[indicator].dropna()\n",
    "            \n",
    "            if len(ind_data) >= 10:\n",
    "                threshold = ind_data.quantile(percentile / 100)\n",
    "                \n",
    "                if not np.isnan(threshold) and not np.isinf(threshold):\n",
    "                    if abs(threshold) < 0.001:\n",
    "                        threshold_str = f\"{threshold:.8f}\"\n",
    "                    elif abs(threshold) < 1:\n",
    "                        threshold_str = f\"{threshold:.6f}\"\n",
    "                    else:\n",
    "                        threshold_str = f\"{threshold:.4f}\"\n",
    "                    \n",
    "                    genes.append(f\"{indicator} {operator} {threshold_str}\")\n",
    "        \n",
    "        if not genes:\n",
    "            # Fallback\n",
    "            ind = indicator_names[0]\n",
    "            threshold = indicators_df[ind].median()\n",
    "            genes = [f\"{ind} > {threshold:.6f}\"]\n",
    "        \n",
    "        return Chromosome(genes=genes)\n",
    "    \n",
    "    def _initialize_population(self, indicators_df: pd.DataFrame,\n",
    "                              indicator_names: List[str], operators: List[str],\n",
    "                              percentiles: List[int]) -> List[Chromosome]:\n",
    "        \"\"\"Initialize random population with HIGH DIVERSITY\"\"\"\n",
    "        population = []\n",
    "        \n",
    "        # Ensure we have variety in percentiles\n",
    "        common_percentiles = [20, 30, 50, 70, 80]\n",
    "        all_percentiles = percentiles\n",
    "        \n",
    "        attempts = 0\n",
    "        max_attempts = self.population_size * 10\n",
    "        \n",
    "        # Create population with forced diversity\n",
    "        for i in range(self.population_size):\n",
    "            if attempts >= max_attempts:\n",
    "                break\n",
    "            \n",
    "            attempts += 1\n",
    "            \n",
    "            try:\n",
    "                # Vary complexity: 50% single, 40% double, 10% triple\n",
    "                rand = np.random.rand()\n",
    "                if rand < 0.5:\n",
    "                    n_conditions = 1\n",
    "                elif rand < 0.9:\n",
    "                    n_conditions = 2\n",
    "                else:\n",
    "                    n_conditions = 3\n",
    "                \n",
    "                # Use different percentile strategies for diversity\n",
    "                if i % 3 == 0:\n",
    "                    # Extreme percentiles\n",
    "                    pcts = [10, 20, 80, 90]\n",
    "                elif i % 3 == 1:\n",
    "                    # Middle percentiles\n",
    "                    pcts = [30, 40, 50, 60, 70]\n",
    "                else:\n",
    "                    # All percentiles\n",
    "                    pcts = all_percentiles\n",
    "                \n",
    "                chromosome = self._create_random_chromosome_with_percentiles(\n",
    "                    indicators_df, indicator_names, operators, pcts, n_conditions\n",
    "                )\n",
    "                population.append(chromosome)\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        # Fill remaining with copies if needed\n",
    "        while len(population) < self.population_size:\n",
    "            idx = len(population) % len(population) if population else 0\n",
    "            if population:\n",
    "                population.append(Chromosome(genes=population[idx].genes.copy()))\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        return population\n",
    "    \n",
    "    def _create_random_chromosome_with_percentiles(self, indicators_df: pd.DataFrame,\n",
    "                                 indicator_names: List[str], operators: List[str],\n",
    "                                 percentiles: List[int], n_conditions: int) -> Chromosome:\n",
    "        \"\"\"Create chromosome with specific percentiles\"\"\"\n",
    "        genes = []\n",
    "        \n",
    "        for _ in range(n_conditions):\n",
    "            indicator = np.random.choice(indicator_names)\n",
    "            operator = np.random.choice(operators)\n",
    "            percentile = np.random.choice(percentiles)\n",
    "            \n",
    "            ind_data = indicators_df[indicator].dropna()\n",
    "            \n",
    "            if len(ind_data) >= 10:\n",
    "                threshold = ind_data.quantile(percentile / 100)\n",
    "                \n",
    "                if not np.isnan(threshold) and not np.isinf(threshold):\n",
    "                    if abs(threshold) < 0.001:\n",
    "                        threshold_str = f\"{threshold:.8f}\"\n",
    "                    elif abs(threshold) < 1:\n",
    "                        threshold_str = f\"{threshold:.6f}\"\n",
    "                    else:\n",
    "                        threshold_str = f\"{threshold:.4f}\"\n",
    "                    \n",
    "                    genes.append(f\"{indicator} {operator} {threshold_str}\")\n",
    "        \n",
    "        if not genes:\n",
    "            # Fallback\n",
    "            ind = indicator_names[0]\n",
    "            threshold = indicators_df[ind].median()\n",
    "            genes = [f\"{ind} > {threshold:.6f}\"]\n",
    "        \n",
    "        return Chromosome(genes=genes)\n",
    "    \n",
    "    def _evaluate_fitness(self, chromosome: Chromosome, indicators_df: pd.DataFrame,\n",
    "                         returns: pd.Series) -> float:\n",
    "        \"\"\"Evaluate fitness - ANTI-OVERFITTING VERSION\"\"\"\n",
    "        try:\n",
    "            rule = \" AND \".join(f\"({g})\" for g in chromosome.genes)\n",
    "            \n",
    "            parser = RobustConditionParser()\n",
    "            signals = parser.evaluate_condition(rule, indicators_df)\n",
    "            \n",
    "            # Very lenient minimum\n",
    "            if signals.sum() < 3:\n",
    "                return -500\n",
    "            \n",
    "            # Align indices\n",
    "            common_idx = indicators_df.index.intersection(returns.index)\n",
    "            if len(common_idx) < 3:\n",
    "                return -450\n",
    "            \n",
    "            aligned_signals = signals[indicators_df.index.isin(common_idx)]\n",
    "            aligned_returns = returns.loc[common_idx]\n",
    "            \n",
    "            signal_returns = aligned_returns[aligned_signals].dropna()\n",
    "            \n",
    "            if len(signal_returns) < 3:\n",
    "                return -400\n",
    "            \n",
    "            # Calculate metrics\n",
    "            mean_return = signal_returns.mean()\n",
    "            \n",
    "            if np.isnan(mean_return) or np.isinf(mean_return):\n",
    "                return -350\n",
    "            \n",
    "            std_return = signal_returns.std()\n",
    "            if std_return < 1e-8 or np.isnan(std_return):\n",
    "                std_return = 1.0\n",
    "            \n",
    "            sharpe = mean_return / std_return\n",
    "            win_rate = (signal_returns > 0).mean()\n",
    "            \n",
    "            # Penalty for too few signals (overfitting indicator)\n",
    "            signal_penalty = 0\n",
    "            if signals.sum() < 10:\n",
    "                signal_penalty = -0.5\n",
    "            elif signals.sum() < 20:\n",
    "                signal_penalty = -0.2\n",
    "            \n",
    "            # Penalty for extreme Sharpe (likely overfitting)\n",
    "            extreme_penalty = 0\n",
    "            if abs(sharpe) > 5:\n",
    "                extreme_penalty = -1.0\n",
    "            elif abs(sharpe) > 3:\n",
    "                extreme_penalty = -0.5\n",
    "            \n",
    "            # Penalty for complex rules (more conditions = more overfitting)\n",
    "            complexity_penalty = -0.1 * (len(chromosome.genes) - 1)\n",
    "            \n",
    "            # Reward consistency and reasonable performance\n",
    "            base_fitness = sharpe * 0.4 + win_rate * 0.3 + (mean_return / 10) * 0.3\n",
    "            \n",
    "            # Apply penalties\n",
    "            fitness = base_fitness + signal_penalty + extreme_penalty + complexity_penalty\n",
    "            \n",
    "            # More moderate bounds\n",
    "            return max(min(fitness, 5), -5)\n",
    "            \n",
    "        except Exception as e:\n",
    "            return -300\n",
    "    \n",
    "    def _tournament_select(self, population: List[Chromosome], \n",
    "                          tournament_size: int = 3) -> Chromosome:\n",
    "        \"\"\"Tournament selection\"\"\"\n",
    "        if len(population) == 0:\n",
    "            raise ValueError(\"Cannot select from empty population\")\n",
    "        \n",
    "        actual_tournament_size = min(tournament_size, len(population))\n",
    "        \n",
    "        tournament = np.random.choice(population, actual_tournament_size, replace=False)\n",
    "        return max(tournament, key=lambda x: x.fitness)\n",
    "    \n",
    "    def _crossover(self, parent1: Chromosome, parent2: Chromosome) -> Chromosome:\n",
    "        \"\"\"Single-point crossover\"\"\"\n",
    "        if not parent1.genes or not parent2.genes:\n",
    "            return Chromosome(genes=parent1.genes if parent1.genes else parent2.genes)\n",
    "        \n",
    "        if len(parent1.genes) == 1 and len(parent2.genes) == 1:\n",
    "            return Chromosome(genes=parent1.genes if np.random.rand() > 0.5 else parent2.genes)\n",
    "        \n",
    "        min_len = min(len(parent1.genes), len(parent2.genes))\n",
    "        if min_len <= 1:\n",
    "            return Chromosome(genes=parent1.genes + parent2.genes)\n",
    "        \n",
    "        split = np.random.randint(1, min_len)\n",
    "        child_genes = parent1.genes[:split] + parent2.genes[split:]\n",
    "        return Chromosome(genes=child_genes)\n",
    "    \n",
    "    def _mutate(self, chromosome: Chromosome, indicators_df: pd.DataFrame,\n",
    "               indicator_names: List[str], operators: List[str],\n",
    "               percentiles: List[int]) -> Chromosome:\n",
    "        \"\"\"Mutate chromosome\"\"\"\n",
    "        if not chromosome.genes:\n",
    "            indicator = np.random.choice([ind for ind in indicator_names \n",
    "                                         if ind in indicators_df.columns])\n",
    "            if indicator in indicators_df.columns:\n",
    "                operator = np.random.choice(operators)\n",
    "                percentile = np.random.choice(percentiles)\n",
    "                ind_data = indicators_df[indicator].dropna()\n",
    "                \n",
    "                if len(ind_data) >= 10:\n",
    "                    threshold = ind_data.quantile(percentile / 100)\n",
    "                    chromosome.genes.append(f\"{indicator} {operator} {threshold:.6f}\")\n",
    "            \n",
    "            chromosome.fitness = -np.inf\n",
    "            return chromosome\n",
    "        \n",
    "        mutation_type = np.random.choice(['change', 'add', 'remove'])\n",
    "        \n",
    "        if mutation_type == 'change':\n",
    "            idx = np.random.randint(len(chromosome.genes))\n",
    "            valid_indicators = [ind for ind in indicator_names \n",
    "                              if ind in indicators_df.columns]\n",
    "            \n",
    "            if valid_indicators:\n",
    "                indicator = np.random.choice(valid_indicators)\n",
    "                operator = np.random.choice(operators)\n",
    "                percentile = np.random.choice(percentiles)\n",
    "                ind_data = indicators_df[indicator].dropna()\n",
    "                \n",
    "                if len(ind_data) >= 10:\n",
    "                    threshold = ind_data.quantile(percentile / 100)\n",
    "                    chromosome.genes[idx] = f\"{indicator} {operator} {threshold:.6f}\"\n",
    "        \n",
    "        elif mutation_type == 'add' and len(chromosome.genes) < 3:\n",
    "            valid_indicators = [ind for ind in indicator_names \n",
    "                              if ind in indicators_df.columns]\n",
    "            \n",
    "            if valid_indicators:\n",
    "                indicator = np.random.choice(valid_indicators)\n",
    "                operator = np.random.choice(operators)\n",
    "                percentile = np.random.choice(percentiles)\n",
    "                ind_data = indicators_df[indicator].dropna()\n",
    "                \n",
    "                if len(ind_data) >= 10:\n",
    "                    threshold = ind_data.quantile(percentile / 100)\n",
    "                    chromosome.genes.append(f\"{indicator} {operator} {threshold:.6f}\")\n",
    "        \n",
    "        elif mutation_type == 'remove' and len(chromosome.genes) > 1:\n",
    "            idx = np.random.randint(len(chromosome.genes))\n",
    "            chromosome.genes.pop(idx)\n",
    "        \n",
    "        chromosome.fitness = -np.inf\n",
    "        return chromosome\n",
    "\n",
    "\n",
    "# ===================== RULE PARSING AND VALIDATION =====================\n",
    "\n",
    "class RobustConditionParser:\n",
    "    \"\"\"Enhanced parser for trading rule conditions\"\"\"\n",
    "    \n",
    "    COMPARISON_OPS = {\n",
    "        '>': lambda a, b: a > b,\n",
    "        '>=': lambda a, b: a >= b,\n",
    "        '<': lambda a, b: a < b,\n",
    "        '<=': lambda a, b: a <= b,\n",
    "        '==': lambda a, b: a == b,\n",
    "        '!=': lambda a, b: a != b\n",
    "    }\n",
    "    \n",
    "    LOGICAL_OPS = {\n",
    "        'AND': lambda a, b: a & b,\n",
    "        'OR': lambda a, b: a | b\n",
    "    }\n",
    "    \n",
    "    @classmethod\n",
    "    def tokenize_condition(cls, condition: str) -> List[Tuple[str, str]]:\n",
    "        \"\"\"Tokenize a condition string\"\"\"\n",
    "        condition = condition.strip()\n",
    "        if condition.startswith('(') and condition.endswith(')'):\n",
    "            condition = condition[1:-1]\n",
    "        \n",
    "        tokens = []\n",
    "        logical_positions = []\n",
    "        for op in ['AND', 'OR']:\n",
    "            pattern = rf'\\s+{op}\\s+'\n",
    "            for match in re.finditer(pattern, condition):\n",
    "                logical_positions.append((match.start(), match.end(), op))\n",
    "        \n",
    "        logical_positions.sort()\n",
    "        \n",
    "        last_pos = 0\n",
    "        for start, end, op in logical_positions:\n",
    "            sub_condition = condition[last_pos:start].strip()\n",
    "            if sub_condition:\n",
    "                sub_condition = sub_condition.strip('()')\n",
    "                tokens.append(('CONDITION', sub_condition))\n",
    "            tokens.append(('LOGICAL', op))\n",
    "            last_pos = end\n",
    "        \n",
    "        remaining = condition[last_pos:].strip()\n",
    "        if remaining:\n",
    "            remaining = remaining.strip('()')\n",
    "            tokens.append(('CONDITION', remaining))\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    @classmethod\n",
    "    def parse_simple_condition(cls, condition: str, data_df: pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"Parse a simple condition\"\"\"\n",
    "        try:\n",
    "            pattern = r'^([A-Za-z_][A-Za-z0-9_]*)\\s*([><=!]+)\\s*(-?\\d+\\.?\\d*)$'\n",
    "            match = re.match(pattern, condition.strip())\n",
    "            \n",
    "            if not match:\n",
    "                return np.zeros(len(data_df), dtype=bool)\n",
    "            \n",
    "            indicator, operator, value = match.groups()\n",
    "            \n",
    "            if operator not in cls.COMPARISON_OPS or indicator not in data_df.columns:\n",
    "                return np.zeros(len(data_df), dtype=bool)\n",
    "            \n",
    "            indicator_values = data_df[indicator].values\n",
    "            threshold = float(value)\n",
    "            \n",
    "            valid_mask = ~np.isnan(indicator_values)\n",
    "            result = np.zeros(len(data_df), dtype=bool)\n",
    "            \n",
    "            comparison_func = cls.COMPARISON_OPS[operator]\n",
    "            result[valid_mask] = comparison_func(indicator_values[valid_mask], threshold)\n",
    "            \n",
    "            return result\n",
    "        except:\n",
    "            return np.zeros(len(data_df), dtype=bool)\n",
    "    \n",
    "    @classmethod\n",
    "    def evaluate_condition(cls, condition: str, data_df: pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"Evaluate complete condition\"\"\"\n",
    "        try:\n",
    "            tokens = cls.tokenize_condition(condition)\n",
    "            \n",
    "            if not tokens:\n",
    "                return np.zeros(len(data_df), dtype=bool)\n",
    "            \n",
    "            if len(tokens) == 1 and tokens[0][0] == 'CONDITION':\n",
    "                return cls.parse_simple_condition(tokens[0][1], data_df)\n",
    "            \n",
    "            result = None\n",
    "            pending_logical_op = None\n",
    "            \n",
    "            for token_type, value in tokens:\n",
    "                if token_type == 'CONDITION':\n",
    "                    condition_result = cls.parse_simple_condition(value, data_df)\n",
    "                    \n",
    "                    if result is None:\n",
    "                        result = condition_result\n",
    "                    elif pending_logical_op:\n",
    "                        logical_func = cls.LOGICAL_OPS[pending_logical_op]\n",
    "                        result = logical_func(result, condition_result)\n",
    "                        pending_logical_op = None\n",
    "                \n",
    "                elif token_type == 'LOGICAL':\n",
    "                    pending_logical_op = value\n",
    "            \n",
    "            return result if result is not None else np.zeros(len(data_df), dtype=bool)\n",
    "        except:\n",
    "            return np.zeros(len(data_df), dtype=bool)\n",
    "    \n",
    "    @classmethod\n",
    "    def validate_condition_syntax(cls, condition: str, available_indicators: List[str]) -> Tuple[bool, str]:\n",
    "        \"\"\"Validate condition syntax and REJECT impossible values\"\"\"\n",
    "        try:\n",
    "            if condition.count('(') != condition.count(')'):\n",
    "                return False, \"Unbalanced parentheses\"\n",
    "            \n",
    "            tokens = cls.tokenize_condition(condition)\n",
    "            \n",
    "            if not tokens:\n",
    "                return False, \"Empty condition\"\n",
    "            \n",
    "            for token_type, value in tokens:\n",
    "                if token_type == 'CONDITION':\n",
    "                    pattern = r'^([A-Za-z_][A-Za-z0-9_]*)\\s*([><=!]+)\\s*(-?\\d+\\.?\\d*)$'\n",
    "                    match = re.match(pattern, value.strip())\n",
    "                    \n",
    "                    if not match:\n",
    "                        return False, f\"Invalid condition format: {value}\"\n",
    "                    \n",
    "                    indicator, operator, threshold_str = match.groups()\n",
    "                    \n",
    "                    if indicator not in available_indicators:\n",
    "                        return False, f\"Unknown indicator: {indicator}\"\n",
    "                    \n",
    "                    if operator not in cls.COMPARISON_OPS:\n",
    "                        return False, f\"Invalid operator: {operator}\"\n",
    "                    \n",
    "                    # CORRECTED: REJECT impossible threshold values for bounded indicators\n",
    "                    try:\n",
    "                        threshold = float(threshold_str)\n",
    "                        indicator_upper = indicator.upper()\n",
    "                        \n",
    "                        # Check STOCHRSI first (before RSI)\n",
    "                        if 'STOCHRSI' in indicator_upper:\n",
    "                            if operator in ['>', '>='] and threshold > 100:\n",
    "                                return False, f\"IMPOSSIBLE: {indicator} {operator} {threshold} (max 100)\"\n",
    "                            if operator in ['<', '<='] and threshold < 0:\n",
    "                                return False, f\"IMPOSSIBLE: {indicator} {operator} {threshold} (min 0)\"\n",
    "                        \n",
    "                        # RSI, MFI, STOCH (0-100)\n",
    "                        elif any(x in indicator_upper for x in ['RSI', 'MFI']) or \\\n",
    "                             ('STOCH' in indicator_upper and 'STOCHRSI' not in indicator_upper):\n",
    "                            if operator in ['>', '>='] and threshold > 100:\n",
    "                                return False, f\"IMPOSSIBLE: {indicator} {operator} {threshold} (max 100)\"\n",
    "                            if operator in ['<', '<='] and threshold < 0:\n",
    "                                return False, f\"IMPOSSIBLE: {indicator} {operator} {threshold} (min 0)\"\n",
    "                        \n",
    "                        # Williams %R (-100 to 0)\n",
    "                        elif 'WILLR' in indicator_upper:\n",
    "                            if operator in ['>', '>='] and threshold > 0:\n",
    "                                return False, f\"IMPOSSIBLE: {indicator} {operator} {threshold} (max 0)\"\n",
    "                            if operator in ['<', '<='] and threshold < -100:\n",
    "                                return False, f\"IMPOSSIBLE: {indicator} {operator} {threshold} (min -100)\"\n",
    "                        \n",
    "                        # ADX, AROON (0-100)\n",
    "                        elif any(x in indicator_upper for x in ['ADX', 'AROON']):\n",
    "                            if operator in ['>', '>='] and threshold > 100:\n",
    "                                return False, f\"IMPOSSIBLE: {indicator} {operator} {threshold} (max 100)\"\n",
    "                            if operator in ['<', '<='] and threshold < 0:\n",
    "                                return False, f\"IMPOSSIBLE: {indicator} {operator} {threshold} (min 0)\"\n",
    "                        \n",
    "                        # Reject excessive decimals (likely overfitting)\n",
    "                        if '.' in threshold_str and len(threshold_str.split('.')[1]) > 4:\n",
    "                            return False, f\"Too many decimals: {threshold_str} (max 4)\"\n",
    "                    \n",
    "                    except ValueError:\n",
    "                        return False, f\"Invalid threshold: {threshold_str}\"\n",
    "                \n",
    "                elif token_type == 'LOGICAL':\n",
    "                    if value not in cls.LOGICAL_OPS:\n",
    "                        return False, f\"Invalid logical operator: {value}\"\n",
    "            \n",
    "            return True, \"Valid\"\n",
    "        except Exception as e:\n",
    "            return False, f\"Validation error: {str(e)}\"\n",
    "\n",
    "\n",
    "class StatisticalValidator:\n",
    "    \"\"\"Statistical significance testing\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_significance(signal_returns: pd.Series, \n",
    "                             confidence_level: float = 0.95) -> Dict:\n",
    "        \"\"\"Calculate statistical significance\"\"\"\n",
    "        if len(signal_returns) < 5:\n",
    "            return {\n",
    "                'p_value': 1.0,\n",
    "                'is_significant': False,\n",
    "                'ci_lower': 0,\n",
    "                'ci_upper': 0,\n",
    "                't_statistic': 0\n",
    "            }\n",
    "        \n",
    "        t_stat, p_value = stats.ttest_1samp(signal_returns, 0)\n",
    "        \n",
    "        alpha = 1 - confidence_level\n",
    "        ci = stats.t.interval(confidence_level, len(signal_returns)-1,\n",
    "                            loc=signal_returns.mean(),\n",
    "                            scale=stats.sem(signal_returns))\n",
    "        \n",
    "        return {\n",
    "            'p_value': p_value,\n",
    "            'is_significant': p_value < 0.05,\n",
    "            'ci_lower': ci[0],\n",
    "            'ci_upper': ci[1],\n",
    "            't_statistic': t_stat\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def apply_multiple_testing_correction(p_values: np.ndarray, \n",
    "                                         alpha: float = 0.05,\n",
    "                                         method: str = 'bonferroni') -> np.ndarray:\n",
    "        \"\"\"Apply multiple testing correction\"\"\"\n",
    "        n_tests = len(p_values)\n",
    "        \n",
    "        if method == 'bonferroni':\n",
    "            corrected_alpha = alpha / n_tests\n",
    "            return p_values < corrected_alpha\n",
    "        \n",
    "        elif method == 'fdr':\n",
    "            sorted_idx = np.argsort(p_values)\n",
    "            sorted_p = p_values[sorted_idx]\n",
    "            \n",
    "            comparisons = sorted_p <= (np.arange(1, n_tests + 1) / n_tests) * alpha\n",
    "            \n",
    "            if comparisons.any():\n",
    "                max_idx = np.where(comparisons)[0][-1]\n",
    "                threshold = sorted_p[max_idx]\n",
    "                return p_values <= threshold\n",
    "            else:\n",
    "                return np.zeros(n_tests, dtype=bool)\n",
    "        \n",
    "        return np.zeros(n_tests, dtype=bool)\n",
    "\n",
    "\n",
    "class ImprovedMassiveRuleGenerator:\n",
    "    \"\"\"\n",
    "    IMPROVED Brute Force Rule Generator\n",
    "    - Detects indicator types (binary, bounded, oscillator, continuous)\n",
    "    - Generates meaningful thresholds based on technical analysis standards\n",
    "    - Eliminates duplicates with similarity check\n",
    "    - Validates rules before accepting\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.accepted_rules = []\n",
    "        self.indicator_types = {}\n",
    "    \n",
    "    def _get_indicator_type(self, ind_data: pd.Series, ind_name: str) -> str:\n",
    "        \"\"\"\n",
    "        Classify indicator type to generate appropriate thresholds\n",
    "        \n",
    "        Types:\n",
    "        - binary: Candlestick patterns (0, Â±100)\n",
    "        - bounded: RSI, Stochastic (0-100)\n",
    "        - oscillator: MACD, CCI (-âˆž to +âˆž, crosses zero)\n",
    "        - continuous: Moving averages, price-based\n",
    "        \"\"\"\n",
    "        unique_vals = ind_data.nunique()\n",
    "        data_range = ind_data.max() - ind_data.min()\n",
    "        min_val = ind_data.min()\n",
    "        max_val = ind_data.max()\n",
    "        \n",
    "        # Binary (Candlestick patterns)\n",
    "        if unique_vals <= 5 and data_range <= 200:\n",
    "            return 'binary'\n",
    "        \n",
    "        # CORRECTED: Bounded (RSI, Stochastic, etc.) - check STOCHRSI FIRST before generic patterns\n",
    "        # This prevents STOCHRSI from being misclassified\n",
    "        if 'STOCHRSI' in ind_name.upper():\n",
    "            # StochRSI is ALWAYS bounded 0-100\n",
    "            return 'bounded'\n",
    "        elif any(x in ind_name.upper() for x in ['RSI', 'STOCH', 'WILLR', 'MFI', 'ADX', 'AROON']):\n",
    "            if 0 <= min_val and max_val <= 100:\n",
    "                return 'bounded'\n",
    "        \n",
    "        # Oscillator (MACD, CCI, etc.) - crosses zero\n",
    "        if min_val < 0 and max_val > 0:\n",
    "            if any(x in ind_name.upper() for x in ['MACD', 'CCI', 'MOM', 'ROC', 'PPO', 'BOP']):\n",
    "                return 'oscillator'\n",
    "        \n",
    "        # Continuous (everything else)\n",
    "        return 'continuous'\n",
    "    \n",
    "    def _get_meaningful_thresholds(self, ind_data: pd.Series, ind_name: str, \n",
    "                                   ind_type: str) -> List[Tuple[str, float]]:\n",
    "        \"\"\"\n",
    "        Generate meaningful thresholds based on indicator type and TA standards\n",
    "        \n",
    "        Returns: List of (operator, threshold) tuples\n",
    "        \"\"\"\n",
    "        thresholds = []\n",
    "        \n",
    "        if ind_type == 'binary':\n",
    "            # For candlestick patterns: only check if pattern exists\n",
    "            # Pattern values are 0, 100, or -100\n",
    "            thresholds = [\n",
    "                ('!=', 0),      # Pattern present (any direction)\n",
    "                ('>', 0),       # Bullish pattern\n",
    "                ('<', 0),       # Bearish pattern\n",
    "            ]\n",
    "        \n",
    "        elif ind_type == 'bounded':\n",
    "            # For RSI, Stochastic, etc. - use standard TA levels\n",
    "            # CORRECTED: Handle STOCHRSI explicitly (0-100 range)\n",
    "            if 'STOCHRSI' in ind_name.upper():\n",
    "                thresholds = [\n",
    "                    ('<', 20),   # Extreme oversold\n",
    "                    ('<', 30),   # Oversold\n",
    "                    ('>', 70),   # Overbought\n",
    "                    ('>', 80),   # Extreme overbought\n",
    "                ]\n",
    "            elif 'RSI' in ind_name.upper():\n",
    "                thresholds = [\n",
    "                    ('<', 20),   # Extreme oversold\n",
    "                    ('<', 30),   # Oversold (standard)\n",
    "                    ('>', 70),   # Overbought (standard)\n",
    "                    ('>', 80),   # Extreme overbought\n",
    "                ]\n",
    "            elif 'STOCH' in ind_name.upper():\n",
    "                thresholds = [\n",
    "                    ('<', 20),   # Oversold\n",
    "                    ('<', 30),\n",
    "                    ('>', 70),   # Overbought\n",
    "                    ('>', 80),\n",
    "                ]\n",
    "            elif 'WILLR' in ind_name.upper():  # Williams %R (inverted, -100 to 0)\n",
    "                thresholds = [\n",
    "                    ('<', -80),  # Oversold\n",
    "                    ('<', -70),\n",
    "                    ('>', -30),  # Overbought\n",
    "                    ('>', -20),\n",
    "                ]\n",
    "            elif 'ADX' in ind_name.upper():  # Trend strength\n",
    "                thresholds = [\n",
    "                    ('>', 20),   # Trend starting\n",
    "                    ('>', 25),   # Strong trend\n",
    "                    ('>', 30),\n",
    "                    ('>', 40),   # Very strong trend\n",
    "                ]\n",
    "            elif 'MFI' in ind_name.upper():  # Money Flow Index\n",
    "                thresholds = [\n",
    "                    ('<', 20),   # Oversold\n",
    "                    ('<', 30),\n",
    "                    ('>', 70),   # Overbought\n",
    "                    ('>', 80),\n",
    "                ]\n",
    "            else:\n",
    "                # Generic bounded indicator\n",
    "                thresholds = [\n",
    "                    ('<', 30),\n",
    "                    ('>', 70),\n",
    "                ]\n",
    "        \n",
    "        elif ind_type == 'oscillator':\n",
    "            # For MACD, CCI, etc. - use percentiles but round to reasonable values\n",
    "            data_clean = ind_data.dropna()\n",
    "            if len(data_clean) < 50:\n",
    "                return []\n",
    "            \n",
    "            # Use standard deviations or percentiles\n",
    "            std_val = data_clean.std()\n",
    "            mean_val = data_clean.mean()\n",
    "            \n",
    "            if 'MACD' in ind_name.upper():\n",
    "                # MACD: use small multiples of std\n",
    "                thresholds = [\n",
    "                    ('>', std_val * 0.5),\n",
    "                    ('>', std_val * 1.0),\n",
    "                    ('<', -std_val * 0.5),\n",
    "                    ('<', -std_val * 1.0),\n",
    "                ]\n",
    "            elif 'CCI' in ind_name.upper():\n",
    "                # CCI: standard levels are Â±100, Â±200\n",
    "                thresholds = [\n",
    "                    ('>', 100),\n",
    "                    ('>', 200),\n",
    "                    ('<', -100),\n",
    "                    ('<', -200),\n",
    "                ]\n",
    "            else:\n",
    "                # Generic oscillator\n",
    "                p10 = data_clean.quantile(0.10)\n",
    "                p25 = data_clean.quantile(0.25)\n",
    "                p75 = data_clean.quantile(0.75)\n",
    "                p90 = data_clean.quantile(0.90)\n",
    "                \n",
    "                thresholds = [\n",
    "                    ('>', p75),\n",
    "                    ('>', p90),\n",
    "                    ('<', p25),\n",
    "                    ('<', p10),\n",
    "                ]\n",
    "        \n",
    "        else:  # continuous\n",
    "            # For price-based indicators (SMA, EMA, etc.)\n",
    "            # Use percentiles but with appropriate precision\n",
    "            data_clean = ind_data.dropna()\n",
    "            if len(data_clean) < 50:\n",
    "                return []\n",
    "            \n",
    "            # Calculate percentiles\n",
    "            p10 = data_clean.quantile(0.10)\n",
    "            p20 = data_clean.quantile(0.20)\n",
    "            p30 = data_clean.quantile(0.30)\n",
    "            p70 = data_clean.quantile(0.70)\n",
    "            p80 = data_clean.quantile(0.80)\n",
    "            p90 = data_clean.quantile(0.90)\n",
    "            \n",
    "            # Determine appropriate rounding based on magnitude\n",
    "            magnitude = abs(data_clean.mean())\n",
    "            if magnitude > 1000:\n",
    "                decimals = 0\n",
    "            elif magnitude > 100:\n",
    "                decimals = 1\n",
    "            elif magnitude > 10:\n",
    "                decimals = 2\n",
    "            else:\n",
    "                decimals = 3\n",
    "            \n",
    "            thresholds = [\n",
    "                ('<', round(p10, decimals)),\n",
    "                ('<', round(p20, decimals)),\n",
    "                ('<', round(p30, decimals)),\n",
    "                ('>', round(p70, decimals)),\n",
    "                ('>', round(p80, decimals)),\n",
    "                ('>', round(p90, decimals)),\n",
    "            ]\n",
    "        \n",
    "        # Filter out any thresholds that don't make sense\n",
    "        valid_thresholds = []\n",
    "        for op, thresh in thresholds:\n",
    "            # Skip if threshold is extreme or nonsensical\n",
    "            if ind_type == 'bounded':\n",
    "                if thresh < -10 or thresh > 110:\n",
    "                    continue\n",
    "            valid_thresholds.append((op, thresh))\n",
    "        \n",
    "        return valid_thresholds\n",
    "    \n",
    "    def _is_trivial_rule(self, condition: str, ind_data: pd.Series, ind_type: str) -> bool:\n",
    "        \"\"\"\n",
    "        Check if rule is trivial (always true or always false)\n",
    "        \"\"\"\n",
    "        # Check for \"0.000000\" in patterns\n",
    "        if ind_type == 'binary' and '0.000000' in condition:\n",
    "            return True\n",
    "        \n",
    "        # Check for impossible thresholds in bounded indicators\n",
    "        if ind_type == 'bounded':\n",
    "            if '> 100' in condition or '< 0' in condition:\n",
    "                return True\n",
    "            # Check for thresholds beyond reasonable range\n",
    "            import re\n",
    "            numbers = re.findall(r'[<>]=?\\s*([-+]?\\d+\\.?\\d*)', condition)\n",
    "            for num_str in numbers:\n",
    "                num = float(num_str)\n",
    "                if num > 100 or num < 0:\n",
    "                    return True\n",
    "        \n",
    "        # Check for overly precise thresholds (likely overfit)\n",
    "        if '.' in condition:\n",
    "            import re\n",
    "            decimals = re.findall(r'\\d+\\.(\\d+)', condition)\n",
    "            for dec in decimals:\n",
    "                if len(dec) > 4:  # More than 4 decimal places\n",
    "                    return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def _calculate_rule_similarity(self, rule1: Dict, rule2: Dict) -> float:\n",
    "        \"\"\"\n",
    "        Calculate similarity between two rules\n",
    "        Returns: similarity score (0-1)\n",
    "        \"\"\"\n",
    "        # Extract indicators from conditions\n",
    "        import re\n",
    "        \n",
    "        def extract_indicators(condition):\n",
    "            # Remove operators and numbers, keep only indicator names\n",
    "            cleaned = re.sub(r'[<>=!]+\\s*[-+]?\\d+\\.?\\d*', '', condition)\n",
    "            cleaned = re.sub(r'[()]+', '', cleaned)\n",
    "            cleaned = re.sub(r'\\s+(AND|OR)\\s+', ' ', cleaned)\n",
    "            return set(cleaned.split())\n",
    "        \n",
    "        ind1 = extract_indicators(rule1['condition'])\n",
    "        ind2 = extract_indicators(rule2['condition'])\n",
    "        \n",
    "        if len(ind1) == 0 or len(ind2) == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        # Calculate Jaccard similarity\n",
    "        intersection = len(ind1 & ind2)\n",
    "        union = len(ind1 | ind2)\n",
    "        \n",
    "        return intersection / union if union > 0 else 0.0\n",
    "    \n",
    "    def _is_duplicate(self, new_rule: Dict, similarity_threshold: float = 0.7) -> bool:\n",
    "        \"\"\"\n",
    "        Check if rule is too similar to existing rules\n",
    "        \"\"\"\n",
    "        for existing_rule in self.accepted_rules:\n",
    "            similarity = self._calculate_rule_similarity(new_rule, existing_rule)\n",
    "            if similarity > similarity_threshold:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def generate_simple_rules(self,\n",
    "                             indicators_df: pd.DataFrame,\n",
    "                             percentiles: List[int],  # Now ignored - we use smart thresholds\n",
    "                             operators: List[str],     # Now ignored - determined by type\n",
    "                             max_indicators: int = None) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Generate SMART simple rules with meaningful thresholds\n",
    "        \"\"\"\n",
    "        rules = []\n",
    "        self.accepted_rules = []  # Reset\n",
    "        \n",
    "        columns = indicators_df.columns[:max_indicators] if max_indicators else indicators_df.columns\n",
    "        \n",
    "        print(f\"\\nðŸ” Generating smart rules for {len(columns)} indicators...\")\n",
    "        \n",
    "        for idx, col in enumerate(columns):\n",
    "            data = indicators_df[col].dropna()\n",
    "            if len(data) < 50:\n",
    "                continue\n",
    "            \n",
    "            # Classify indicator type\n",
    "            ind_type = self._get_indicator_type(data, col)\n",
    "            self.indicator_types[col] = ind_type\n",
    "            \n",
    "            # Get meaningful thresholds for this type\n",
    "            thresholds = self._get_meaningful_thresholds(data, col, ind_type)\n",
    "            \n",
    "            if not thresholds:\n",
    "                continue\n",
    "            \n",
    "            # Generate rules\n",
    "            for op, threshold in thresholds:\n",
    "                # Format threshold appropriately\n",
    "                if ind_type == 'binary' or (ind_type == 'bounded' and threshold in [0, 20, 30, 70, 80, 100]):\n",
    "                    # Use integers for standard levels\n",
    "                    threshold_str = f\"{int(threshold)}\"\n",
    "                elif ind_type == 'continuous':\n",
    "                    # Already rounded in _get_meaningful_thresholds\n",
    "                    if abs(threshold) > 100:\n",
    "                        threshold_str = f\"{threshold:.1f}\"\n",
    "                    elif abs(threshold) > 10:\n",
    "                        threshold_str = f\"{threshold:.2f}\"\n",
    "                    else:\n",
    "                        threshold_str = f\"{threshold:.3f}\"\n",
    "                else:\n",
    "                    threshold_str = f\"{threshold:.2f}\"\n",
    "                \n",
    "                condition = f\"{col} {op} {threshold_str}\"\n",
    "                \n",
    "                # Check if trivial\n",
    "                if self._is_trivial_rule(condition, data, ind_type):\n",
    "                    continue\n",
    "                \n",
    "                # Determine trade type based on operator and indicator type\n",
    "                if ind_type == 'binary':\n",
    "                    if op == '>':\n",
    "                        trade_type = 'BUY'\n",
    "                    elif op == '<':\n",
    "                        trade_type = 'SELL'\n",
    "                    else:  # !=\n",
    "                        trade_type = 'BUY'  # Default\n",
    "                elif ind_type in ['bounded', 'oscillator']:\n",
    "                    if op in ['<', '<=']:\n",
    "                        trade_type = 'BUY'  # Low values = oversold = buy\n",
    "                    else:\n",
    "                        trade_type = 'SELL'  # High values = overbought = sell\n",
    "                else:  # continuous\n",
    "                    # For price-based indicators, low = buy, high = sell\n",
    "                    if op in ['<', '<=']:\n",
    "                        trade_type = 'BUY'\n",
    "                    else:\n",
    "                        trade_type = 'SELL'\n",
    "                \n",
    "                rule = {\n",
    "                    'condition': condition,\n",
    "                    'indicator': col,\n",
    "                    'operator': op,\n",
    "                    'threshold': threshold,\n",
    "                    'threshold_str': threshold_str,\n",
    "                    'type': 'simple',\n",
    "                    'trade_type': trade_type,\n",
    "                    'indicator_type': ind_type\n",
    "                }\n",
    "                \n",
    "                # Check for duplicates\n",
    "                if not self._is_duplicate(rule, similarity_threshold=0.9):  # Very strict for simple rules\n",
    "                    rules.append(rule)\n",
    "                    self.accepted_rules.append(rule)\n",
    "            \n",
    "            # Progress update every 20 indicators\n",
    "            if (idx + 1) % 20 == 0:\n",
    "                print(f\"  âœ“ Processed {idx + 1}/{len(columns)} indicators, {len(rules)} rules generated\")\n",
    "        \n",
    "        print(f\"âœ… Generated {len(rules)} SMART simple rules (no duplicates, no trivial rules)\\n\")\n",
    "        \n",
    "        # Print statistics\n",
    "        type_counts = {}\n",
    "        for rule in rules:\n",
    "            ind_type = rule.get('indicator_type', 'unknown')\n",
    "            type_counts[ind_type] = type_counts.get(ind_type, 0) + 1\n",
    "        \n",
    "        print(\"ðŸ“Š Rules by indicator type:\")\n",
    "        for ind_type, count in type_counts.items():\n",
    "            print(f\"  â€¢ {ind_type}: {count} rules\")\n",
    "        print()\n",
    "        \n",
    "        return rules\n",
    "    \n",
    "    def generate_compound_rules(self,\n",
    "                               simple_rules: List[Dict],\n",
    "                               max_depth: int = 2,\n",
    "                               max_combinations: int = 10000) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Generate SMART compound rules (avoiding similar indicator combinations)\n",
    "        \"\"\"\n",
    "        compound_rules = []\n",
    "        \n",
    "        # Separate by trade type\n",
    "        buy_rules = [r for r in simple_rules if r['trade_type'] == 'BUY']\n",
    "        sell_rules = [r for r in simple_rules if r['trade_type'] == 'SELL']\n",
    "        \n",
    "        print(f\"ðŸ”— Generating compound rules from {len(buy_rules)} BUY and {len(sell_rules)} SELL rules...\")\n",
    "        \n",
    "        # Sample if too many rules\n",
    "        if len(buy_rules) > 150:\n",
    "            buy_rules = np.random.choice(buy_rules, 150, replace=False).tolist()\n",
    "        if len(sell_rules) > 150:\n",
    "            sell_rules = np.random.choice(sell_rules, 150, replace=False).tolist()\n",
    "        \n",
    "        attempts = 0\n",
    "        max_attempts = max_combinations * 3\n",
    "        \n",
    "        # Generate BUY compounds\n",
    "        for r1, r2 in combinations(buy_rules, 2):\n",
    "            attempts += 1\n",
    "            if attempts > max_attempts:\n",
    "                break\n",
    "            \n",
    "            # Skip if same indicator\n",
    "            if r1['indicator'] == r2['indicator']:\n",
    "                continue\n",
    "            \n",
    "            # Create AND rule\n",
    "            and_rule = {\n",
    "                'condition': f\"({r1['condition']}) AND ({r2['condition']})\",\n",
    "                'type': 'compound-2',\n",
    "                'logic': 'AND',\n",
    "                'trade_type': 'BUY',\n",
    "                'indicators': [r1['indicator'], r2['indicator']],\n",
    "                'indicator_types': [r1.get('indicator_type', 'unknown'), \n",
    "                                   r2.get('indicator_type', 'unknown')]\n",
    "            }\n",
    "            \n",
    "            # Check duplicates\n",
    "            if not self._is_duplicate(and_rule, similarity_threshold=0.7):\n",
    "                compound_rules.append(and_rule)\n",
    "                self.accepted_rules.append(and_rule)\n",
    "            \n",
    "            if len(compound_rules) >= max_combinations // 2:\n",
    "                break\n",
    "            \n",
    "            # Create OR rule\n",
    "            or_rule = {\n",
    "                'condition': f\"({r1['condition']}) OR ({r2['condition']})\",\n",
    "                'type': 'compound-2',\n",
    "                'logic': 'OR',\n",
    "                'trade_type': 'BUY',\n",
    "                'indicators': [r1['indicator'], r2['indicator']],\n",
    "                'indicator_types': [r1.get('indicator_type', 'unknown'), \n",
    "                                   r2.get('indicator_type', 'unknown')]\n",
    "            }\n",
    "            \n",
    "            if not self._is_duplicate(or_rule, similarity_threshold=0.7):\n",
    "                compound_rules.append(or_rule)\n",
    "                self.accepted_rules.append(or_rule)\n",
    "            \n",
    "            if len(compound_rules) >= max_combinations:\n",
    "                break\n",
    "        \n",
    "        # Generate SELL compounds\n",
    "        for r1, r2 in combinations(sell_rules, 2):\n",
    "            if len(compound_rules) >= max_combinations:\n",
    "                break\n",
    "            \n",
    "            if r1['indicator'] == r2['indicator']:\n",
    "                continue\n",
    "            \n",
    "            and_rule = {\n",
    "                'condition': f\"({r1['condition']}) AND ({r2['condition']})\",\n",
    "                'type': 'compound-2',\n",
    "                'logic': 'AND',\n",
    "                'trade_type': 'SELL',\n",
    "                'indicators': [r1['indicator'], r2['indicator']],\n",
    "                'indicator_types': [r1.get('indicator_type', 'unknown'), \n",
    "                                   r2.get('indicator_type', 'unknown')]\n",
    "            }\n",
    "            \n",
    "            if not self._is_duplicate(and_rule, similarity_threshold=0.7):\n",
    "                compound_rules.append(and_rule)\n",
    "                self.accepted_rules.append(and_rule)\n",
    "            \n",
    "            if len(compound_rules) >= max_combinations:\n",
    "                break\n",
    "            \n",
    "            or_rule = {\n",
    "                'condition': f\"({r1['condition']}) OR ({r2['condition']})\",\n",
    "                'type': 'compound-2',\n",
    "                'logic': 'OR',\n",
    "                'trade_type': 'SELL',\n",
    "                'indicators': [r1['indicator'], r2['indicator']],\n",
    "                'indicator_types': [r1.get('indicator_type', 'unknown'), \n",
    "                                   r2.get('indicator_type', 'unknown')]\n",
    "            }\n",
    "            \n",
    "            if not self._is_duplicate(or_rule, similarity_threshold=0.7):\n",
    "                compound_rules.append(or_rule)\n",
    "                self.accepted_rules.append(or_rule)\n",
    "        \n",
    "        print(f\"âœ… Generated {len(compound_rules)} SMART compound rules (no duplicates)\\n\")\n",
    "        \n",
    "        return compound_rules[:max_combinations]\n",
    "\n",
    "\n",
    "class EnhancedComprehensiveValidator:\n",
    "    \"\"\"Enhanced validator WITHOUT look-ahead bias\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.parser = RobustConditionParser()\n",
    "        self.stat_validator = StatisticalValidator()\n",
    "    \n",
    "    def validate_single_rule(self,\n",
    "                           rule: Dict,\n",
    "                           is_indicators: pd.DataFrame,\n",
    "                           is_data: pd.DataFrame,\n",
    "                           oos_indicators: pd.DataFrame,\n",
    "                           oos_data: pd.DataFrame,\n",
    "                           holding_period: int = 5,\n",
    "                           transaction_cost: float = 0.001) -> Dict:\n",
    "        \"\"\"Validate a single rule - REJECT GARBAGE RULES\"\"\"\n",
    "        \n",
    "        is_valid, error_msg = self.parser.validate_condition_syntax(\n",
    "            rule['condition'], is_indicators.columns.tolist()\n",
    "        )\n",
    "        \n",
    "        if not is_valid:\n",
    "            return None\n",
    "        \n",
    "        is_eval = is_indicators.copy()\n",
    "        oos_eval = oos_indicators.copy()\n",
    "        \n",
    "        is_signals = self.parser.evaluate_condition(rule['condition'], is_eval)\n",
    "        oos_signals = self.parser.evaluate_condition(rule['condition'], oos_eval)\n",
    "        \n",
    "        # CRITICAL: Reject rules with too few signals IMMEDIATELY\n",
    "        if is_signals.sum() < 5:\n",
    "            return None\n",
    "        if oos_signals.sum() < 3:\n",
    "            return None\n",
    "        \n",
    "        is_returns = is_data['Close'].pct_change(holding_period).shift(-holding_period) * 100\n",
    "        oos_returns = oos_data['Close'].pct_change(holding_period).shift(-holding_period) * 100\n",
    "        \n",
    "        is_metrics = self._calculate_metrics(is_signals, is_returns, \n",
    "                                            rule.get('trade_type', 'BUY'),\n",
    "                                            holding_period, transaction_cost)\n",
    "        oos_metrics = self._calculate_metrics(oos_signals, oos_returns,\n",
    "                                             rule.get('trade_type', 'BUY'),\n",
    "                                             holding_period, transaction_cost)\n",
    "        \n",
    "        # REJECT if metrics show zero activity\n",
    "        if is_metrics['signals'] == 0 or oos_metrics['signals'] == 0:\n",
    "            return None\n",
    "        \n",
    "        # REJECT if both Sharpe ratios are near zero (useless rule)\n",
    "        if abs(is_metrics['sharpe']) < 0.01 and abs(oos_metrics['sharpe']) < 0.01:\n",
    "            return None\n",
    "        \n",
    "        return {\n",
    "            'rule': rule['condition'],\n",
    "            'type': rule.get('type', 'simple'),\n",
    "            'trade_type': rule.get('trade_type', 'UNKNOWN'),\n",
    "            **{f'IS_{k}': v for k, v in is_metrics.items()},\n",
    "            **{f'OOS_{k}': v for k, v in oos_metrics.items()},\n",
    "            'sharpe_degradation': (oos_metrics['sharpe'] - is_metrics['sharpe']) if is_metrics['sharpe'] != 0 else -999,\n",
    "            'consistency_score': self._calculate_consistency(is_metrics, oos_metrics)\n",
    "        }\n",
    "    \n",
    "    def validate_single_rule_multiperiod(self,\n",
    "                                        rule: Dict,\n",
    "                                        is_indicators: pd.DataFrame,\n",
    "                                        is_data: pd.DataFrame,\n",
    "                                        oos_indicators: pd.DataFrame,\n",
    "                                        oos_data: pd.DataFrame,\n",
    "                                        holding_periods: List[int],\n",
    "                                        transaction_cost: float = 0.001) -> Dict:\n",
    "        \"\"\"\n",
    "        Validate a rule across multiple holding periods and return the best one.\n",
    "        Returns the result for the period with highest OOS Sharpe ratio.\n",
    "        \"\"\"\n",
    "        best_result = None\n",
    "        best_oos_sharpe = -999\n",
    "        best_period = None\n",
    "        \n",
    "        for period in holding_periods:\n",
    "            result = self.validate_single_rule(\n",
    "                rule, is_indicators, is_data, oos_indicators, oos_data,\n",
    "                period, transaction_cost\n",
    "            )\n",
    "            \n",
    "            if result is not None:\n",
    "                oos_sharpe = result.get('OOS_sharpe', -999)\n",
    "                # Track best OOS Sharpe (most important for robustness)\n",
    "                if oos_sharpe > best_oos_sharpe:\n",
    "                    best_oos_sharpe = oos_sharpe\n",
    "                    best_result = result\n",
    "                    best_period = period\n",
    "        \n",
    "        # Add the best holding period to the result\n",
    "        if best_result is not None:\n",
    "            best_result['best_holding_period'] = best_period\n",
    "        \n",
    "        return best_result\n",
    "    \n",
    "    def validate_rules_batch(self,\n",
    "                           rules: List[Dict],\n",
    "                           is_indicators: pd.DataFrame,\n",
    "                           is_data: pd.DataFrame,\n",
    "                           oos_indicators: pd.DataFrame,\n",
    "                           oos_data: pd.DataFrame,\n",
    "                           holding_period = 5,  # Can be int or List[int]\n",
    "                           transaction_cost: float = 0.001,\n",
    "                           batch_size: int = 100) -> pd.DataFrame:\n",
    "        \"\"\"Validate multiple rules - supports single or multiple holding periods\"\"\"\n",
    "        \n",
    "        results = []\n",
    "        total = len(rules)\n",
    "        \n",
    "        progress_bar = st.progress(0)\n",
    "        status_text = st.empty()\n",
    "        \n",
    "        # Check if testing multiple periods\n",
    "        is_multi_period = isinstance(holding_period, list)\n",
    "        \n",
    "        for i in range(0, total, batch_size):\n",
    "            batch = rules[i:i + batch_size]\n",
    "            progress = min((i + batch_size) / total, 1.0)\n",
    "            progress_bar.progress(progress)\n",
    "            \n",
    "            if is_multi_period:\n",
    "                status_text.text(f\"Testing rules {i+1} to {min(i+batch_size, total)} of {total} across {len(holding_period)} periods\")\n",
    "            else:\n",
    "                status_text.text(f\"Validating rules {i+1} to {min(i+batch_size, total)} of {total}\")\n",
    "            \n",
    "            for rule in batch:\n",
    "                if is_multi_period:\n",
    "                    # Test across multiple periods and get best\n",
    "                    result = self.validate_single_rule_multiperiod(\n",
    "                        rule, is_indicators, is_data, oos_indicators, oos_data,\n",
    "                        holding_period, transaction_cost\n",
    "                    )\n",
    "                else:\n",
    "                    # Single period validation\n",
    "                    result = self.validate_single_rule(\n",
    "                        rule, is_indicators, is_data, oos_indicators, oos_data,\n",
    "                        holding_period, transaction_cost\n",
    "                    )\n",
    "                \n",
    "                if result is not None:\n",
    "                    results.append(result)\n",
    "        \n",
    "        progress_bar.empty()\n",
    "        status_text.empty()\n",
    "        \n",
    "        results_df = pd.DataFrame(results)\n",
    "        \n",
    "        if len(results_df) > 0 and 'IS_p_value' in results_df.columns:\n",
    "            results_df['fdr_significant'] = self.stat_validator.apply_multiple_testing_correction(\n",
    "                results_df['IS_p_value'].values, method='fdr'\n",
    "            )\n",
    "        \n",
    "        return results_df\n",
    "    \n",
    "    def _calculate_metrics(self, signals: np.ndarray, returns: pd.Series,\n",
    "                          trade_type: str, holding_period: int,\n",
    "                          transaction_cost: float = 0.001) -> Dict:\n",
    "        \"\"\"Calculate metrics\"\"\"\n",
    "        \n",
    "        if signals.sum() < 5:\n",
    "            return {\n",
    "                'signals': 0, 'mean_return': 0, 'total_return': 0,\n",
    "                'sharpe': 0, 'sortino': 0, 'profit_factor': 0,\n",
    "                'win_rate': 0, 'max_dd': 0, 'p_value': 1.0,\n",
    "                'is_significant': False\n",
    "            }\n",
    "        \n",
    "        signal_returns = returns[signals].dropna()\n",
    "        \n",
    "        if trade_type == 'SELL':\n",
    "            signal_returns = -signal_returns\n",
    "        \n",
    "        signal_returns = signal_returns - (2 * transaction_cost * 100)\n",
    "        \n",
    "        if len(signal_returns) < 5:\n",
    "            return {\n",
    "                'signals': int(signals.sum()), 'mean_return': 0,\n",
    "                'total_return': 0, 'sharpe': 0, 'sortino': 0,\n",
    "                'profit_factor': 0, 'win_rate': 0, 'max_dd': 0,\n",
    "                'p_value': 1.0, 'is_significant': False\n",
    "            }\n",
    "        \n",
    "        mean_return = signal_returns.mean()\n",
    "        std_return = signal_returns.std()\n",
    "        downside_std = signal_returns[signal_returns < 0].std() if len(signal_returns[signal_returns < 0]) > 0 else 1e-8\n",
    "        \n",
    "        cumulative = (1 + signal_returns / 100).cumprod()\n",
    "        peak = cumulative.cummax()\n",
    "        drawdown = ((cumulative - peak) / peak * 100)\n",
    "        max_dd = drawdown.min()\n",
    "        \n",
    "        positive = signal_returns[signal_returns > 0].sum()\n",
    "        negative = -signal_returns[signal_returns < 0].sum()\n",
    "        profit_factor = positive / negative if negative > 0 else 999\n",
    "        \n",
    "        sig_results = self.stat_validator.calculate_significance(signal_returns)\n",
    "        \n",
    "        annual_factor = np.sqrt(252 / holding_period)\n",
    "        \n",
    "        return {\n",
    "            'signals': int(signals.sum()),\n",
    "            'mean_return': mean_return,\n",
    "            'total_return': signal_returns.sum(),\n",
    "            'sharpe': (mean_return / (std_return + 1e-8)) * annual_factor,\n",
    "            'sortino': (mean_return / (downside_std + 1e-8)) * annual_factor,\n",
    "            'profit_factor': min(profit_factor, 999),\n",
    "            'win_rate': (signal_returns > 0).mean() * 100,\n",
    "            'max_dd': max_dd,\n",
    "            'calmar': abs(signal_returns.sum() / max_dd) if max_dd != 0 else 0,\n",
    "            'p_value': sig_results['p_value'],\n",
    "            'is_significant': sig_results['is_significant'],\n",
    "            't_statistic': sig_results['t_statistic']\n",
    "        }\n",
    "    \n",
    "    def _calculate_consistency(self, is_metrics: Dict, oos_metrics: Dict) -> float:\n",
    "        \"\"\"Calculate consistency score\"\"\"\n",
    "        if is_metrics['signals'] == 0 or oos_metrics['signals'] == 0:\n",
    "            return 0\n",
    "        \n",
    "        sharpe_consistency = 1 - abs(is_metrics['sharpe'] - oos_metrics['sharpe']) / (abs(is_metrics['sharpe']) + 1)\n",
    "        pf_consistency = 1 - abs(is_metrics['profit_factor'] - oos_metrics['profit_factor']) / (is_metrics['profit_factor'] + 1)\n",
    "        wr_consistency = 1 - abs(is_metrics['win_rate'] - oos_metrics['win_rate']) / 100\n",
    "        \n",
    "        consistency = (sharpe_consistency * 0.4 + pf_consistency * 0.3 + wr_consistency * 0.3)\n",
    "        return max(0, min(1, consistency))\n",
    "\n",
    "\n",
    "class RuleSelector:\n",
    "    \"\"\"Select best rules\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def filter_robust_rules(results_df: pd.DataFrame,\n",
    "                          min_is_sharpe: float = 0.5,\n",
    "                          min_oos_sharpe: float = 0.0,\n",
    "                          max_degradation: float = 1.0,\n",
    "                          min_consistency: float = 0.3,\n",
    "                          min_signals_is: int = 20,\n",
    "                          min_signals_oos: int = 10) -> pd.DataFrame:\n",
    "        \"\"\"Filter robust rules\"\"\"\n",
    "        \n",
    "        filtered = results_df[\n",
    "            (results_df['IS_sharpe'] >= min_is_sharpe) &\n",
    "            (results_df['OOS_sharpe'] >= min_oos_sharpe) &\n",
    "            (results_df['sharpe_degradation'].abs() <= max_degradation) &\n",
    "            (results_df['consistency_score'] >= min_consistency) &\n",
    "            (results_df['IS_signals'] >= min_signals_is) &\n",
    "            (results_df['OOS_signals'] >= min_signals_oos)\n",
    "        ]\n",
    "        \n",
    "        return filtered.sort_values('consistency_score', ascending=False)\n",
    "    \n",
    "    @staticmethod\n",
    "    def select_diverse_rules(filtered_df: pd.DataFrame, n_rules: int = 10) -> pd.DataFrame:\n",
    "        \"\"\"Select diverse rules\"\"\"\n",
    "        \n",
    "        if len(filtered_df) <= n_rules:\n",
    "            return filtered_df\n",
    "        \n",
    "        selected_indices = []\n",
    "        \n",
    "        top_consistency = filtered_df.nlargest(n_rules // 3, 'consistency_score').index\n",
    "        selected_indices.extend(top_consistency)\n",
    "        \n",
    "        top_oos = filtered_df.nlargest(n_rules // 3, 'OOS_sharpe').index\n",
    "        selected_indices.extend(top_oos)\n",
    "        \n",
    "        top_pf = filtered_df.nlargest(n_rules // 3, 'OOS_profit_factor').index\n",
    "        selected_indices.extend(top_pf)\n",
    "        \n",
    "        unique_indices = list(dict.fromkeys(selected_indices))[:n_rules]\n",
    "        \n",
    "        return filtered_df.loc[unique_indices]\n",
    "\n",
    "\n",
    "# ===================== VISUALIZATION FUNCTIONS =====================\n",
    "\n",
    "def create_optimization_convergence_plot(history: List[float], title: str) -> go.Figure:\n",
    "    \"\"\"Create convergence plot for optimization\"\"\"\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        y=history,\n",
    "        mode='lines+markers',\n",
    "        line=dict(color='#667eea', width=3),\n",
    "        marker=dict(size=6, color='#764ba2'),\n",
    "        name='Best Fitness'\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        xaxis_title='Iteration',\n",
    "        yaxis_title='Fitness Score',\n",
    "        template='plotly_dark',\n",
    "        height=400,\n",
    "        paper_bgcolor='#0D1117',\n",
    "        plot_bgcolor='#161B22',\n",
    "        font=dict(color='#C9D1D9', family='Inter')\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "def create_performance_comparison_chart(results_df: pd.DataFrame) -> go.Figure:\n",
    "    \"\"\"Create IS vs OOS performance comparison\"\"\"\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    top_rules = results_df.nlargest(20, 'consistency_score')\n",
    "    \n",
    "    fig.add_trace(go.Bar(\n",
    "        name='In-Sample Sharpe',\n",
    "        x=list(range(len(top_rules))),\n",
    "        y=top_rules['IS_sharpe'].values,\n",
    "        marker=dict(color='#4ade80'),\n",
    "    ))\n",
    "    \n",
    "    fig.add_trace(go.Bar(\n",
    "        name='Out-of-Sample Sharpe',\n",
    "        x=list(range(len(top_rules))),\n",
    "        y=top_rules['OOS_sharpe'].values,\n",
    "        marker=dict(color='#f59e0b'),\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title='Top 20 Rules: IS vs OOS Sharpe Ratio',\n",
    "        xaxis_title='Rule Rank',\n",
    "        yaxis_title='Sharpe Ratio',\n",
    "        template='plotly_dark',\n",
    "        height=500,\n",
    "        paper_bgcolor='#0D1117',\n",
    "        plot_bgcolor='#161B22',\n",
    "        font=dict(color='#C9D1D9', family='Inter'),\n",
    "        barmode='group'\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "def create_scatter_performance(results_df: pd.DataFrame) -> go.Figure:\n",
    "    \"\"\"Create IS vs OOS scatter plot\"\"\"\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=results_df['IS_sharpe'],\n",
    "        y=results_df['OOS_sharpe'],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=8,\n",
    "            color=results_df['consistency_score'],\n",
    "            colorscale='Viridis',\n",
    "            showscale=True,\n",
    "            colorbar=dict(title=\"Consistency\"),\n",
    "            line=dict(width=1, color='white')\n",
    "        ),\n",
    "        text=[f\"Rule: {r[:50]}...\" for r in results_df['rule']],\n",
    "        hovertemplate='<b>%{text}</b><br>IS Sharpe: %{x:.2f}<br>OOS Sharpe: %{y:.2f}<extra></extra>'\n",
    "    ))\n",
    "    \n",
    "    max_val = max(results_df['IS_sharpe'].max(), results_df['OOS_sharpe'].max())\n",
    "    min_val = min(results_df['IS_sharpe'].min(), results_df['OOS_sharpe'].min())\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=[min_val, max_val],\n",
    "        y=[min_val, max_val],\n",
    "        mode='lines',\n",
    "        line=dict(color='red', width=2, dash='dash'),\n",
    "        name='Perfect Consistency',\n",
    "        showlegend=True\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title='In-Sample vs Out-of-Sample Performance',\n",
    "        xaxis_title='In-Sample Sharpe',\n",
    "        yaxis_title='Out-of-Sample Sharpe',\n",
    "        template='plotly_dark',\n",
    "        height=600,\n",
    "        paper_bgcolor='#0D1117',\n",
    "        plot_bgcolor='#161B22',\n",
    "        font=dict(color='#C9D1D9', family='Inter')\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "# ===================== MAIN APPLICATION =====================\n",
    "\n",
    "def main():\n",
    "    st.markdown(f\"\"\"\n",
    "        <h1 class='main-header'>Advanced Quantitative Analysis Platform</h1>\n",
    "        <p class='sub-header'>\n",
    "            {TechnicalIndicators.get_total_count()} TECHNICAL INDICATORS Â· PSO & GENETIC ALGORITHMS Â· IS/OOS VALIDATION\n",
    "        </p>\n",
    "    \"\"\", unsafe_allow_html=True)\n",
    "    \n",
    "    # ===================== COMPREHENSIVE SESSION STATE INITIALIZATION =====================\n",
    "    # Initialize ALL session state variables at once to prevent resets\n",
    "    \n",
    "    if 'initialized' not in st.session_state:\n",
    "        st.session_state.initialized = True\n",
    "        \n",
    "        # Basic analysis state\n",
    "        st.session_state.analysis_done = False\n",
    "        st.session_state.returns_data = None\n",
    "        st.session_state.indicators = None\n",
    "        st.session_state.data = None\n",
    "        st.session_state.summary = None\n",
    "        st.session_state.periods_to_test = None\n",
    "        st.session_state.selected_categories = None\n",
    "        \n",
    "        # Advanced analysis state\n",
    "        st.session_state.advanced_analysis_done = False\n",
    "        st.session_state.advanced_results = None\n",
    "        st.session_state.optimization_histories = None\n",
    "        \n",
    "        # Configuration state - preserve user inputs\n",
    "        st.session_state.ticker_input = \"SPY\"\n",
    "        st.session_state.period_input = \"max\"\n",
    "        st.session_state.last_tab = \"ðŸ“Š ANALYSIS\"\n",
    "    \n",
    "    # Restore or initialize config values\n",
    "    if 'ticker_input' not in st.session_state:\n",
    "        st.session_state.ticker_input = \"SPY\"\n",
    "    if 'period_input' not in st.session_state:\n",
    "        st.session_state.period_input = \"max\"\n",
    "    \n",
    "    # ===================== DATA CONFIGURATION =====================\n",
    "    st.markdown(\"<div class='config-section'>\", unsafe_allow_html=True)\n",
    "    st.markdown(\"<div class='section-title'>Data Configuration</div>\", unsafe_allow_html=True)\n",
    "    \n",
    "    col1, col2, col3, col4 = st.columns([2, 1.5, 1.5, 1])\n",
    "    \n",
    "    with col1:\n",
    "        ticker = st.text_input(\n",
    "            \"SYMBOL\", \n",
    "            value=st.session_state.ticker_input, \n",
    "            help=\"Stock symbol to analyze\",\n",
    "            key=\"ticker_widget\"\n",
    "        )\n",
    "        # Update session state when changed\n",
    "        if ticker != st.session_state.ticker_input:\n",
    "            st.session_state.ticker_input = ticker\n",
    "    \n",
    "    with col2:\n",
    "        period_option = st.selectbox(\n",
    "            \"PERIOD\",\n",
    "            [\"1mo\", \"3mo\", \"6mo\", \"1y\", \"2y\", \"5y\", \"10y\", \"max\"],\n",
    "            index=[\"1mo\", \"3mo\", \"6mo\", \"1y\", \"2y\", \"5y\", \"10y\", \"max\"].index(st.session_state.period_input) if st.session_state.period_input in [\"1mo\", \"3mo\", \"6mo\", \"1y\", \"2y\", \"5y\", \"10y\", \"max\"] else 4,\n",
    "            key=\"period_widget\"\n",
    "        )\n",
    "        if period_option != st.session_state.period_input:\n",
    "            st.session_state.period_input = period_option\n",
    "    \n",
    "    with col3:\n",
    "        col3a, col3b = st.columns(2)\n",
    "        with col3a:\n",
    "            min_return_days = st.number_input(\"MIN DAYS\", value=1, min_value=1, max_value=60, key=\"min_days_widget\")\n",
    "        with col3b:\n",
    "            max_return_days = st.number_input(\"MAX DAYS\", value=20, min_value=1, max_value=60, key=\"max_days_widget\")\n",
    "    \n",
    "    with col4:\n",
    "        quantiles = st.number_input(\"PERCENTILES\", value=10, min_value=5, max_value=20, step=5, key=\"quantiles_widget\")\n",
    "    \n",
    "    st.markdown(\"</div>\", unsafe_allow_html=True)\n",
    "    \n",
    "    # ===================== PERIOD CONFIGURATION =====================\n",
    "    st.markdown(\"<div class='config-section'>\", unsafe_allow_html=True)\n",
    "    st.markdown(\"<div class='section-title'>Period Configuration</div>\", unsafe_allow_html=True)\n",
    "    \n",
    "    col1, col2, col3, col4 = st.columns([1, 1, 1, 2])\n",
    "    \n",
    "    with col1:\n",
    "        min_period = st.number_input(\"MIN\", value=5, min_value=2, max_value=500)\n",
    "    with col2:\n",
    "        max_period = st.number_input(\"MAX\", value=50, min_value=5, max_value=500)\n",
    "    with col3:\n",
    "        step_period = st.number_input(\"STEP\", value=5, min_value=1, max_value=50)\n",
    "    with col4:\n",
    "        periods_to_test = list(range(min_period, max_period + 1, step_period))\n",
    "        st.markdown(f\"\"\"\n",
    "            <div class=\"info-badge\">\n",
    "                Periods: {', '.join(map(str, periods_to_test))}\n",
    "            </div>\n",
    "        \"\"\", unsafe_allow_html=True)\n",
    "    \n",
    "    st.markdown(\"</div>\", unsafe_allow_html=True)\n",
    "    \n",
    "    # ===================== INDICATOR SELECTION =====================\n",
    "    st.markdown(\"<div class='config-section'>\", unsafe_allow_html=True)\n",
    "    st.markdown(\"<div class='section-title'>Indicator Selection</div>\", unsafe_allow_html=True)\n",
    "    \n",
    "    col1, col2 = st.columns([1, 3])\n",
    "    \n",
    "    with col1:\n",
    "        select_mode = st.radio(\"MODE\", [\"Presets\", \"Categories\", \"All\"])\n",
    "    \n",
    "    with col2:\n",
    "        if select_mode == \"Presets\":\n",
    "            preset = st.selectbox(\n",
    "                \"CONFIGURATION\",\n",
    "                [\"Essential (30 indicators)\", \"Extended (60 indicators)\", \n",
    "                 \"Complete (100 indicators)\", \"All (158+ indicators)\"]\n",
    "            )\n",
    "            \n",
    "            if \"Essential\" in preset:\n",
    "                selected_categories = [\"SuperposiciÃ³n\", \"Momentum\"][:1]\n",
    "            elif \"Extended\" in preset:\n",
    "                selected_categories = [\"Momentum\", \"Volatilidad\", \"Volumen\", \"SuperposiciÃ³n\"]\n",
    "            elif \"Complete\" in preset:\n",
    "                selected_categories = list(TechnicalIndicators.CATEGORIES.keys())[:7]\n",
    "            else:\n",
    "                selected_categories = [\"TODO\"]\n",
    "        \n",
    "        elif select_mode == \"Categories\":\n",
    "            selected_categories = st.multiselect(\n",
    "                \"SELECT CATEGORIES\",\n",
    "                list(TechnicalIndicators.CATEGORIES.keys()),\n",
    "                default=[\"Momentum\", \"SuperposiciÃ³n\"]\n",
    "            )\n",
    "        else:\n",
    "            selected_categories = [\"TODO\"]\n",
    "    \n",
    "    st.markdown(\"</div>\", unsafe_allow_html=True)\n",
    "    \n",
    "    # ===================== ANALYZE BUTTON =====================\n",
    "    st.markdown(\"<br>\", unsafe_allow_html=True)\n",
    "    col1, col2, col3 = st.columns([2, 1, 2])\n",
    "    with col2:\n",
    "        analyze_button = st.button(\"ANALYZE\", use_container_width=True, type=\"primary\", key=\"analyze_main\")\n",
    "    \n",
    "    if analyze_button and max_return_days >= min_return_days:\n",
    "        with st.spinner('Processing indicators...'):\n",
    "            returns_data, indicators, data, summary = calculate_all_indicators(\n",
    "                ticker, period_option, quantiles, min_return_days, max_return_days,\n",
    "                periods_to_test, selected_categories\n",
    "            )\n",
    "            \n",
    "            if returns_data and indicators is not None and data is not None:\n",
    "                st.session_state.analysis_done = True\n",
    "                st.session_state.returns_data = returns_data\n",
    "                st.session_state.indicators = indicators\n",
    "                st.session_state.data = data\n",
    "                st.session_state.summary = summary\n",
    "                st.session_state.periods_to_test = periods_to_test\n",
    "                st.session_state.selected_categories = selected_categories\n",
    "                st.rerun()\n",
    "    \n",
    "    # ===================== RESULTS DISPLAY =====================\n",
    "    if st.session_state.analysis_done:\n",
    "        returns_data = st.session_state.returns_data\n",
    "        indicators = st.session_state.indicators\n",
    "        data = st.session_state.data\n",
    "        summary = st.session_state.summary\n",
    "        \n",
    "        st.markdown(\"<hr>\", unsafe_allow_html=True)\n",
    "        \n",
    "        col1, col2, col3, col4 = st.columns(4)\n",
    "        with col1:\n",
    "            st.metric(\"INDICATORS\", summary['indicators_count'])\n",
    "        with col2:\n",
    "            st.metric(\"SUCCESS RATE\", f\"{(summary['successful']/summary['total_attempted']*100):.1f}%\")\n",
    "        with col3:\n",
    "            st.metric(\"DATA POINTS\", summary['data_points'])\n",
    "        with col4:\n",
    "            st.metric(\"RANGE\", summary['date_range'].split(' to ')[0])\n",
    "        \n",
    "        # ===================== TABS =====================\n",
    "        tab1, tab2, tab3 = st.tabs([\"ðŸ“Š ANALYSIS\", \"ðŸš€ ADVANCED RULES\", \"ðŸ’¾ EXPORT\"])\n",
    "        \n",
    "        # TAB 1: Analysis\n",
    "        with tab1:\n",
    "            col1, col2 = st.columns([3, 1])\n",
    "            with col1:\n",
    "                selected_indicator = st.selectbox(\n",
    "                    \"SELECT INDICATOR\",\n",
    "                    sorted(indicators.columns),\n",
    "                    key=\"indicator_select\"\n",
    "                )\n",
    "            with col2:\n",
    "                return_period = st.number_input(\n",
    "                    \"DAYS\",\n",
    "                    min_value=summary['min_return_days'],\n",
    "                    max_value=summary['max_return_days'],\n",
    "                    value=min(5, summary['max_return_days']),\n",
    "                    key=\"return_period_select\"\n",
    "                )\n",
    "            \n",
    "            if selected_indicator:\n",
    "                fig = create_percentile_plot(\n",
    "                    indicators, returns_data, data,\n",
    "                    selected_indicator, return_period, quantiles\n",
    "                )\n",
    "                \n",
    "                if fig:\n",
    "                    st.plotly_chart(fig, use_container_width=True)\n",
    "        \n",
    "        # TAB 2: Advanced Rules\n",
    "        with tab2:\n",
    "            st.markdown(\"### ðŸš€ Advanced Rule Discovery Platform\")\n",
    "            \n",
    "            st.info(\"\"\"\n",
    "            **ðŸŽ¯ Choose Your Optimization Method:**\n",
    "            \n",
    "            Select ONE method that best fits your needs:\n",
    "            - **Particle Swarm (PSO)** - Fast continuous optimization with optional Enhanced Multi-Swarm mode for 2-3x more diverse rules  \n",
    "            - **Brute Force** - Traditional exhaustive search (~5-15 min, tests thousands of rules)\n",
    "            \n",
    "            **ðŸ”„ Auto-Optimization:** Each rule is tested across your selected holding period range, and the best performing period is reported in results.\n",
    "            \n",
    "            **âš ï¸ Quality Control:** Rules with <5 IS signals, <3 OOS signals, or near-zero Sharpe are automatically rejected.\n",
    "            \"\"\")\n",
    "            \n",
    "            # Method Selection - RADIO BUTTON\n",
    "            st.markdown(\"### ðŸŽ¯ Select Optimization Method\")\n",
    "            optimization_method = st.radio(\n",
    "                \"Choose ONE method:\",\n",
    "                [\"ðŸ” Brute Force Search\", \"ðŸŒŸ Particle Swarm Optimization (PSO)\"],\n",
    "                index=0,\n",
    "                horizontal=True,\n",
    "                help=\"Only one method can be selected at a time\",\n",
    "                key=\"optimization_method_radio\"\n",
    "            )\n",
    "            \n",
    "            use_brute = \"Brute Force\" in optimization_method\n",
    "            use_pso = \"Particle Swarm\" in optimization_method\n",
    "            use_genetic = False  # Genetic Algorithm removed\n",
    "            \n",
    "            st.markdown(\"---\")\n",
    "            \n",
    "            # Configuration Form\n",
    "            with st.form(\"rule_config_form\"):\n",
    "                st.markdown(\"### âš™ï¸ Configuration\")\n",
    "                \n",
    "                config_col1, config_col2 = st.columns(2)\n",
    "                \n",
    "                with config_col1:\n",
    "                    st.markdown(\"**ðŸ“Š Data & Trading**\")\n",
    "                    sample_split = st.slider(\"In-Sample %\", 50, 80, 70, 5, key=\"sample_split\")\n",
    "                    \n",
    "                    # Holding Period Range\n",
    "                    st.markdown(\"**Holding Period Range (Auto-Optimize)**\")\n",
    "                    holding_period_range = st.slider(\n",
    "                        \"Test multiple holding periods and find the best for each rule\",\n",
    "                        min_value=summary['min_return_days'],\n",
    "                        max_value=summary['max_return_days'],\n",
    "                        value=(summary['min_return_days'], min(summary['min_return_days'] + 15, summary['max_return_days'])),\n",
    "                        step=5,\n",
    "                        key=\"holding_period_range\",\n",
    "                        help=\"System will test each rule across this range and report the best period\"\n",
    "                    )\n",
    "                    holding_periods_to_test = list(range(holding_period_range[0], holding_period_range[1] + 1, 5))\n",
    "                    if holding_period_range[1] not in holding_periods_to_test:\n",
    "                        holding_periods_to_test.append(holding_period_range[1])\n",
    "                    \n",
    "                    st.info(f\"ðŸŽ¯ Will test: {len(holding_periods_to_test)} periods â†’ {holding_periods_to_test}\")\n",
    "                    \n",
    "                    transaction_cost = st.number_input(\"Transaction Cost (%)\", 0.0, 1.0, 0.1, 0.01, key=\"tx_cost\") / 100\n",
    "                \n",
    "                with config_col2:\n",
    "                    st.markdown(\"**ðŸŽ¯ Rule Parameters**\")\n",
    "                    # CORRECTED: Allow using ALL indicators (no 50 limit)\n",
    "                    max_indicators_opt = st.number_input(\"Max Indicators to Use (0 = ALL)\", 0, 500, 0, key=\"max_ind\",\n",
    "                                                        help=\"Set to 0 to use ALL available indicators. Otherwise specify a limit.\")\n",
    "                    min_signals_is = st.number_input(\"Min Signals (IS)\", 5, 100, 15, key=\"min_sig_is\", \n",
    "                                                     help=\"Minimum signals required in training data (validation enforces â‰¥5)\")\n",
    "                    min_signals_oos = st.number_input(\"Min Signals (OOS)\", 3, 50, 10, key=\"min_sig_oos\",\n",
    "                                                      help=\"Minimum signals required in test data (validation enforces â‰¥3)\")\n",
    "                \n",
    "                # Method-specific configuration\n",
    "                if use_pso:\n",
    "                    st.markdown(\"---\")\n",
    "                    st.markdown(\"### ðŸŒŸ PSO-Specific Settings\")\n",
    "                    \n",
    "                    # Add improved PSO option\n",
    "                    use_improved_pso = st.checkbox(\n",
    "                        \"âœ¨ Use Enhanced Multi-Swarm PSO (finds MORE diverse rules)\",\n",
    "                        value=True,\n",
    "                        key=\"use_improved_pso\",\n",
    "                        help=\"Enhanced version with multiple swarms, diversity archive, and adaptive parameters. Finds 2-3x more unique rules!\"\n",
    "                    )\n",
    "                    \n",
    "                    pso_col1, pso_col2 = st.columns(2)\n",
    "                    \n",
    "                    with pso_col1:\n",
    "                        pso_n_particles = st.slider(\"Number of Particles\", 20, 150, 50, key=\"pso_particles\")\n",
    "                        pso_n_iterations = st.slider(\"Number of Iterations\", 20, 150, 80, key=\"pso_iter\")\n",
    "                        \n",
    "                        if use_improved_pso:\n",
    "                            pso_n_swarms = st.slider(\"Number of Swarms\", 2, 5, 3, key=\"pso_swarms\",\n",
    "                                                    help=\"Multiple swarms explore different regions\")\n",
    "                    \n",
    "                    with pso_col2:\n",
    "                        if use_improved_pso:\n",
    "                            pso_inertia_max = st.slider(\"Max Inertia (w_max)\", 0.7, 1.0, 0.9, 0.05, key=\"pso_w_max\")\n",
    "                            pso_inertia_min = st.slider(\"Min Inertia (w_min)\", 0.2, 0.6, 0.4, 0.05, key=\"pso_w_min\")\n",
    "                        else:\n",
    "                            pso_inertia = st.slider(\"Inertia Weight (w)\", 0.1, 1.0, 0.7, 0.1, key=\"pso_w\")\n",
    "                        \n",
    "                        pso_cognitive = st.slider(\"Cognitive (c1)\", 0.5, 2.5, 1.5, 0.1, key=\"pso_c1\")\n",
    "                        pso_social = st.slider(\"Social (c2)\", 0.5, 2.5, 1.5, 0.1, key=\"pso_c2\")\n",
    "                    \n",
    "                    if use_improved_pso:\n",
    "                        st.info(\"\"\"\n",
    "                        ðŸš€ **Enhanced PSO Features:**\n",
    "                        - Multiple swarms explore different strategies simultaneously\n",
    "                        - Diversity archive maintains unique high-quality rules\n",
    "                        - Adaptive parameters optimize exploration/exploitation\n",
    "                        - Inter-swarm migration shares best solutions\n",
    "                        - Expects to find **20-30 unique rules** (vs 10-15 in standard)\n",
    "                        \"\"\")\n",
    "                else:\n",
    "                    use_improved_pso = False\n",
    "                    pso_n_particles = 50\n",
    "                    pso_n_iterations = 80\n",
    "                    pso_inertia = 0.7\n",
    "                    pso_inertia_max = 0.9\n",
    "                    pso_inertia_min = 0.4\n",
    "                    pso_cognitive = 1.5\n",
    "                    pso_social = 1.5\n",
    "                    pso_n_swarms = 3\n",
    "                \n",
    "                if use_brute:\n",
    "                    st.markdown(\"---\")\n",
    "                    st.markdown(\"### ðŸ” Brute Force Settings\")\n",
    "                    brute_col1, brute_col2 = st.columns(2)\n",
    "                    \n",
    "                    with brute_col1:\n",
    "                        max_rules_test = st.number_input(\"Max Rules to Test\", 100, 10000, 2000, 100, key=\"max_rules\")\n",
    "                        use_compound = st.checkbox(\"Generate Compound Rules\", True, key=\"use_compound\")\n",
    "                        \n",
    "                        if use_compound:\n",
    "                            max_compound = st.number_input(\"Max Compound Rules\", 100, 5000, 1000, key=\"max_comp\")\n",
    "                            compound_depth = st.select_slider(\"Max Conditions\", [2, 3], 2, key=\"comp_depth\")\n",
    "                        else:\n",
    "                            max_compound = 0\n",
    "                            compound_depth = 2\n",
    "                    \n",
    "                    with brute_col2:\n",
    "                        percentiles_use = st.multiselect(\n",
    "                            \"Percentile Thresholds\",\n",
    "                            list(range(5, 100, 5)),\n",
    "                            default=[10, 20, 30, 50, 70, 80, 90],\n",
    "                            key=\"percentiles\"\n",
    "                        )\n",
    "                        operators = st.multiselect(\"Operators\", ['>', '<', '>=', '<='], ['>', '<'], key=\"operators\")\n",
    "                else:\n",
    "                    max_rules_test = 2000\n",
    "                    use_compound = True\n",
    "                    max_compound = 1000\n",
    "                    compound_depth = 2\n",
    "                    percentiles_use = [10, 20, 30, 50, 70, 80, 90]\n",
    "                    operators = ['>', '<']\n",
    "                \n",
    "                st.markdown(\"---\")\n",
    "                \n",
    "                # Quality Filters\n",
    "                with st.expander(\"âœ… Quality Filters\", expanded=False):\n",
    "                    filter_col1, filter_col2, filter_col3 = st.columns(3)\n",
    "                    \n",
    "                    with filter_col1:\n",
    "                        min_is_sharpe = st.number_input(\"Min IS Sharpe\", -2.0, 3.0, 0.2, 0.1, key=\"min_is_sharpe\")\n",
    "                        min_oos_sharpe = st.number_input(\"Min OOS Sharpe\", -2.0, 3.0, -0.5, 0.1, key=\"min_oos_sharpe\")\n",
    "                    \n",
    "                    with filter_col2:\n",
    "                        max_degradation = st.slider(\"Max Sharpe Degradation\", 0.5, 5.0, 3.0, 0.1, key=\"max_deg\")\n",
    "                        min_consistency = st.slider(\"Min Consistency\", 0.0, 1.0, 0.1, 0.05, key=\"min_cons\")\n",
    "                    \n",
    "                    with filter_col3:\n",
    "                        if use_pso:\n",
    "                            est_time = \"~3-8 min\"\n",
    "                        else:\n",
    "                            est_time = \"~5-15 min\"\n",
    "                        \n",
    "                        st.info(f\"\"\"\n",
    "                        **Expected Time:**\n",
    "                        {est_time}\n",
    "                        \"\"\")\n",
    "                \n",
    "                # RULE VALIDATION SETTINGS (NEW)\n",
    "                with st.expander(\"ðŸ”§ Rule Validation Settings\", expanded=False):\n",
    "                    st.markdown(\"\"\"\n",
    "                    **Control how strictly rules are validated:**\n",
    "                    - Higher values = more lenient (keeps more rules)\n",
    "                    - Lower values = stricter (filters more rules)\n",
    "                    \"\"\")\n",
    "                    \n",
    "                    val_col1, val_col2, val_col3 = st.columns(3)\n",
    "                    \n",
    "                    with val_col1:\n",
    "                        enable_validation = st.checkbox(\n",
    "                            \"Enable Rule Validation\",\n",
    "                            value=True,\n",
    "                            help=\"Automatically filter rules with unrealistic thresholds\"\n",
    "                        )\n",
    "                        \n",
    "                        show_validation_details = st.checkbox(\n",
    "                            \"Show Filtering Details\",\n",
    "                            value=False,\n",
    "                            help=\"Display which rules were filtered and why\"\n",
    "                        )\n",
    "                    \n",
    "                    with val_col2:\n",
    "                        data_range_margin = st.slider(\n",
    "                            \"Data Range Margin (%)\",\n",
    "                            min_value=5,\n",
    "                            max_value=50,\n",
    "                            value=15,\n",
    "                            step=5,\n",
    "                            help=\"How far beyond min/max to allow thresholds (15% = default)\"\n",
    "                        )\n",
    "                    \n",
    "                    with val_col3:\n",
    "                        extreme_margin = st.slider(\n",
    "                            \"Extreme Value Margin (%)\",\n",
    "                            min_value=10,\n",
    "                            max_value=100,\n",
    "                            value=20,\n",
    "                            step=10,\n",
    "                            help=\"How far beyond 99th percentile to allow (20% = default)\"\n",
    "                        )\n",
    "                    \n",
    "                    st.info(f\"\"\"\n",
    "                    **Current Settings:**\n",
    "                    - Validation: {'Enabled âœ“' if enable_validation else 'Disabled âœ—'}\n",
    "                    - Range Margin: Â±{data_range_margin}% of data range\n",
    "                    - Extreme Margin: +{extreme_margin}% beyond percentiles\n",
    "                    \"\"\")\n",
    "                \n",
    "                # SUBMIT BUTTON\n",
    "                st.markdown(\"---\")\n",
    "                run_advanced = st.form_submit_button(\n",
    "                    \"ðŸš€ RUN ADVANCED ANALYSIS\", \n",
    "                    use_container_width=True,\n",
    "                    type=\"primary\"\n",
    "                )\n",
    "            \n",
    "            # EXECUTION\n",
    "            if run_advanced:\n",
    "                # Clear previous results\n",
    "                st.session_state.advanced_analysis_done = False\n",
    "                st.session_state.advanced_results = None\n",
    "                st.session_state.optimization_histories = None\n",
    "                \n",
    "                # Split data\n",
    "                split_index = int(len(data) * sample_split / 100)\n",
    "                is_data = data.iloc[:split_index].copy()\n",
    "                oos_data = data.iloc[split_index:].copy()\n",
    "                \n",
    "                # Calculate indicators\n",
    "                with st.spinner(\"Calculating indicators...\"):\n",
    "                    is_indicators = calculate_indicators_for_dataset(\n",
    "                        is_data, st.session_state.periods_to_test,\n",
    "                        st.session_state.selected_categories\n",
    "                    )\n",
    "                    oos_indicators = calculate_indicators_for_dataset(\n",
    "                        oos_data, st.session_state.periods_to_test,\n",
    "                        st.session_state.selected_categories\n",
    "                    )\n",
    "                \n",
    "                # Prepare for PSO (uses first period in range for optimization)\n",
    "                # Actual validation will test all periods\n",
    "                pso_holding_period = holding_periods_to_test[0]\n",
    "                is_returns = is_data['Close'].pct_change(pso_holding_period).shift(-pso_holding_period) * 100\n",
    "                oos_returns = oos_data['Close'].pct_change(pso_holding_period).shift(-pso_holding_period) * 100\n",
    "                \n",
    "                # ============= FILTER VALID INDICATORS =============\n",
    "                # Remove math functions and invalid columns\n",
    "                invalid_names = {\n",
    "                    'CEIL', 'FLOOR', 'SQRT', 'EXP', 'LOG', 'LOG10', 'LN',\n",
    "                    'TAN', 'SIN', 'COS', 'ASIN', 'ACOS', 'ATAN', 'SINH', 'COSH', 'TANH',\n",
    "                    'ABS', 'MAX', 'MIN', 'SUM', 'AVG', 'MEAN',\n",
    "                    'Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close',\n",
    "                    'Date', 'Datetime', 'Timestamp'\n",
    "                }\n",
    "                \n",
    "                # Filter to only valid indicators\n",
    "                valid_indicator_names = []\n",
    "                for col in is_indicators.columns:\n",
    "                    # Skip if column name is in invalid set\n",
    "                    col_upper = col.upper()\n",
    "                    if any(inv in col_upper for inv in invalid_names):\n",
    "                        continue\n",
    "                    \n",
    "                    # Check if indicator has enough valid data\n",
    "                    col_data = is_indicators[col].dropna()\n",
    "                    if len(col_data) < 20:\n",
    "                        continue\n",
    "                    \n",
    "                    # Check if indicator has reasonable variance (not all same value)\n",
    "                    if col_data.std() < 1e-10:\n",
    "                        continue\n",
    "                    \n",
    "                    # Check for reasonable value range (not all zeros or extreme)\n",
    "                    if abs(col_data.mean()) < 1e-10 and col_data.std() < 1e-10:\n",
    "                        continue\n",
    "                    \n",
    "                    valid_indicator_names.append(col)\n",
    "                \n",
    "                # CORRECTED: Limit to max_indicators_opt (0 = use ALL)\n",
    "                if max_indicators_opt > 0:\n",
    "                    indicator_names = valid_indicator_names[:max_indicators_opt]\n",
    "                else:\n",
    "                    indicator_names = valid_indicator_names  # Use ALL indicators\n",
    "                \n",
    "                if len(indicator_names) < 3:\n",
    "                    st.error(f\"âš ï¸ Only {len(indicator_names)} valid indicators found. Need at least 3. Try selecting more indicator categories in Tab 1.\")\n",
    "                    st.stop()\n",
    "                \n",
    "                st.info(f\"âœ… Using {len(indicator_names)} valid indicators: {', '.join(indicator_names[:10])}{'...' if len(indicator_names) > 10 else ''}\")\n",
    "                \n",
    "                # CRITICAL FIX: Filter to only indicators that exist in BOTH is_indicators and oos_indicators\n",
    "                available_in_is = set(is_indicators.columns)\n",
    "                available_in_oos = set(oos_indicators.columns)\n",
    "                indicator_names = [ind for ind in indicator_names if ind in available_in_is and ind in available_in_oos]\n",
    "                \n",
    "                if len(indicator_names) < 3:\n",
    "                    st.error(f\"âš ï¸ Only {len(indicator_names)} indicators available in both IS and OOS periods. Need at least 3.\")\n",
    "                    st.info(\"\"\"\n",
    "                    **Possible causes:**\n",
    "                    - OOS period too short to calculate some indicators\n",
    "                    - Indicators require more data points than available\n",
    "                    - Try increasing the train/test split ratio\n",
    "                    \"\"\")\n",
    "                    st.stop()\n",
    "                \n",
    "                st.info(f\"âœ… Using {len(indicator_names)} valid indicators: {', '.join(indicator_names[:10])}{'...' if len(indicator_names) > 10 else ''}\")\n",
    "                \n",
    "                # Now safely filter the indicators DataFrames\n",
    "                is_indicators = is_indicators[indicator_names].copy()\n",
    "                oos_indicators = oos_indicators[indicator_names].copy()\n",
    "                \n",
    "                all_results = []\n",
    "                optimization_histories = {}\n",
    "                \n",
    "                # ============= PSO OPTIMIZATION =============\n",
    "                if use_pso:\n",
    "                    st.markdown(\"### ðŸŒŸ Particle Swarm Optimization\")\n",
    "                    \n",
    "                    if use_improved_pso:\n",
    "                        st.success(\"âœ¨ Using **Enhanced Multi-Swarm PSO** - Expect 20-30+ diverse rules\")\n",
    "                        pso = ImprovedParticleSwarmOptimizer(\n",
    "                            n_particles=pso_n_particles,\n",
    "                            n_iterations=pso_n_iterations,\n",
    "                            w_max=pso_inertia_max,\n",
    "                            w_min=pso_inertia_min,\n",
    "                            c1=pso_cognitive,\n",
    "                            c2=pso_social,\n",
    "                            n_swarms=pso_n_swarms,\n",
    "                            archive_size=50\n",
    "                        )\n",
    "                    else:\n",
    "                        st.info(\"ðŸ“Š Using **Standard PSO** - Expect 10-15 rules\")\n",
    "                        pso = ParticleSwarmOptimizer(\n",
    "                            n_particles=pso_n_particles,\n",
    "                            n_iterations=pso_n_iterations,\n",
    "                            w=pso_inertia,\n",
    "                            c1=pso_cognitive,\n",
    "                            c2=pso_social\n",
    "                        )\n",
    "                    \n",
    "                    progress_container = st.empty()\n",
    "                    status_container = st.empty()\n",
    "                    metrics_container = st.empty()\n",
    "                    \n",
    "                    def pso_callback(iteration, total, fitness, valid_count):\n",
    "                        progress = iteration / total\n",
    "                        progress_container.progress(progress)\n",
    "                        status_container.markdown(f\"\"\"\n",
    "                        <div class=\"progress-text\">\n",
    "                            ðŸŒŸ PSO Iteration {iteration}/{total} | Best Fitness: {fitness:.4f} | Valid: {valid_count}\n",
    "                        </div>\n",
    "                        \"\"\", unsafe_allow_html=True)\n",
    "                    \n",
    "                    metrics_container.info(f\"ðŸ” Optimizing with {pso_n_particles} particles over {pso_n_iterations} iterations...\")\n",
    "                    \n",
    "                    pso_result = pso.optimize_rule_parameters(\n",
    "                        is_indicators, is_returns, indicator_names, pso_callback\n",
    "                    )\n",
    "                    \n",
    "                    progress_container.empty()\n",
    "                    status_container.empty()\n",
    "                    metrics_container.empty()\n",
    "                    \n",
    "                    # Display PSO statistics\n",
    "                    if use_improved_pso and 'archive_size' in pso_result:\n",
    "                        st.info(f\"ðŸ“Š Enhanced PSO Stats: {pso_result.get('valid_rules_found', 0)} valid evaluations | Archive: {pso_result.get('archive_size', 0)} diverse rules | Total evaluations: {pso_result.get('total_evaluations', 0)}\")\n",
    "                    else:\n",
    "                        st.info(f\"ðŸ“Š PSO Stats: {pso_result.get('valid_rules_found', 0)} valid rules found out of {pso_result.get('total_evaluations', 0)} evaluations\")\n",
    "                    \n",
    "                    # Process multiple rules from PSO\n",
    "                    if pso_result.get('rules') and len(pso_result['rules']) > 0:\n",
    "                        n_rules_found = len(pso_result['rules'])\n",
    "                        st.success(f\"âœ… PSO found {n_rules_found} diverse rules!\")\n",
    "                        \n",
    "                        # Warning if fewer than expected\n",
    "                        if use_improved_pso and n_rules_found < 15:\n",
    "                            st.warning(f\"âš ï¸ Enhanced PSO found fewer rules than expected ({n_rules_found} vs expected 20-30). This may indicate:\\n\"\n",
    "                                     \"- Limited indicator variety (try selecting more indicator categories in Tab 1)\\n\"\n",
    "                                     \"- Tight quality filters (try loosening Min Sharpe or other filters)\\n\"\n",
    "                                     \"- Small dataset (need more historical data)\")\n",
    "                        elif not use_improved_pso and n_rules_found < 5:\n",
    "                            st.warning(f\"âš ï¸ Standard PSO found very few rules ({n_rules_found}). Consider using Enhanced PSO or checking data quality.\")\n",
    "                        \n",
    "                        # Show the rules found\n",
    "                        with st.expander(f\"View {n_rules_found} PSO Rules (Before Validation)\", expanded=False):\n",
    "                            for idx, (rule, fitness) in enumerate(zip(pso_result['rules'][:10], pso_result['fitness_scores'][:10])):\n",
    "                                st.text(f\"{idx+1}. Fitness: {fitness:.3f} | {rule}\")\n",
    "                        \n",
    "                        validator = EnhancedComprehensiveValidator()\n",
    "                        \n",
    "                        # Validate with progress\n",
    "                        validation_status = st.empty()\n",
    "                        validation_status.info(f\"ðŸ” Validating {len(pso_result['rules'])} PSO rules across {len(holding_periods_to_test)} periods...\")\n",
    "                        \n",
    "                        for idx, (rule, fitness) in enumerate(zip(pso_result['rules'], pso_result['fitness_scores'])):\n",
    "                            pso_validation = validator.validate_single_rule_multiperiod(\n",
    "                                {'condition': rule, 'type': 'pso', 'trade_type': 'BUY'},\n",
    "                                is_indicators, is_data, oos_indicators, oos_data,\n",
    "                                holding_periods_to_test, transaction_cost\n",
    "                            )\n",
    "                            \n",
    "                            if pso_validation:\n",
    "                                all_results.append(pso_validation)\n",
    "                        \n",
    "                        validation_status.empty()\n",
    "                        \n",
    "                        optimization_histories['PSO'] = pso_result['history']\n",
    "                        \n",
    "                        if len(all_results) > 0:\n",
    "                            st.info(f\"ðŸ“Š {len(all_results)} PSO rules passed validation (out of {len(pso_result['rules'])} found)\")\n",
    "                        else:\n",
    "                            st.warning(f\"âš ï¸ PSO found {len(pso_result['rules'])} rules but ALL were rejected (insufficient signals or poor quality)\")\n",
    "                    else:\n",
    "                        st.warning(f\"âš ï¸ PSO did not find valid rules\")\n",
    "                        if 'message' in pso_result:\n",
    "                            st.info(f\"â„¹ï¸ {pso_result['message']}\")\n",
    "                    \n",
    "                    # Plot convergence\n",
    "                    if len(pso_result['history']) > 0:\n",
    "                        fig = create_optimization_convergence_plot(\n",
    "                            pso_result['history'], \"PSO Convergence\"\n",
    "                        )\n",
    "                        st.plotly_chart(fig, use_container_width=True)\n",
    "                \n",
    "                # ============= BRUTE FORCE =============\n",
    "                if use_brute:\n",
    "                    st.markdown(\"### ðŸ” Brute Force Search\")\n",
    "                    \n",
    "                    generator = ImprovedMassiveRuleGenerator()\n",
    "                    \n",
    "                    simple_rules = generator.generate_simple_rules(\n",
    "                        is_indicators, percentiles_use, operators, max_indicators_opt\n",
    "                    )\n",
    "                    \n",
    "                    if use_compound:\n",
    "                        compound_rules = generator.generate_compound_rules(\n",
    "                            simple_rules, max_depth=compound_depth, max_combinations=max_compound\n",
    "                        )\n",
    "                        all_rules = simple_rules + compound_rules\n",
    "                    else:\n",
    "                        all_rules = simple_rules\n",
    "                    \n",
    "                    st.info(f\"Generated {len(all_rules)} rules for validation\")\n",
    "                    \n",
    "                    validator = EnhancedComprehensiveValidator()\n",
    "                    brute_results = validator.validate_rules_batch(\n",
    "                        all_rules[:max_rules_test],\n",
    "                        is_indicators, is_data, oos_indicators, oos_data,\n",
    "                        holding_periods_to_test, transaction_cost\n",
    "                    )\n",
    "                    \n",
    "                    if not brute_results.empty:\n",
    "                        for _, row in brute_results.iterrows():\n",
    "                            all_results.append(row.to_dict())\n",
    "                        \n",
    "                        st.success(f\"âœ… Validated {len(brute_results)} rules\")\n",
    "                \n",
    "                # ============= VALIDATE AND SAVE RESULTS =============\n",
    "                if all_results:\n",
    "                    results_df = pd.DataFrame(all_results)\n",
    "                    original_count = len(results_df)\n",
    "                    \n",
    "                    # === RULE VALIDATION (filters out bad thresholds) ===\n",
    "                    if enable_validation:\n",
    "                        st.info(\"ðŸ” Validating rule thresholds...\")\n",
    "                        \n",
    "                        # Calculate indicator statistics\n",
    "                        indicator_stats = {}\n",
    "                        for col in indicators.columns:\n",
    "                            if col not in ['Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close']:\n",
    "                                data = indicators[col].dropna()\n",
    "                                if len(data) > 0:\n",
    "                                    indicator_stats[col] = {\n",
    "                                        'min': data.min(),\n",
    "                                        'max': data.max(),\n",
    "                                        'p01': data.quantile(0.01),\n",
    "                                        'p99': data.quantile(0.99),\n",
    "                                        'mean': data.mean(),\n",
    "                                        'std': data.std(),\n",
    "                                    }\n",
    "                        \n",
    "                        # Convert margins from percentage to decimal\n",
    "                        range_margin_pct = data_range_margin / 100.0\n",
    "                        extreme_margin_pct = extreme_margin / 100.0\n",
    "                        \n",
    "                        # Validation function with detailed feedback\n",
    "                        def validate_rule_detailed(rule_str):\n",
    "                            \"\"\"Check if rule has realistic thresholds and return reason\"\"\"\n",
    "                            import re\n",
    "                            \n",
    "                            # Parse conditions\n",
    "                            conditions = re.split(r'\\s+(?:AND|OR)\\s+', rule_str)\n",
    "                            reasons = []\n",
    "                            \n",
    "                            for condition in conditions:\n",
    "                                condition = condition.strip('() ')\n",
    "                                \n",
    "                                # Extract indicator, operator, threshold\n",
    "                                match = re.match(r'([A-Z_]+_\\d+)\\s*([><]=?)\\s*([-+]?\\d+\\.?\\d*)', condition)\n",
    "                                if not match:\n",
    "                                    continue\n",
    "                                \n",
    "                                indicator, operator, threshold = match.groups()\n",
    "                                threshold = float(threshold)\n",
    "                                \n",
    "                                if indicator not in indicator_stats:\n",
    "                                    reasons.append(f\"{indicator} not in data\")\n",
    "                                    return False, reasons\n",
    "                                \n",
    "                                stats = indicator_stats[indicator]\n",
    "                                data_range = stats['max'] - stats['min']\n",
    "                                \n",
    "                                # Check for bounded indicators (RSI, Stochastic, etc.)\n",
    "                                if any(x in indicator.upper() for x in ['RSI', 'STOCH', 'WILLR']):\n",
    "                                    if threshold < 0 or threshold > 100:\n",
    "                                        reasons.append(f\"{indicator} {operator} {threshold:.2f} (bounded 0-100)\")\n",
    "                                        return False, reasons\n",
    "                                \n",
    "                                # Check for volatility indicators (must be positive)\n",
    "                                if any(x in indicator.upper() for x in ['ATR', 'NATR', 'BBANDS', 'STDDEV']):\n",
    "                                    if threshold < 0:\n",
    "                                        reasons.append(f\"{indicator} < 0 (volatility must be positive)\")\n",
    "                                        return False, reasons\n",
    "                                \n",
    "                                # Check if threshold is way outside data range\n",
    "                                margin = data_range * range_margin_pct\n",
    "                                if operator in ['>', '>=']:\n",
    "                                    if threshold > stats['max'] + margin:\n",
    "                                        reasons.append(f\"{indicator} > {threshold:.2f} (max: {stats['max']:.2f})\")\n",
    "                                        return False, reasons\n",
    "                                    # Too extreme (beyond 99th percentile + margin)\n",
    "                                    if threshold > stats['p99'] + data_range * extreme_margin_pct:\n",
    "                                        reasons.append(f\"{indicator} > {threshold:.2f} (99th: {stats['p99']:.2f})\")\n",
    "                                        return False, reasons\n",
    "                                elif operator in ['<', '<=']:\n",
    "                                    if threshold < stats['min'] - margin:\n",
    "                                        reasons.append(f\"{indicator} < {threshold:.2f} (min: {stats['min']:.2f})\")\n",
    "                                        return False, reasons\n",
    "                                    # Too extreme (below 1st percentile - margin)\n",
    "                                    if threshold < stats['p01'] - data_range * extreme_margin_pct:\n",
    "                                        reasons.append(f\"{indicator} < {threshold:.2f} (1st: {stats['p01']:.2f})\")\n",
    "                                        return False, reasons\n",
    "                            \n",
    "                            return True, []\n",
    "                        \n",
    "                        # Filter rules with detailed tracking\n",
    "                        validation_results = results_df['rule'].apply(validate_rule_detailed)\n",
    "                        results_df['is_valid'] = validation_results.apply(lambda x: x[0])\n",
    "                        results_df['validation_reason'] = validation_results.apply(lambda x: ', '.join(x[1]))\n",
    "                        \n",
    "                        # Separate valid and invalid\n",
    "                        valid_results = results_df[results_df['is_valid']].drop(columns=['is_valid', 'validation_reason'])\n",
    "                        invalid_results = results_df[~results_df['is_valid']]\n",
    "                        invalid_count = len(invalid_results)\n",
    "                        \n",
    "                        # Show filtering statistics\n",
    "                        if invalid_count > 0:\n",
    "                            st.warning(f\"âš ï¸ Filtered out {invalid_count} rules with unrealistic thresholds \"\n",
    "                                     f\"({invalid_count/original_count*100:.1f}% of total)\")\n",
    "                            \n",
    "                            # Show detailed filtering info if requested\n",
    "                            if show_validation_details and len(invalid_results) > 0:\n",
    "                                with st.expander(f\"ðŸ“‹ View {min(20, len(invalid_results))} Filtered Rules (Click to expand)\", expanded=False):\n",
    "                                    st.markdown(\"**Sample of filtered rules and reasons:**\")\n",
    "                                    \n",
    "                                    # Group by reason\n",
    "                                    reason_counts = invalid_results['validation_reason'].value_counts()\n",
    "                                    \n",
    "                                    st.markdown(\"**Filtering Breakdown:**\")\n",
    "                                    for reason, count in reason_counts.head(10).items():\n",
    "                                        st.markdown(f\"- `{reason}`: **{count} rules** ({count/invalid_count*100:.1f}%)\")\n",
    "                                    \n",
    "                                    st.markdown(\"---\")\n",
    "                                    st.markdown(\"**Examples of filtered rules:**\")\n",
    "                                    \n",
    "                                    for idx, row in invalid_results.head(20).iterrows():\n",
    "                                        st.markdown(f\"âŒ `{row['rule']}`\")\n",
    "                                        st.markdown(f\"   â†³ *{row['validation_reason']}*\")\n",
    "                        else:\n",
    "                            st.success(f\"âœ… All {original_count} rules passed validation!\")\n",
    "                        \n",
    "                        results_to_save = valid_results\n",
    "                    else:\n",
    "                        st.info(\"â„¹ï¸ Rule validation disabled - using all generated rules\")\n",
    "                        results_to_save = results_df\n",
    "                    \n",
    "                    st.session_state.advanced_analysis_done = True\n",
    "                    st.session_state.advanced_results = results_to_save\n",
    "                    st.session_state.optimization_histories = optimization_histories\n",
    "                    \n",
    "                    if len(results_to_save) > 0:\n",
    "                        st.success(f\"âœ… Analysis complete! {len(results_to_save)} valid rules found. Scroll down to see results.\")\n",
    "                    else:\n",
    "                        st.warning(\"âš ï¸ No valid rules passed threshold validation.\")\n",
    "                        st.info(\"\"\"\n",
    "                        **ðŸ’¡ Try these adjustments:**\n",
    "                        1. **Disable validation** temporarily to see all rules\n",
    "                        2. **Increase margins** in validation settings (use 30-50%)\n",
    "                        3. **Lower optimization thresholds** (Min IS Sharpe, Min OOS Sharpe)\n",
    "                        4. Use **more historical data** for better indicator coverage\n",
    "                        5. Try **different indicators** or optimization methods\n",
    "                        \"\"\")\n",
    "                        \n",
    "                else:\n",
    "                    st.session_state.advanced_analysis_done = True\n",
    "                    st.session_state.advanced_results = pd.DataFrame()\n",
    "                    st.session_state.optimization_histories = optimization_histories\n",
    "                    \n",
    "                    st.warning(\"âš ï¸ No valid rules passed all quality filters.\")\n",
    "                    st.info(\"\"\"\n",
    "                    **ðŸ’¡ Suggestions to find rules:**\n",
    "                    1. Lower **Min IS Sharpe** to 0.0 or negative\n",
    "                    2. Lower **Min OOS Sharpe** to -1.0\n",
    "                    3. Increase **Min Signals** thresholds (more data = easier to find patterns)\n",
    "                    4. Try **Brute Force** method (tests more combinations)\n",
    "                    5. Increase holding period to 15-30 days\n",
    "                    \"\"\")\n",
    "                st.rerun()\n",
    "            \n",
    "            # ============= DISPLAY RESULTS =============\n",
    "            if st.session_state.advanced_analysis_done:\n",
    "                st.markdown(\"---\")\n",
    "                st.markdown(\"## ðŸ“Š Analysis Results\")\n",
    "                \n",
    "                # Clear button\n",
    "                if st.button(\"ðŸ”„ Clear Results & Run New Analysis\", key=\"clear_advanced\"):\n",
    "                    st.session_state.advanced_analysis_done = False\n",
    "                    st.session_state.advanced_results = None\n",
    "                    st.session_state.optimization_histories = None\n",
    "                    st.rerun()\n",
    "                \n",
    "                results_df = st.session_state.advanced_results\n",
    "                optimization_histories = st.session_state.optimization_histories or {}\n",
    "                \n",
    "                if results_df is not None and len(results_df) > 0:\n",
    "                    # Metrics\n",
    "                    col1, col2, col3, col4 = st.columns(4)\n",
    "                    \n",
    "                    with col1:\n",
    "                        st.metric(\"Total Rules Found\", len(results_df))\n",
    "                    \n",
    "                    with col2:\n",
    "                        positive_oos = len(results_df[results_df['OOS_sharpe'] > 0])\n",
    "                        st.metric(\"Positive OOS Sharpe\", positive_oos, \n",
    "                                f\"{positive_oos/len(results_df)*100:.1f}%\")\n",
    "                    \n",
    "                    with col3:\n",
    "                        avg_consistency = results_df['consistency_score'].mean()\n",
    "                        st.metric(\"Avg Consistency\", f\"{avg_consistency:.2%}\")\n",
    "                    \n",
    "                    with col4:\n",
    "                        selector = RuleSelector()\n",
    "                        robust = selector.filter_robust_rules(\n",
    "                            results_df, min_is_sharpe, min_oos_sharpe,\n",
    "                            max_degradation, min_consistency, min_signals_is, min_signals_oos\n",
    "                        )\n",
    "                        st.metric(\"Robust Rules\", len(robust))\n",
    "                    \n",
    "                    # Visualizations\n",
    "                    st.markdown(\"### ðŸ“ˆ Performance Analysis\")\n",
    "                    \n",
    "                    viz_col1, viz_col2 = st.columns(2)\n",
    "                    \n",
    "                    with viz_col1:\n",
    "                        fig = create_performance_comparison_chart(results_df)\n",
    "                        st.plotly_chart(fig, use_container_width=True)\n",
    "                    \n",
    "                    with viz_col2:\n",
    "                        fig = create_scatter_performance(results_df)\n",
    "                        st.plotly_chart(fig, use_container_width=True)\n",
    "                    \n",
    "                    # Results Table\n",
    "                    st.markdown(\"### ðŸ“‹ Detailed Results\")\n",
    "                    \n",
    "                    view_option = st.radio(\n",
    "                        \"Show:\",\n",
    "                        [\"Top 20 by Consistency\", \"Robust Rules Only\", \"All Results\"],\n",
    "                        horizontal=True,\n",
    "                        key=\"view_option_radio\"\n",
    "                    )\n",
    "                    \n",
    "                    display_cols = [\n",
    "                        'rule', 'type', 'trade_type', 'best_holding_period',\n",
    "                        'IS_signals', 'IS_sharpe', 'IS_sortino', 'IS_win_rate',\n",
    "                        'OOS_signals', 'OOS_sharpe', 'OOS_sortino', 'OOS_win_rate',\n",
    "                        'consistency_score', 'sharpe_degradation'\n",
    "                    ]\n",
    "                    \n",
    "                    # Filter columns that exist in results_df\n",
    "                    display_cols = [col for col in display_cols if col in results_df.columns]\n",
    "                    \n",
    "                    if view_option == \"Top 20 by Consistency\":\n",
    "                        display_df = results_df.nlargest(20, 'consistency_score')[display_cols]\n",
    "                    elif view_option == \"Robust Rules Only\" and len(robust) > 0:\n",
    "                        display_df = robust[display_cols]\n",
    "                    else:\n",
    "                        display_df = results_df[display_cols]\n",
    "                    \n",
    "                    st.dataframe(\n",
    "                        display_df.style.format({\n",
    "                            'IS_sharpe': '{:.2f}',\n",
    "                            'IS_sortino': '{:.2f}',\n",
    "                            'IS_win_rate': '{:.1f}',\n",
    "                            'OOS_sharpe': '{:.2f}',\n",
    "                            'OOS_sortino': '{:.2f}',\n",
    "                            'OOS_win_rate': '{:.1f}',\n",
    "                            'consistency_score': '{:.2%}',\n",
    "                            'sharpe_degradation': '{:.2f}'\n",
    "                        }).background_gradient(subset=['OOS_sharpe'], cmap='RdYlGn', vmin=-1, vmax=2)\n",
    "                           .background_gradient(subset=['consistency_score'], cmap='YlGn', vmin=0, vmax=1),\n",
    "                        use_container_width=True,\n",
    "                        height=600\n",
    "                    )\n",
    "                    \n",
    "                    # Export Options\n",
    "                    st.markdown(\"### ðŸ’¾ Export Results\")\n",
    "                    \n",
    "                    exp_col1, exp_col2, exp_col3 = st.columns(3)\n",
    "                    \n",
    "                    with exp_col1:\n",
    "                        csv = results_df.to_csv(index=False)\n",
    "                        st.download_button(\n",
    "                            \"ðŸ“¥ All Results CSV\",\n",
    "                            csv,\n",
    "                            f\"{ticker}_advanced_rules_{datetime.now().strftime('%Y%m%d_%H%M')}.csv\",\n",
    "                            \"text/csv\",\n",
    "                            use_container_width=True\n",
    "                        )\n",
    "                    \n",
    "                    with exp_col2:\n",
    "                        if len(robust) > 0:\n",
    "                            csv = robust.to_csv(index=False)\n",
    "                            st.download_button(\n",
    "                                \"ðŸ“¥ Robust Rules CSV\",\n",
    "                                csv,\n",
    "                                f\"{ticker}_robust_{datetime.now().strftime('%Y%m%d_%H%M')}.csv\",\n",
    "                                \"text/csv\",\n",
    "                                use_container_width=True\n",
    "                            )\n",
    "                    \n",
    "                    with exp_col3:\n",
    "                        if optimization_histories:\n",
    "                            history_json = json.dumps(optimization_histories, indent=2)\n",
    "                            st.download_button(\n",
    "                                \"ðŸ“¥ Optimization History JSON\",\n",
    "                                history_json,\n",
    "                                f\"{ticker}_opt_history_{datetime.now().strftime('%Y%m%d_%H%M')}.json\",\n",
    "                                \"application/json\",\n",
    "                                use_container_width=True\n",
    "                            )\n",
    "                \n",
    "                elif results_df is not None and len(results_df) == 0:\n",
    "                    st.warning(\"âš ï¸ Analysis completed but no valid rules were found. Try different parameters.\")\n",
    "                    if st.button(\"ðŸ”„ Try Again\", key=\"retry_advanced\"):\n",
    "                        st.session_state.advanced_analysis_done = False\n",
    "                        st.rerun()\n",
    "                \n",
    "                else:\n",
    "                    st.error(\"âš ï¸ An error occurred during analysis. Please try again.\")\n",
    "                    if st.button(\"ðŸ”„ Retry\", key=\"retry_error\"):\n",
    "                        st.session_state.advanced_analysis_done = False\n",
    "                        st.rerun()\n",
    "            \n",
    "            elif not st.session_state.advanced_analysis_done:\n",
    "                st.info(\"ðŸ‘† Configure your parameters above and click 'RUN ADVANCED ANALYSIS' to begin.\")\n",
    "        \n",
    "        # TAB 3: Export\n",
    "        with tab3:\n",
    "            st.markdown(\"### ðŸ’¾ Export Options\")\n",
    "            \n",
    "            col1, col2 = st.columns(2)\n",
    "            \n",
    "            with col1:\n",
    "                if st.button(\"ðŸ“¥ Download Indicators\", key=\"download_indicators\"):\n",
    "                    csv = indicators.to_csv()\n",
    "                    st.download_button(\n",
    "                        \"Download CSV\",\n",
    "                        csv,\n",
    "                        f\"{ticker}_indicators_{datetime.now().strftime('%Y%m%d')}.csv\",\n",
    "                        \"text/csv\"\n",
    "                    )\n",
    "            \n",
    "            with col2:\n",
    "                if st.button(\"ðŸ“¥ Download Data\", key=\"download_data\"):\n",
    "                    csv = data.to_csv()\n",
    "                    st.download_button(\n",
    "                        \"Download CSV\",\n",
    "                        csv,\n",
    "                        f\"{ticker}_data_{datetime.now().strftime('%Y%m%d')}.csv\",\n",
    "                        \"text/csv\"\n",
    "                    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f284318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting TA-Lib\n",
      "  Downloading ta_lib-0.6.8-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (23 kB)\n",
      "Collecting build (from TA-Lib)\n",
      "  Downloading build-1.4.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: numpy in /home/antonio/anaconda3/lib/python3.12/site-packages (from TA-Lib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=24.0 in /home/antonio/anaconda3/lib/python3.12/site-packages (from build->TA-Lib) (24.1)\n",
      "Collecting pyproject_hooks (from build->TA-Lib)\n",
      "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Downloading ta_lib-0.6.8-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (4.1 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading build-1.4.0-py3-none-any.whl (24 kB)\n",
      "Downloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
      "Installing collected packages: pyproject_hooks, build, TA-Lib\n",
      "Successfully installed TA-Lib-0.6.8 build-1.4.0 pyproject_hooks-1.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "de"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
